[
  {
    "objectID": "pages/index.html",
    "href": "pages/index.html",
    "title": "Formula One Historical Data Analysis",
    "section": "",
    "text": "About the Project\nFormula One (F1) is a series of races held at different venues around the world, of the fastest open-wheel, open-cabin, single-seated, four-wheeled autos in the world. There have been a few years when even six-wheeled cars participated in F1. The word Formula, included in the name, refers to a set of rules that the constructors of the car, its mechanics and the drivers have to strictly follow during the tenure that they are members of F1. When motor racing first began, there were no limitations on the power or the size of the cars. The races became unequal with cars with more power and bigger sizes easily outpacing the smaller cars. The races also created dangerous situations in which many participants got seriously hurt, some even fatally. When racing resumed after World War II, the governing body of the sport, the FIA, introduced a set of rules that set limits on the size and the power of the cars. This created an even playing field for the participants and drivers, while more stress was laid on the efficiency and design of the cars and also the capability of the drivers. Formula 1 cars are data-driven, intelligent systems that can reach speeds of over 200 mph. There have been more people in space than have driven an F1 car. These Formula One racers are the pinnacle of automobile technology. Hundreds of sensors monitor every aspect of each vehicle, including lap times, tyre and brake temperatures, air flow, and engine performance. Hundreds of stories are unfolding behind the scenes as you watch a race, such as tyre wear, engine health, and driver responsiveness. Few sports use data analytics as extensively as Formula One. It influences the design of the cars, how they are driven, and how the race is aired. Prior to the adoption of data analytics, a race’s success or failure was entirely determined by the driver’s split-second decisions on the track. By the 1970s, however, telemetry systems had shrunk in size and sophistication to the point where they could be installed in vehicles to provide information about their operation.Electronic systems were commonly installed on F1 vehicles by the 1980s. Initially, storage was limited to a single laps worth of data, and drivers were given a signal to turn on the telemetry when the team needed to collect data. The data would then be removed from the vehicle and transferred to computer systems in the garage for further analysis. By the end of the decade, burst telemetry had been developed, which sent radio signals from the car to the garage during a race, giving the pit crew advance warning of the car’s physical condition. These bursts were replaced by streaming data, which was piped back to the garage and then to the factory. Pre-race simulations, real-time decision-making by analysts and pit crew, post-race analysis, and the broadcast experience are all influenced by real-time data streams today. Considering so much of the data stored after every lap and as the level of racing between different teams is now more or less even with respect to engine sizes, weight and fuel tanks, there are a ton of other factors that influence the result of any particular race in a season, I thought of creating my project based on this data. The main aim of this project is to observe how these factors influence the results of a race and use these observations to build a model that predicts the results of a race.\n\n\nBasic Information and Definitions in Formula 1\n\nOverview: Formula One, also called F1 in short, is an international auto racing sport. F1 is the highest level of single-seat, open-wheel and open-cockpit professional motor racing contest.\nObjective: The objective of a Formula 1 contest is to determine the winner of a race. The driver who crosses the finish line first after completing a pre-determined number of laps is declared the winner.\nFormula One-History & Team Size: Formula 1 racing originated during the 1920-30s in Europe from other similar racing competitions. In 1946, the FIA standardized racing rules and this formed the basis of Formula One racing. The inaugural Formula One World Drivers’ championship was then held in 1950, the first world championship series. Apart from the world championship series, many other non-championship F1 races were also held, but as the costs of conducting these contests got higher, such races were discontinued after 1983. Each F1 team can have maximum of four drivers per season. There is support staff with every F1 team that plays a vital role in the team’s success.\nNumber Of Grand Prix in a Season: The number of Grand Prix in a season has varied through the years, starting from 1950 which had 7 races. This number kept increasing up to a maximum of 20 GPs a year (in 2012). Normally there are 19 to 20 GPs in a season now.\nNumber of Teams in F1 World Championship: 10 teams with two cars each are permitted to compete in the F1 World Championship as of 2022. That is, a total of 20 cars can enter the competition. However, the FIA regulations allow a limit of 26 cars for the championship.\nConstructor: Since 1981, FIA has passed a rule that respective F1 teams have to build their own engine and chassis of the car. The owner of the engine and chassis is called the constructor.\nCircuit: Formula One circuits are tracks specifically and purposefully built for conducting races. F1 circuits are of two types − Street Circuit and Road Circuit.\nPole position: Driver who recorded fastest time during qualifying session is awarded the first grid position on race day.\nRace Distance and Duration: The length of the race must be 305 km (260 km in case of Monaco GP) and is defined as “the smallest number of complete laps that exceeds 305 kilometers”. The number of laps in a race is obtained by dividing 305 by the length of a lap, which differs from track to track. The duration of the race cannot be more than 2 hours. If the allocated time of 2 hours is exceeded, the race is considered to be finished at the end of the ongoing lap.\nRacing Points System: The present system of Formula One World Championship points scoring was adopted in 2010 and has been continuing since. According to this system, the top 10 drivers at the end of each Grand Prix will receive points based on the positions they finished and these points will contribute towards determining both, the World Drivers’ and World Constructors’ Championships at the end of the season. The winner receives 25 points (25 Drivers’ Championship points as well as 25 Constructors’ Championship points).\n\n\n\nData Science Questions\n\nAre the points gained and wins in career related to each other for constructors and drivers both?\nWhat led to the downfall of great drivers like Sebastian Vettel and Fernando Alonso?\nWhat really happened in 2021 (One of the best seasons ever) between Hamilton and Max?\nWhat are the sentiments of fans of different teams?\nIs weather an important factor for some drivers to win/lose a race?\nAre some tracks/circuits more prone to accidents than others?\nHow has the popularity of F1 increased over the years?\nWhich constructor(team) has the highest support every season?\nDoes the positions of some drivers depend on final position of other drivers?\nIs historic data relevant and preferred to use while predicting future races?"
  },
  {
    "objectID": "pages/conclusions.html",
    "href": "pages/conclusions.html",
    "title": "Conclusions",
    "section": "",
    "text": "It has been 7 years since I first started watching Formula One on a daily basis and it still is my favourite sport to watch. It does not seem complex while watching it but there are hundreds of factors affecting any driver winning a race. Once you start learning the mechanics behind F1 cars you will understand the vastness of the factors. It has been years since I have followed F1 and will continue to do so in the upcoming years as well. But these past 3 months working on this project has made me realise a lot of new things while also my domain knowledge has made it easier to understand specifics. I have learnt a lot from this project than watching a race and would like share my findings with everyone.\nData Visualization is an important step while getting more information about a particular dataset. It helps us understand the data better and answers a lot of data science questions.\n\nAre the points gained and wins in career related to each other for constructors and drivers both?While Michael Schumacher has more wins in his entire career than other drivers (except Lewis Hamilton), he has lesser number of points as compared to them. By domain knowledge we also know that Schumacher was a part of F1 from the early 90s to 2004, which leads us to the conclusion the point distribution for getting wins has increased over the years. This strange behaviour may also mean that other drivers apart of Schumacher gained more points while not winning (positions 2-10 also has points). A team like McLaren has been in F1 since a very early time which justifies it being the second team while seeing race wins. But the points gathered for McLaren have been lesser in the past few years compared to before 2000. Since we already know the point distribution from being 1st in the race was less before 2000 and McLaren is now a mid-tier team it makes sense that it has less overall points despite being second in Race wins. There are other factors that come into play while comparing points gained and wins in the entire career.\nWhat led to the downfall of great drivers like Sebastian Vettel and Fernando Alonso? The number of points gained in each season for Alonso and Vettel peaked during 2011-2014 seasons while Hamilton peaked during 2017-2020. The downfall of Alonso and Vettel resulted in the success of Hamiltion who became more successful after 2014.\nWhat really happened in 2021 (One of the best seasons ever) between Hamilton and Max? Max struggled at the early rounds with a lot of mechanical failures but gained many podiums at the later stage constantly chipping away from the difference in Hamilton’s lead while Hamilton was pretty constant throughout the season but the podiums and the points gained from Max was a little surprise to Hamilton at the end when they realised that Max was only 2 points behind Hamilton.\nWhat are the sentiments of fans of different teams? Mercedes fans posted the most negative tweets (27.3%) compared to all the other teams but Ferrari and Redbull are not that far behind (26.7%), which leads us to believe that the more famous the team is, the higher the change that its fans would be negative. McLaren also ranks the highest in positive tweets (52.3%).\n\nNaive Bayes Algorithm is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n\nRecord Data: We get training accuracy from our model as 83.93% and balanced accuracy as 93% for Podium, 75% for Top_10 and 84% for Outside_Top_10. Test accuracy from our model as 83.24% and balanced accuracy as 93% for Podium, 74% for Top_10 and 85% for Outisde_Top_10. There is a not a lot of difference between train and test accuracy which means our model is not over fitted. Seeing the balanced accuracy for both test and train dataset we notice that it is difficult to predict the label variable “Top_10” seeing that it’s balanced accuracy is less than the other 2 variables.\nTwitter Data: Since twitter tweets data is not very accurate all the times after running Count Vectorizer and may contain a lot of words that are common to other labels of the same domain, it makes sense that the models would not do very good while predicting the label variables. Even after tuning the hyperparameters, we get the highest accuracy from the model as 76%.\n\nDecision Trees: Decision tree builds classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. Decision Trees usually implement exactly the human thinking ability while making a decision, so it is easy to understand.\n\nSince twitter tweets data is not very accurate all the times after running Count Vectorizer and may contain a lot of words that are common to other labels of the same domain, it makes sense that the models would not do very good while predicting the label variables. Even after tuning the hyperparameters, we get the highest accuracy from the model as 74%.\n\nSupport Vector Machines: SVMs are different from other classification algorithms because of the way they choose the decision boundary that maximizes the distance from the nearest data points of all the classes. The decision boundary created by SVMs is called the maximum margin classifier or the maximum margin hyper plane.\n\nWe run SVM on our record data while doing GridSearchCV for the best hyperparameters.\nAfter running 810 different iterations and combinations of hyperparameters we get 99% accuracy on our dataset which is a lot as compared to the Naive Bayes model(84%). This tells us the disadvantages of the Naive Bayes model even if it has a faster computational time (30 seconds as compared to 160 seconds of SVM).\n\nClustering: Clustering is a Machine Learning method that groups vectors or observations (set of objects) into groups (clusters).\n\nThrough various hyper-parameters and algorithms we come to a conclusion that splitting the race positions (target variable) into 3 sections (1-3, 4-10, 11-20) was the best option as these algorithms also gave the same result.\n\nAssociation Rule Mining: ARM is a technique for identifying frequent patterns, correlations, associations, or causal structures in data sets found in a variety of databases, including relational databases, transactional databases, and other types of data repositories. There are a lot of interesting relations from the rules than can be seen from the network graph:\n\nSuppose if the weather is windy, the season is 2016 and Rosberg is on the pole (1st in the starting grid), it is highly likely that he will get Top 3 (Podium) in the race.\nAnd if the status of the race is Lapped and Hamilton has won the race, it most likely the position that a driver got is Outside Top 10.\nFor the season 2021, if Max Verstappen is on the pole and the weather conditions are Sunny, it is likely that he will win that race."
  },
  {
    "objectID": "pages/codes/record_datacleaningR.html",
    "href": "pages/codes/record_datacleaningR.html",
    "title": "Record Data Cleaning in R",
    "section": "",
    "text": "Data Cleaning\n\nIn the era of big data, cleaning or scrubbing your data has become an essential part of the data management process. Even though data cleaning can be tedious at times, it is absolutely crucial for getting accurate business intelligence (BI) that can drive your strategic decisions.\nIncorrect or inconsistent data leads to false conclusions. And so, how well you clean and understand the data has a high impact on the quality of the results.\nData cleaning involve different techniques based on the problem and the data type. Different methods can be applied with each has its own trade-offs. Overall, incorrect data is either removed, corrected, or imputed.\nData cleaning is the process of removing incorrect, duplicate, or otherwise erroneous data from a dataset. These errors can include incorrectly formatted data, redundant entries, mislabeled data, and other issues; they often arise when two or more datasets are combined together. Data cleaning improves the quality of your data as well as any business decisions that you draw based on the data.\nThere is no one right way to clean a dataset, as every set is different and presents its own unique slate of errors that need to be corrected. Many data cleaning techniques can now be automated with the help of dedicated software, but some portion of the work must be done manually to ensure the greatest accuracy. Usually this work is done by data quality analysts, BI analysts, and business users.\nEvery organization’s data cleaning methods will vary according to their individual needs as well as the particular constraints of the dataset. However, most data cleaning steps follow a standard framework:\n\nDetermine the critical data values you need for your analysis.\nCollect the data you need, then sort and organize it.\nIdentify duplicate or irrelevant values and remove them.\nSearch for missing values and fill them in, so you have a complete dataset.\nFix any remaining structural or repetitive errors in the dataset.\nIdentify outliers and remove them, so they will not interfere with your analysis.\nValidate your dataset to ensure it is ready for data transformation and analysis.\nOnce the set has been validated, perform your transformation and analysis.\n\nData Cleaning vs. Data Cleansing vs. Data Scrubbing:\n\nYou might sometimes hear the terms data cleansing or data scrubbing used instead of data cleaning. In most situations, these terms are all being used interchangeably and refer to the exact same thing. Data scrubbing may sometimes be used to refer to a specific aspect of data cleaning—namely, removing duplicate or bad data from datasets.\nYou should also know that data scrubbing can have a slightly different meaning within the specific context of data storage; in this case, it refers to an automated function that evaluates storage systems and disk drives to identify any bad sectors or blocks and to confirm the data in them can be read.\nNote that all three of these terms—data cleaning, data cleansing, and data scrubbing—are different from data transformation, which is the act of taking clean data and converting it into a new format or structure. Data transformation is a separate process that comes after data cleaning.\n\nBenefits of Data Cleaning:\n\nNot having clean data exacts a high price: IBM estimates that bad data costs the U.S. over $3 trillion each year. That’s because data-driven decisions are only as good as the data you are relying on. Bad quality data leads to equally bad quality decisions. If the data you are basing your strategy on is inaccurate, then your strategy will have the same issues present in the data, even if it seems sound. In fact, sometimes no data at all is better than bad data.\nCleaning your data results in many benefits for your organization in both the short- and long-term. It leads to better decision making, which can boost your efficiency and your customer satisfaction, in turn giving your business a competitive edge. Over time, it also reduces your costs of data management by preemptively removing errors and other mistakes that would necessitate performing analysis over and over again.\n\n\n\n\nImporting Libraries\n\n\nCode\nlibrary(tidyr)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ dplyr   1.0.10\n✔ tibble  3.1.8      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n✔ purrr   0.3.4      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(plyr)\n\n\n------------------------------------------------------------------------------\nYou have loaded plyr after dplyr - this is likely to cause problems.\nIf you need functions from both plyr and dplyr, please load plyr first, then dplyr:\nlibrary(plyr); library(dplyr)\n------------------------------------------------------------------------------\n\nAttaching package: 'plyr'\n\nThe following objects are masked from 'package:dplyr':\n\n    arrange, count, desc, failwith, id, mutate, rename, summarise,\n    summarize\n\nThe following object is masked from 'package:purrr':\n\n    compact\n\n\nCode\nlibrary(stringr)\n\n\n\n\nImporting Data\n\n\nCode\nrace_df = read_csv(\"../../data/00-raw-data/race_results.csv\")\n\n\nNew names:\nRows: 1096 Columns: 9\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(5): url, raceName, Circuit, Results, time dbl (3): ...1, season, round date\n(1): date\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n\n\nCode\ncircuit_df = read_csv(\"../../data/00-raw-data/circuit_info.csv\")\n\n\nNew names:\nRows: 76 Columns: 5\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(4): circuitId, url, circuitName, Location dbl (1): ...1\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -> `...1`\n\n\nCode\ndriver_df = read_csv('../../data/00-raw-data/drivers.csv')\n\n\nRows: 855 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (7): driverRef, number, code, forename, surname, nationality, url\ndbl  (1): driverId\ndate (1): dob\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nCleaning the Data\nDropping unnecesarry columns:\n\n\nCode\ncolnames(race_df) = c('index', 'season', 'round', 'url', 'raceName', 'Circuit', 'date', 'Results', 'time')\ndrop = c(\"index\", \"url\")\nrace_df = race_df[,!(names(race_df) %in% drop)]\n\n\n\n\nCode\ncolnames(circuit_df) = c('index', 'circuitId', 'url', 'circuitName', 'Location')\ndrop = c(\"index\", \"url\")\ncircuit_df = circuit_df[,!(names(circuit_df) %in% drop)]\n\n\nRemoving brackets and apostrophes from the data as it is not needed:\n\n\nCode\nrace_df = race_df %>% \n  mutate(across('Circuit', str_replace, fixed(\"{\"), ''),\n         across('Circuit', str_replace, fixed(\"}\"), ''))\n\n\n\n\nCode\nrace_df$circuitId =  word(race_df$Circuit,1,sep = \",\")\nrace_df$circuitId = word(race_df$circuitId,2,sep = \": \")\nrace_df$circuitId = gsub(\"'\", \"\", race_df$circuitId)\n\n\nMerging Dataframes:\n\n\nCode\ndf = merge(race_df, circuit_df, by = 'circuitId', all.x = TRUE)\ndf = df[order(df$season, df$round), ]\nhead(df)\n\n\n       circuitId season round           raceName\n812  silverstone   1950     1 British Grand Prix\n497       monaco   1950     2  Monaco Grand Prix\n326 indianapolis   1950     3   Indianapolis 500\n102   bremgarten   1950     4   Swiss Grand Prix\n919          spa   1950     5 Belgian Grand Prix\n731        reims   1950     6  French Grand Prix\n                                                                                                                                                                                                                                            Circuit\n812                   {'circuitId': 'silverstone', 'url': 'http://en.wikipedia.org/wiki/Silverstone_Circuit', 'circuitName': 'Silverstone Circuit', 'Location': {'lat': '52.0786', 'long': '-1.01694', 'locality': 'Silverstone', 'country': 'UK'}}\n497                         {'circuitId': 'monaco', 'url': 'http://en.wikipedia.org/wiki/Circuit_de_Monaco', 'circuitName': 'Circuit de Monaco', 'Location': {'lat': '43.7347', 'long': '7.42056', 'locality': 'Monte-Carlo', 'country': 'Monaco'}}\n326 {'circuitId': 'indianapolis', 'url': 'http://en.wikipedia.org/wiki/Indianapolis_Motor_Speedway', 'circuitName': 'Indianapolis Motor Speedway', 'Location': {'lat': '39.795', 'long': '-86.2347', 'locality': 'Indianapolis', 'country': 'USA'}}\n102                     {'circuitId': 'bremgarten', 'url': 'http://en.wikipedia.org/wiki/Circuit_Bremgarten', 'circuitName': 'Circuit Bremgarten', 'Location': {'lat': '46.9589', 'long': '7.40194', 'locality': 'Bern', 'country': 'Switzerland'}}\n919             {'circuitId': 'spa', 'url': 'http://en.wikipedia.org/wiki/Circuit_de_Spa-Francorchamps', 'circuitName': 'Circuit de Spa-Francorchamps', 'Location': {'lat': '50.4372', 'long': '5.97139', 'locality': 'Spa', 'country': 'Belgium'}}\n731                                            {'circuitId': 'reims', 'url': 'http://en.wikipedia.org/wiki/Reims-Gueux', 'circuitName': 'Reims-Gueux', 'Location': {'lat': '49.2542', 'long': '3.93083', 'locality': 'Reims', 'country': 'France'}}\n          date\n812 1950-05-13\n497 1950-05-21\n326 1950-05-30\n102 1950-06-04\n919 1950-06-18\n731 1950-07-02\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Results\n812                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               [{'number': '2', 'position': '1', 'positionText': '1', 'points': '9', 'Driver': {'driverId': 'farina', 'url': 'http://en.wikipedia.org/wiki/Nino_Farina', 'givenName': 'Nino', 'familyName': 'Farina', 'dateOfBirth': '1906-10-30', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'alfa', 'url': 'http://en.wikipedia.org/wiki/Alfa_Romeo_in_Formula_One', 'name': 'Alfa Romeo', 'nationality': 'Swiss'}, 'grid': '1', 'laps': '70', 'status': 'Finished', 'Time': {'millis': '8003600', 'time': '2:13:23.6'}}, {'number': '3', 'position': '2', 'positionText': '2', 'points': '6', 'Driver': {'driverId': 'fagioli', 'url': 'http://en.wikipedia.org/wiki/Luigi_Fagioli', 'givenName': 'Luigi', 'familyName': 'Fagioli', 'dateOfBirth': '1898-06-09', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'alfa', 'url': 'http://en.wikipedia.org/wiki/Alfa_Romeo_in_Formula_One', 'name': 'Alfa Romeo', 'nationality': 'Swiss'}, 'grid': '2', 'laps': '70', 'status': 'Finished', 'Time': {'millis': '8006200', 'time': '+2.6'}}, {'number': '4', 'position': '3', 'positionText': '3', 'points': '4', 'Driver': {'driverId': 'reg_parnell', 'url': 'http://en.wikipedia.org/wiki/Reg_Parnell', 'givenName': 'Reg', 'familyName': 'Parnell', 'dateOfBirth': '1911-07-02', 'nationality': 'British'}, 'Constructor': {'constructorId': 'alfa', 'url': 'http://en.wikipedia.org/wiki/Alfa_Romeo_in_Formula_One', 'name': 'Alfa Romeo', 'nationality': 'Swiss'}, 'grid': '4', 'laps': '70', 'status': 'Finished', 'Time': {'millis': '8055600', 'time': '+52.0'}}, {'number': '14', 'position': '4', 'positionText': '4', 'points': '3', 'Driver': {'driverId': 'cabantous', 'url': 'http://en.wikipedia.org/wiki/Yves_Giraud_Cabantous', 'givenName': 'Yves', 'familyName': 'Cabantous', 'dateOfBirth': '1904-10-08', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '6', 'laps': '68', 'status': '+2 Laps'}, {'number': '15', 'position': '5', 'positionText': '5', 'points': '2', 'Driver': {'driverId': 'rosier', 'url': 'http://en.wikipedia.org/wiki/Louis_Rosier', 'givenName': 'Louis', 'familyName': 'Rosier', 'dateOfBirth': '1905-11-05', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '9', 'laps': '68', 'status': '+2 Laps'}, {'number': '12', 'position': '6', 'positionText': '6', 'points': '0', 'Driver': {'driverId': 'gerard', 'url': 'http://en.wikipedia.org/wiki/Bob_Gerard', 'givenName': 'Bob', 'familyName': 'Gerard', 'dateOfBirth': '1914-01-19', 'nationality': 'British'}, 'Constructor': {'constructorId': 'era', 'url': 'http://en.wikipedia.org/wiki/English_Racing_Automobiles', 'name': 'ERA', 'nationality': 'British'}, 'grid': '13', 'laps': '67', 'status': '+3 Laps'}, {'number': '11', 'position': '7', 'positionText': '7', 'points': '0', 'Driver': {'driverId': 'harrison', 'url': 'http://en.wikipedia.org/wiki/Cuth_Harrison', 'givenName': 'Cuth', 'familyName': 'Harrison', 'dateOfBirth': '1906-07-06', 'nationality': 'British'}, 'Constructor': {'constructorId': 'era', 'url': 'http://en.wikipedia.org/wiki/English_Racing_Automobiles', 'name': 'ERA', 'nationality': 'British'}, 'grid': '15', 'laps': '67', 'status': '+3 Laps'}, {'number': '16', 'position': '8', 'positionText': '8', 'points': '0', 'Driver': {'driverId': 'etancelin', 'url': 'http://en.wikipedia.org/wiki/Philippe_%C3%89tancelin', 'givenName': 'Philippe', 'familyName': 'Étancelin', 'dateOfBirth': '1896-12-28', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '14', 'laps': '65', 'status': '+5 Laps'}, {'number': '6', 'position': '9', 'positionText': '9', 'points': '0', 'Driver': {'driverId': 'hampshire', 'url': 'http://en.wikipedia.org/wiki/David_Hampshire', 'givenName': 'David', 'familyName': 'Hampshire', 'dateOfBirth': '1917-12-29', 'nationality': 'British'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '16', 'laps': '64', 'status': '+6 Laps'}, {'number': '10', 'position': '10', 'positionText': '10', 'points': '0', 'Driver': {'driverId': 'shawe_taylor', 'url': 'http://en.wikipedia.org/wiki/Brian_Shawe_Taylor', 'givenName': 'Brian', 'familyName': 'Shawe Taylor', 'dateOfBirth': '1915-01-28', 'nationality': 'British'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '20', 'laps': '64', 'status': '+6 Laps'}, {'number': '10', 'position': '10', 'positionText': '10', 'points': '0', 'Driver': {'driverId': 'fry', 'url': 'http://en.wikipedia.org/wiki/Joe_Fry', 'givenName': 'Joe', 'familyName': 'Fry', 'dateOfBirth': '1915-10-26', 'nationality': 'British'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '20', 'laps': '64', 'status': '+6 Laps'}, {'number': '18', 'position': '11', 'positionText': '11', 'points': '0', 'Driver': {'driverId': 'claes', 'url': 'http://en.wikipedia.org/wiki/Johnny_Claes', 'givenName': 'Johnny', 'familyName': 'Claes', 'dateOfBirth': '1916-08-11', 'nationality': 'Belgian'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '21', 'laps': '64', 'status': '+6 Laps'}, {'number': '1', 'position': '12', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'fangio', 'url': 'http://en.wikipedia.org/wiki/Juan_Manuel_Fangio', 'givenName': 'Juan', 'familyName': 'Fangio', 'dateOfBirth': '1911-06-24', 'nationality': 'Argentine'}, 'Constructor': {'constructorId': 'alfa', 'url': 'http://en.wikipedia.org/wiki/Alfa_Romeo_in_Formula_One', 'name': 'Alfa Romeo', 'nationality': 'Swiss'}, 'grid': '3', 'laps': '62', 'status': 'Oil leak'}, {'number': '23', 'position': '13', 'positionText': 'N', 'points': '0', 'Driver': {'driverId': 'kelly', 'url': 'http://en.wikipedia.org/wiki/Joe_Kelly_(Formula_One)', 'givenName': 'Joe', 'familyName': 'Kelly', 'dateOfBirth': '1913-03-13', 'nationality': 'Irish'}, 'Constructor': {'constructorId': 'alta', 'url': 'http://en.wikipedia.org/wiki/Alta_auto_racing_team', 'name': 'Alta', 'nationality': 'British'}, 'grid': '19', 'laps': '57', 'status': 'Not classified'}, {'number': '21', 'position': '14', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'bira', 'url': 'http://en.wikipedia.org/wiki/Prince_Bira', 'givenName': 'Prince', 'familyName': 'Bira', 'dateOfBirth': '1914-07-15', 'nationality': 'Thai'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '5', 'laps': '49', 'status': 'Out of fuel'}, {'number': '5', 'position': '15', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'murray', 'url': 'http://en.wikipedia.org/wiki/David_Murray_(driver)', 'givenName': 'David', 'familyName': 'Murray', 'dateOfBirth': '1909-12-28', 'nationality': 'British'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '18', 'laps': '44', 'status': 'Engine'}, {'number': '24', 'position': '16', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'crossley', 'url': 'http://en.wikipedia.org/wiki/Geoff_Crossley', 'givenName': 'Geoff', 'familyName': 'Crossley', 'dateOfBirth': '1921-05-11', 'nationality': 'British'}, 'Constructor': {'constructorId': 'alta', 'url': 'http://en.wikipedia.org/wiki/Alta_auto_racing_team', 'name': 'Alta', 'nationality': 'British'}, 'grid': '17', 'laps': '43', 'status': 'Transmission'}, {'number': '20', 'position': '17', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'graffenried', 'url': 'http://en.wikipedia.org/wiki/Toulo_de_Graffenried', 'givenName': 'Toulo', 'familyName': 'de Graffenried', 'dateOfBirth': '1914-05-18', 'nationality': 'Swiss'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '8', 'laps': '36', 'status': 'Engine'}, {'number': '19', 'position': '18', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'chiron', 'url': 'http://en.wikipedia.org/wiki/Louis_Chiron', 'givenName': 'Louis', 'familyName': 'Chiron', 'dateOfBirth': '1899-08-03', 'nationality': 'Monegasque'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '11', 'laps': '26', 'status': 'Clutch'}, {'number': '17', 'position': '19', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'martin', 'url': 'http://en.wikipedia.org/wiki/Eug%C3%A8ne_Martin', 'givenName': 'Eugène', 'familyName': 'Martin', 'dateOfBirth': '1915-03-24', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '7', 'laps': '8', 'status': 'Oil pressure'}, {'number': '9', 'position': '20', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'peter_walker', 'url': 'http://en.wikipedia.org/wiki/Peter_Walker_(driver)', 'givenName': 'Peter', 'familyName': 'Walker', 'dateOfBirth': '1912-10-07', 'nationality': 'British'}, 'Constructor': {'constructorId': 'era', 'url': 'http://en.wikipedia.org/wiki/English_Racing_Automobiles', 'name': 'ERA', 'nationality': 'British'}, 'grid': '10', 'laps': '5', 'status': 'Gearbox'}, {'number': '9', 'position': '20', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'rolt', 'url': 'http://en.wikipedia.org/wiki/Tony_Rolt', 'givenName': 'Tony', 'familyName': 'Rolt', 'dateOfBirth': '1918-10-16', 'nationality': 'British'}, 'Constructor': {'constructorId': 'era', 'url': 'http://en.wikipedia.org/wiki/English_Racing_Automobiles', 'name': 'ERA', 'nationality': 'British'}, 'grid': '10', 'laps': '5', 'status': 'Gearbox'}, {'number': '8', 'position': '21', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'leslie_johnson', 'url': 'http://en.wikipedia.org/wiki/Leslie_Johnson_(racing_driver)', 'givenName': 'Leslie', 'familyName': 'Johnson', 'dateOfBirth': '1912-03-22', 'nationality': 'British'}, 'Constructor': {'constructorId': 'era', 'url': 'http://en.wikipedia.org/wiki/English_Racing_Automobiles', 'name': 'ERA', 'nationality': 'British'}, 'grid': '12', 'laps': '2', 'status': 'Supercharger'}]\n497                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      [{'number': '34', 'position': '1', 'positionText': '1', 'points': '9', 'Driver': {'driverId': 'fangio', 'url': 'http://en.wikipedia.org/wiki/Juan_Manuel_Fangio', 'givenName': 'Juan', 'familyName': 'Fangio', 'dateOfBirth': '1911-06-24', 'nationality': 'Argentine'}, 'Constructor': {'constructorId': 'alfa', 'url': 'http://en.wikipedia.org/wiki/Alfa_Romeo_in_Formula_One', 'name': 'Alfa Romeo', 'nationality': 'Swiss'}, 'grid': '1', 'laps': '100', 'status': 'Finished', 'Time': {'millis': '11598700', 'time': '3:13:18.7'}}, {'number': '40', 'position': '2', 'positionText': '2', 'points': '6', 'Driver': {'driverId': 'ascari', 'url': 'http://en.wikipedia.org/wiki/Alberto_Ascari', 'givenName': 'Alberto', 'familyName': 'Ascari', 'dateOfBirth': '1918-07-13', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'ferrari', 'url': 'http://en.wikipedia.org/wiki/Scuderia_Ferrari', 'name': 'Ferrari', 'nationality': 'Italian'}, 'grid': '7', 'laps': '99', 'status': '+1 Lap'}, {'number': '48', 'position': '3', 'positionText': '3', 'points': '4', 'Driver': {'driverId': 'chiron', 'url': 'http://en.wikipedia.org/wiki/Louis_Chiron', 'givenName': 'Louis', 'familyName': 'Chiron', 'dateOfBirth': '1899-08-03', 'nationality': 'Monegasque'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '8', 'laps': '98', 'status': '+2 Laps'}, {'number': '42', 'position': '4', 'positionText': '4', 'points': '3', 'Driver': {'driverId': 'sommer', 'url': 'http://en.wikipedia.org/wiki/Raymond_Sommer', 'givenName': 'Raymond', 'familyName': 'Sommer', 'dateOfBirth': '1906-08-31', 'nationality': 'French'}, 'Constructor': {'constructorId': 'ferrari', 'url': 'http://en.wikipedia.org/wiki/Scuderia_Ferrari', 'name': 'Ferrari', 'nationality': 'Italian'}, 'grid': '9', 'laps': '97', 'status': '+3 Laps'}, {'number': '50', 'position': '5', 'positionText': '5', 'points': '2', 'Driver': {'driverId': 'bira', 'url': 'http://en.wikipedia.org/wiki/Prince_Bira', 'givenName': 'Prince', 'familyName': 'Bira', 'dateOfBirth': '1914-07-15', 'nationality': 'Thai'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '15', 'laps': '95', 'status': '+5 Laps'}, {'number': '26', 'position': '6', 'positionText': '6', 'points': '0', 'Driver': {'driverId': 'gerard', 'url': 'http://en.wikipedia.org/wiki/Bob_Gerard', 'givenName': 'Bob', 'familyName': 'Gerard', 'dateOfBirth': '1914-01-19', 'nationality': 'British'}, 'Constructor': {'constructorId': 'era', 'url': 'http://en.wikipedia.org/wiki/English_Racing_Automobiles', 'name': 'ERA', 'nationality': 'British'}, 'grid': '16', 'laps': '94', 'status': '+6 Laps'}, {'number': '6', 'position': '7', 'positionText': '7', 'points': '0', 'Driver': {'driverId': 'claes', 'url': 'http://en.wikipedia.org/wiki/Johnny_Claes', 'givenName': 'Johnny', 'familyName': 'Claes', 'dateOfBirth': '1916-08-11', 'nationality': 'Belgian'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '19', 'laps': '94', 'status': '+6 Laps'}, {'number': '38', 'position': '8', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'villoresi', 'url': 'http://en.wikipedia.org/wiki/Luigi_Villoresi', 'givenName': 'Luigi', 'familyName': 'Villoresi', 'dateOfBirth': '1909-05-16', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'ferrari', 'url': 'http://en.wikipedia.org/wiki/Scuderia_Ferrari', 'name': 'Ferrari', 'nationality': 'Italian'}, 'grid': '6', 'laps': '63', 'status': 'Axle'}, {'number': '14', 'position': '9', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'etancelin', 'url': 'http://en.wikipedia.org/wiki/Philippe_%C3%89tancelin', 'givenName': 'Philippe', 'familyName': 'Étancelin', 'dateOfBirth': '1896-12-28', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '4', 'laps': '38', 'status': 'Oil leak'}, {'number': '2', 'position': '10', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'gonzalez', 'url': 'http://en.wikipedia.org/wiki/Jos%C3%A9_Froil%C3%A1n_Gonz%C3%A1lez', 'givenName': 'José Froilán', 'familyName': 'González', 'dateOfBirth': '1922-10-05', 'nationality': 'Argentine'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '3', 'laps': '1', 'status': 'Accident'}, {'number': '32', 'position': '11', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'farina', 'url': 'http://en.wikipedia.org/wiki/Nino_Farina', 'givenName': 'Nino', 'familyName': 'Farina', 'dateOfBirth': '1906-10-30', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'alfa', 'url': 'http://en.wikipedia.org/wiki/Alfa_Romeo_in_Formula_One', 'name': 'Alfa Romeo', 'nationality': 'Swiss'}, 'grid': '2', 'laps': '0', 'status': 'Accident'}, {'number': '36', 'position': '12', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'fagioli', 'url': 'http://en.wikipedia.org/wiki/Luigi_Fagioli', 'givenName': 'Luigi', 'familyName': 'Fagioli', 'dateOfBirth': '1898-06-09', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'alfa', 'url': 'http://en.wikipedia.org/wiki/Alfa_Romeo_in_Formula_One', 'name': 'Alfa Romeo', 'nationality': 'Swiss'}, 'grid': '5', 'laps': '0', 'status': 'Accident'}, {'number': '16', 'position': '13', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'rosier', 'url': 'http://en.wikipedia.org/wiki/Louis_Rosier', 'givenName': 'Louis', 'familyName': 'Rosier', 'dateOfBirth': '1905-11-05', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '10', 'laps': '0', 'status': 'Accident'}, {'number': '10', 'position': '14', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'manzon', 'url': 'http://en.wikipedia.org/wiki/Robert_Manzon', 'givenName': 'Robert', 'familyName': 'Manzon', 'dateOfBirth': '1917-04-12', 'nationality': 'French'}, 'Constructor': {'constructorId': 'simca', 'url': 'http://en.wikipedia.org/wiki/Simca', 'name': 'Simca', 'nationality': 'French'}, 'grid': '11', 'laps': '0', 'status': 'Accident'}, {'number': '52', 'position': '15', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'graffenried', 'url': 'http://en.wikipedia.org/wiki/Toulo_de_Graffenried', 'givenName': 'Toulo', 'familyName': 'de Graffenried', 'dateOfBirth': '1914-05-18', 'nationality': 'Swiss'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '12', 'laps': '0', 'status': 'Accident'}, {'number': '12', 'position': '16', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'trintignant', 'url': 'http://en.wikipedia.org/wiki/Maurice_Trintignant', 'givenName': 'Maurice', 'familyName': 'Trintignant', 'dateOfBirth': '1917-10-30', 'nationality': 'French'}, 'Constructor': {'constructorId': 'simca', 'url': 'http://en.wikipedia.org/wiki/Simca', 'name': 'Simca', 'nationality': 'French'}, 'grid': '13', 'laps': '0', 'status': 'Accident'}, {'number': '24', 'position': '17', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'harrison', 'url': 'http://en.wikipedia.org/wiki/Cuth_Harrison', 'givenName': 'Cuth', 'familyName': 'Harrison', 'dateOfBirth': '1906-07-06', 'nationality': 'British'}, 'Constructor': {'constructorId': 'era', 'url': 'http://en.wikipedia.org/wiki/English_Racing_Automobiles', 'name': 'ERA', 'nationality': 'British'}, 'grid': '14', 'laps': '0', 'status': 'Accident'}, {'number': '44', 'position': '18', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'rol', 'url': 'http://en.wikipedia.org/wiki/Franco_Rol', 'givenName': 'Franco', 'familyName': 'Rol', 'dateOfBirth': '1908-06-05', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '17', 'laps': '0', 'status': 'Accident'}, {'number': '8', 'position': '19', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'schell', 'url': 'http://en.wikipedia.org/wiki/Harry_Schell', 'givenName': 'Harry', 'familyName': 'Schell', 'dateOfBirth': '1921-06-29', 'nationality': 'American'}, 'Constructor': {'constructorId': 'cooper', 'url': 'http://en.wikipedia.org/wiki/Cooper_Car_Company', 'name': 'Cooper', 'nationality': 'British'}, 'grid': '20', 'laps': '0', 'status': 'Collision'}, {'number': '28', 'position': '20', 'positionText': 'W', 'points': '0', 'Driver': {'driverId': 'whitehead', 'url': 'http://en.wikipedia.org/wiki/Peter_Whitehead_(racing_driver)', 'givenName': 'Peter', 'familyName': 'Whitehead', 'dateOfBirth': '1914-11-12', 'nationality': 'British'}, 'Constructor': {'constructorId': 'ferrari', 'url': 'http://en.wikipedia.org/wiki/Scuderia_Ferrari', 'name': 'Ferrari', 'nationality': 'Italian'}, 'grid': '21', 'laps': '0', 'status': 'Engine'}, {'number': '4', 'position': '21', 'positionText': 'W', 'points': '0', 'Driver': {'driverId': 'pian', 'url': 'http://en.wikipedia.org/wiki/Alfredo_Pi%C3%A0n', 'givenName': 'Alfredo', 'familyName': 'Pián', 'dateOfBirth': '1912-10-21', 'nationality': 'Argentine'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '18', 'laps': '0', 'status': 'Accident'}]\n326 [{'number': '1', 'position': '1', 'positionText': '1', 'points': '9', 'Driver': {'driverId': 'parsons', 'url': 'http://en.wikipedia.org/wiki/Johnnie_Parsons', 'givenName': 'Johnnie', 'familyName': 'Parsons', 'dateOfBirth': '1918-07-04', 'nationality': 'American'}, 'Constructor': {'constructorId': 'kurtis_kraft', 'url': 'http://en.wikipedia.org/wiki/Kurtis_Kraft', 'name': 'Kurtis Kraft', 'nationality': 'American'}, 'grid': '5', 'laps': '138', 'status': 'Finished', 'Time': {'millis': '10015970', 'time': '2:46:55.97'}}, {'number': '3', 'position': '2', 'positionText': '2', 'points': '6', 'Driver': {'driverId': 'holland', 'url': 'http://en.wikipedia.org/wiki/Bill_Holland', 'givenName': 'Bill', 'familyName': 'Holland', 'dateOfBirth': '1907-12-18', 'nationality': 'American'}, 'Constructor': {'constructorId': 'deidt', 'url': 'http://en.wikipedia.org/wiki/Deidt', 'name': 'Deidt', 'nationality': 'American'}, 'grid': '10', 'laps': '137', 'status': '+1 Lap'}, {'number': '31', 'position': '3', 'positionText': '3', 'points': '4', 'Driver': {'driverId': 'rose', 'url': 'http://en.wikipedia.org/wiki/Mauri_Rose', 'givenName': 'Mauri', 'familyName': 'Rose', 'dateOfBirth': '1906-05-26', 'nationality': 'American'}, 'Constructor': {'constructorId': 'deidt', 'url': 'http://en.wikipedia.org/wiki/Deidt', 'name': 'Deidt', 'nationality': 'American'}, 'grid': '3', 'laps': '137', 'status': '+1 Lap'}, {'number': '54', 'position': '4', 'positionText': '4', 'points': '3', 'Driver': {'driverId': 'green', 'url': 'http://en.wikipedia.org/wiki/Cecil_Green', 'givenName': 'Cecil', 'familyName': 'Green', 'dateOfBirth': '1919-09-30', 'nationality': 'American'}, 'Constructor': {'constructorId': 'kurtis_kraft', 'url': 'http://en.wikipedia.org/wiki/Kurtis_Kraft', 'name': 'Kurtis Kraft', 'nationality': 'American'}, 'grid': '12', 'laps': '137', 'status': '+1 Lap'}, {'number': '17', 'position': '5', 'positionText': '5', 'points': '1', 'Driver': {'driverId': 'chitwood', 'url': 'http://en.wikipedia.org/wiki/Joie_Chitwood', 'givenName': 'Joie', 'familyName': 'Chitwood', 'dateOfBirth': '1912-04-14', 'nationality': 'American'}, 'Constructor': {'constructorId': 'kurtis_kraft', 'url': 'http://en.wikipedia.org/wiki/Kurtis_Kraft', 'name': 'Kurtis Kraft', 'nationality': 'American'}, 'grid': '9', 'laps': '136', 'status': '+2 Laps'}, {'number': '17', 'position': '5', 'positionText': '5', 'points': '1', 'Driver': {'driverId': 'bettenhausen', 'url': 'http://en.wikipedia.org/wiki/Tony_Bettenhausen', 'givenName': 'Tony', 'familyName': 'Bettenhausen', 'dateOfBirth': '1916-09-12', 'nationality': 'American'}, 'Constructor': {'constructorId': 'kurtis_kraft', 'url': 'http://en.wikipedia.org/wiki/Kurtis_Kraft', 'name': 'Kurtis Kraft', 'nationality': 'American'}, 'grid': '9', 'laps': '136', 'status': '+2 Laps'}, {'number': '8', 'position': '6', 'positionText': '6', 'points': '0', 'Driver': {'driverId': 'wallard', 'url': 'http://en.wikipedia.org/wiki/Lee_Wallard', 'givenName': 'Lee', 'familyName': 'Wallard', 'dateOfBirth': '1910-09-07', 'nationality': 'American'}, 'Constructor': {'constructorId': 'moore', 'url': 'http://en.wikipedia.org/wiki/Moore_(constructor)', 'name': 'Moore', 'nationality': 'American'}, 'grid': '23', 'laps': '136', 'status': '+2 Laps'}, {'number': '98', 'position': '7', 'positionText': '7', 'points': '0', 'Driver': {'driverId': 'faulkner', 'url': 'http://en.wikipedia.org/wiki/Walt_Faulkner', 'givenName': 'Walt', 'familyName': 'Faulkner', 'dateOfBirth': '1920-02-16', 'nationality': 'American'}, 'Constructor': {'constructorId': 'kurtis_kraft', 'url': 'http://en.wikipedia.org/wiki/Kurtis_Kraft', 'name': 'Kurtis Kraft', 'nationality': 'American'}, 'grid': '1', 'laps': '135', 'status': '+3 Laps'}, {'number': '5', 'position': '8', 'positionText': '8', 'points': '0', 'Driver': {'driverId': 'george_connor', 'url': 'http://en.wikipedia.org/wiki/George_Connor_(driver)', 'givenName': 'George', 'familyName': 'Connor', 'dateOfBirth': '1906-08-16', 'nationality': 'American'}, 'Constructor': {'constructorId': 'lesovsky', 'url': 'http://en.wikipedia.org/wiki/Lesovsky', 'name': 'Lesovsky', 'nationality': 'American'}, 'grid': '4', 'laps': '135', 'status': '+3 Laps'}, {'number': '7', 'position': '9', 'positionText': '9', 'points': '0', 'Driver': {'driverId': 'paul_russo', 'url': 'http://en.wikipedia.org/wiki/Paul_Russo', 'givenName': 'Paul', 'familyName': 'Russo', 'dateOfBirth': '1914-04-10', 'nationality': 'American'}, 'Constructor': {'constructorId': 'nichels', 'url': 'http://en.wikipedia.org/wiki/Nichels', 'name': 'Nichels', 'nationality': 'American'}, 'grid': '19', 'laps': '135', 'status': '+3 Laps'}, {'number': '59', 'position': '10', 'positionText': '10', 'points': '0', 'Driver': {'driverId': 'flaherty', 'url': 'http://en.wikipedia.org/wiki/Pat_Flaherty_(racing_driver)', 'givenName': 'Pat', 'familyName': 'Flaherty', 'dateOfBirth': '1926-01-06', 'nationality': 'American'}, 'Constructor': {'constructorId': 'kurtis_kraft', 'url': 'http://en.wikipedia.org/wiki/Kurtis_Kraft', 'name': 'Kurtis Kraft', 'nationality': 'American'}, 'grid': '11', 'laps': '135', 'status': '+3 Laps'}, {'number': '2', 'position': '11', 'positionText': '11', 'points': '0', 'Driver': {'driverId': 'fohr', 'url': 'http://en.wikipedia.org/wiki/Myron_Fohr', 'givenName': 'Myron', 'familyName': 'Fohr', 'dateOfBirth': '1912-06-17', 'nationality': 'American'}, 'Constructor': {'constructorId': 'marchese', 'url': 'http://en.wikipedia.org/wiki/Marchese_(constructor)', 'name': 'Marchese', 'nationality': 'American'}, 'grid': '16', 'laps': '133', 'status': '+5 Laps'}, {'number': '18', 'position': '12', 'positionText': '12', 'points': '0', 'Driver': {'driverId': 'darter', 'url': 'http://en.wikipedia.org/wiki/Duane_Carter', 'givenName': 'Duane', 'familyName': 'Carter', 'dateOfBirth': '1913-05-05', 'nationality': 'American'}, 'Constructor': {'constructorId': 'stevens', 'url': 'http://en.wikipedia.org/wiki/Stevens_(constructor)', 'name': 'Stevens', 'nationality': 'American'}, 'grid': '13', 'laps': '133', 'status': '+5 Laps'}, {'number': '15', 'position': '13', 'positionText': '13', 'points': '0', 'Driver': {'driverId': 'hellings', 'url': 'http://en.wikipedia.org/wiki/Mack_Hellings', 'givenName': 'Mack', 'familyName': 'Hellings', 'dateOfBirth': '1915-09-14', 'nationality': 'American'}, 'Constructor': {'constructorId': 'kurtis_kraft', 'url': 'http://en.wikipedia.org/wiki/Kurtis_Kraft', 'name': 'Kurtis Kraft', 'nationality': 'American'}, 'grid': '26', 'laps': '132', 'status': '+6 Laps'}, {'number': '49', 'position': '14', 'positionText': '14', 'points': '0', 'Driver': {'driverId': 'mcgrath', 'url': 'http://en.wikipedia.org/wiki/Jack_McGrath_(racing_driver)', 'givenName': 'Jack', 'familyName': 'McGrath', 'dateOfBirth': '1919-10-08', 'nationality': 'American'}, 'Constructor': {'constructorId': 'kurtis_kraft', 'url': 'http://en.wikipedia.org/wiki/Kurtis_Kraft', 'name': 'Kurtis Kraft', 'nationality': 'American'}, 'grid': '6', 'laps': '131', 'status': 'Spun off'}, {'number': '55', 'position': '15', 'positionText': '15', 'points': '0', 'Driver': {'driverId': 'ruttman', 'url': 'http://en.wikipedia.org/wiki/Troy_Ruttman', 'givenName': 'Troy', 'familyName': 'Ruttman', 'dateOfBirth': '1930-03-11', 'nationality': 'American'}, 'Constructor': {'constructorId': 'lesovsky', 'url': 'http://en.wikipedia.org/wiki/Lesovsky', 'name': 'Lesovsky', 'nationality': 'American'}, 'grid': '24', 'laps': '130', 'status': '+8 Laps'}, {'number': '75', 'position': '16', 'positionText': '16', 'points': '0', 'Driver': {'driverId': 'hartley', 'url': 'http://en.wikipedia.org/wiki/Gene_Hartley', 'givenName': 'Gene', 'familyName': 'Hartley', 'dateOfBirth': '1926-01-28', 'nationality': 'American'}, 'Constructor': {'constructorId': 'langley', 'url': 'http://en.wikipedia.org/wiki/Langley_(constructor)', 'name': 'Langley', 'nationality': 'American'}, 'grid': '31', 'laps': '128', 'status': '+10 Laps'}, {'number': '22', 'position': '17', 'positionText': '17', 'points': '0', 'Driver': {'driverId': 'davies', 'url': 'http://en.wikipedia.org/wiki/Jimmy_Davies', 'givenName': 'Jimmy', 'familyName': 'Davies', 'dateOfBirth': '1929-08-08', 'nationality': 'American'}, 'Constructor': {'constructorId': 'ewing', 'url': 'http://en.wikipedia.org/wiki/Ewing_(constructor)', 'name': 'Ewing', 'nationality': 'American'}, 'grid': '27', 'laps': '128', 'status': '+10 Laps'}, {'number': '62', 'position': '18', 'positionText': '18', 'points': '0', 'Driver': {'driverId': 'mcdowell', 'url': 'http://en.wikipedia.org/wiki/Johnny_McDowell', 'givenName': 'Johnny', 'familyName': 'McDowell', 'dateOfBirth': '1915-01-29', 'nationality': 'American'}, 'Constructor': {'constructorId': 'kurtis_kraft', 'url': 'http://en.wikipedia.org/wiki/Kurtis_Kraft', 'name': 'Kurtis Kraft', 'nationality': 'American'}, 'grid': '33', 'laps': '128', 'status': '+10 Laps'}, {'number': '4', 'position': '19', 'positionText': '19', 'points': '0', 'Driver': {'driverId': 'walt_brown', 'url': 'http://en.wikipedia.org/wiki/Walt_Brown_(auto_racer)', 'givenName': 'Walt', 'familyName': 'Brown', 'dateOfBirth': '1911-12-30', 'nationality': 'American'}, 'Constructor': {'constructorId': 'kurtis_kraft', 'url': 'http://en.wikipedia.org/wiki/Kurtis_Kraft', 'name': 'Kurtis Kraft', 'nationality': 'American'}, 'grid': '20', 'laps': '127', 'status': '+11 Laps'}, {'number': '21', 'position': '20', 'positionText': '20', 'points': '0', 'Driver': {'driverId': 'webb', 'url': 'http://en.wikipedia.org/wiki/Travis_Webb', 'givenName': 'Travis', 'familyName': 'Webb', 'dateOfBirth': '1910-10-08', 'nationality': 'American'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '14', 'laps': '126', 'status': '+12 Laps'}, {'number': '81', 'position': '21', 'positionText': '21', 'points': '0', 'Driver': {'driverId': 'hoyt', 'url': 'http://en.wikipedia.org/wiki/Jerry_Hoyt', 'givenName': 'Jerry', 'familyName': 'Hoyt', 'dateOfBirth': '1929-01-29', 'nationality': 'American'}, 'Constructor': {'constructorId': 'kurtis_kraft', 'url': 'http://en.wikipedia.org/wiki/Kurtis_Kraft', 'name': 'Kurtis Kraft', 'nationality': 'American'}, 'grid': '15', 'laps': '125', 'status': '+13 Laps'}, {'number': '27', 'position': '22', 'positionText': '22', 'points': '0', 'Driver': {'driverId': 'ader', 'url': 'http://en.wikipedia.org/wiki/Walt_Ader', 'givenName': 'Walt', 'familyName': 'Ader', 'dateOfBirth': '1913-12-15', 'nationality': 'American'}, 'Constructor': {'constructorId': 'rae', 'url': 'http://en.wikipedia.org/wiki/Rae_(motorsport)', 'name': 'Rae', 'nationality': 'American'}, 'grid': '29', 'laps': '123', 'status': '+15 Laps'}, {'number': '77', 'position': '23', 'positionText': '23', 'points': '0', 'Driver': {'driverId': 'holmes', 'url': 'http://en.wikipedia.org/wiki/Jackie_Holmes', 'givenName': 'Jackie', 'familyName': 'Holmes', 'dateOfBirth': '1920-09-04', 'nationality': 'American'}, 'Constructor': {'constructorId': 'olson', 'url': 'http://en.wikipedia.org/wiki/Olson_(constructor)', 'name': 'Olson', 'nationality': 'American'}, 'grid': '30', 'laps': '123', 'status': 'Spun off'}, {'number': '76', 'position': '24', 'positionText': '24', 'points': '0', 'Driver': {'driverId': 'rathmann', 'url': 'http://en.wikipedia.org/wiki/Jim_Rathmann', 'givenName': 'Jim', 'familyName': 'Rathmann', 'dateOfBirth': '1928-07-16', 'nationality': 'American'}, 'Constructor': {'constructorId': 'wetteroth', 'url': 'http://en.wikipedia.org/wiki/Wetteroth', 'name': 'Wetteroth', 'nationality': 'American'}, 'grid': '28', 'laps': '122', 'status': '+16 Laps'}, {'number': '12', 'position': '25', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'banks', 'url': 'http://en.wikipedia.org/wiki/Henry_Banks', 'givenName': 'Henry', 'familyName': 'Banks', 'dateOfBirth': '1913-06-14', 'nationality': 'American'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '21', 'laps': '112', 'status': 'Oil line'}, {'number': '67', 'position': '26', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'schindler', 'url': 'http://en.wikipedia.org/wiki/Bill_Schindler', 'givenName': 'Bill', 'familyName': 'Schindler', 'dateOfBirth': '1909-03-06', 'nationality': 'American'}, 'Constructor': {'constructorId': 'snowberger', 'url': 'http://en.wikipedia.org/wiki/Snowberger', 'name': 'Snowberger', 'nationality': 'American'}, 'grid': '22', 'laps': '111', 'status': 'Transmission'}, {'number': '24', 'position': '27', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'levrett', 'url': 'http://en.wikipedia.org/wiki/Bayliss_Levrett', 'givenName': 'Bayliss', 'familyName': 'Levrett', 'dateOfBirth': '1914-02-14', 'nationality': 'American'}, 'Constructor': {'constructorId': 'adams', 'url': 'http://en.wikipedia.org/wiki/Adams_(constructor)', 'name': 'Adams', 'nationality': 'American'}, 'grid': '17', 'laps': '108', 'status': 'Oil pressure'}, {'number': '24', 'position': '27', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'cantrell', 'url': 'http://en.wikipedia.org/wiki/William_Cantrell', 'givenName': 'Bill', 'familyName': 'Cantrell', 'dateOfBirth': '1908-01-31', 'nationality': 'American'}, 'Constructor': {'constructorId': 'adams', 'url': 'http://en.wikipedia.org/wiki/Adams_(constructor)', 'name': 'Adams', 'nationality': 'American'}, 'grid': '17', 'laps': '108', 'status': 'Oil pressure'}, {'number': '28', 'position': '28', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'agabashian', 'url': 'http://en.wikipedia.org/wiki/Fred_Agabashian', 'givenName': 'Fred', 'familyName': 'Agabashian', 'dateOfBirth': '1913-08-21', 'nationality': 'American'}, 'Constructor': {'constructorId': 'kurtis_kraft', 'url': 'http://en.wikipedia.org/wiki/Kurtis_Kraft', 'name': 'Kurtis Kraft', 'nationality': 'American'}, 'grid': '2', 'laps': '64', 'status': 'Oil leak'}, {'number': '61', 'position': '29', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'jackson', 'url': 'http://en.wikipedia.org/wiki/Jimmy_Jackson_(driver)', 'givenName': 'Jimmy', 'familyName': 'Jackson', 'dateOfBirth': '1910-07-25', 'nationality': 'American'}, 'Constructor': {'constructorId': 'kurtis_kraft', 'url': 'http://en.wikipedia.org/wiki/Kurtis_Kraft', 'name': 'Kurtis Kraft', 'nationality': 'American'}, 'grid': '32', 'laps': '52', 'status': 'Supercharger'}, {'number': '23', 'position': '30', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'hanks', 'url': 'http://en.wikipedia.org/wiki/Sam_Hanks', 'givenName': 'Sam', 'familyName': 'Hanks', 'dateOfBirth': '1914-07-13', 'nationality': 'American'}, 'Constructor': {'constructorId': 'kurtis_kraft', 'url': 'http://en.wikipedia.org/wiki/Kurtis_Kraft', 'name': 'Kurtis Kraft', 'nationality': 'American'}, 'grid': '25', 'laps': '42', 'status': 'Oil pressure'}, {'number': '14', 'position': '31', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'bettenhausen', 'url': 'http://en.wikipedia.org/wiki/Tony_Bettenhausen', 'givenName': 'Tony', 'familyName': 'Bettenhausen', 'dateOfBirth': '1916-09-12', 'nationality': 'American'}, 'Constructor': {'constructorId': 'deidt', 'url': 'http://en.wikipedia.org/wiki/Deidt', 'name': 'Deidt', 'nationality': 'American'}, 'grid': '8', 'laps': '30', 'status': 'Wheel bearing'}, {'number': '45', 'position': '32', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'dick_rathmann', 'url': 'http://en.wikipedia.org/wiki/Dick_Rathmann', 'givenName': 'Dick', 'familyName': 'Rathmann', 'dateOfBirth': '1924-01-06', 'nationality': 'American'}, 'Constructor': {'constructorId': 'watson', 'url': 'http://en.wikipedia.org/wiki/A.J._Watson', 'name': 'Watson', 'nationality': 'American'}, 'grid': '18', 'laps': '25', 'status': 'Stalled'}, {'number': '69', 'position': '33', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'dinsmore', 'url': 'http://en.wikipedia.org/wiki/Duke_Dinsmore', 'givenName': 'Duke', 'familyName': 'Dinsmore', 'dateOfBirth': '1913-04-10', 'nationality': 'American'}, 'Constructor': {'constructorId': 'kurtis_kraft', 'url': 'http://en.wikipedia.org/wiki/Kurtis_Kraft', 'name': 'Kurtis Kraft', 'nationality': 'American'}, 'grid': '7', 'laps': '10', 'status': 'Oil leak'}]\n102                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  [{'number': '16', 'position': '1', 'positionText': '1', 'points': '9', 'Driver': {'driverId': 'farina', 'url': 'http://en.wikipedia.org/wiki/Nino_Farina', 'givenName': 'Nino', 'familyName': 'Farina', 'dateOfBirth': '1906-10-30', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'alfa', 'url': 'http://en.wikipedia.org/wiki/Alfa_Romeo_in_Formula_One', 'name': 'Alfa Romeo', 'nationality': 'Swiss'}, 'grid': '2', 'laps': '42', 'status': 'Finished', 'Time': {'millis': '7373700', 'time': '2:02:53.7'}}, {'number': '12', 'position': '2', 'positionText': '2', 'points': '6', 'Driver': {'driverId': 'fagioli', 'url': 'http://en.wikipedia.org/wiki/Luigi_Fagioli', 'givenName': 'Luigi', 'familyName': 'Fagioli', 'dateOfBirth': '1898-06-09', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'alfa', 'url': 'http://en.wikipedia.org/wiki/Alfa_Romeo_in_Formula_One', 'name': 'Alfa Romeo', 'nationality': 'Swiss'}, 'grid': '3', 'laps': '42', 'status': 'Finished', 'Time': {'millis': '7374100', 'time': '+0.4'}}, {'number': '10', 'position': '3', 'positionText': '3', 'points': '4', 'Driver': {'driverId': 'rosier', 'url': 'http://en.wikipedia.org/wiki/Louis_Rosier', 'givenName': 'Louis', 'familyName': 'Rosier', 'dateOfBirth': '1905-11-05', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '10', 'laps': '41', 'status': '+1 Lap'}, {'number': '30', 'position': '4', 'positionText': '4', 'points': '3', 'Driver': {'driverId': 'bira', 'url': 'http://en.wikipedia.org/wiki/Prince_Bira', 'givenName': 'Prince', 'familyName': 'Bira', 'dateOfBirth': '1914-07-15', 'nationality': 'Thai'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '8', 'laps': '40', 'status': '+2 Laps'}, {'number': '34', 'position': '5', 'positionText': '5', 'points': '2', 'Driver': {'driverId': 'bonetto', 'url': 'http://en.wikipedia.org/wiki/Felice_Bonetto', 'givenName': 'Felice', 'familyName': 'Bonetto', 'dateOfBirth': '1903-06-09', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '12', 'laps': '40', 'status': '+2 Laps'}, {'number': '32', 'position': '6', 'positionText': '6', 'points': '0', 'Driver': {'driverId': 'graffenried', 'url': 'http://en.wikipedia.org/wiki/Toulo_de_Graffenried', 'givenName': 'Toulo', 'familyName': 'de Graffenried', 'dateOfBirth': '1914-05-18', 'nationality': 'Swiss'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '11', 'laps': '40', 'status': '+2 Laps'}, {'number': '2', 'position': '7', 'positionText': '7', 'points': '0', 'Driver': {'driverId': 'pagani', 'url': 'http://en.wikipedia.org/wiki/Nello_Pagani', 'givenName': 'Nello', 'familyName': 'Pagani', 'dateOfBirth': '1911-10-11', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '15', 'laps': '39', 'status': '+3 Laps'}, {'number': '44', 'position': '8', 'positionText': '8', 'points': '0', 'Driver': {'driverId': 'schell', 'url': 'http://en.wikipedia.org/wiki/Harry_Schell', 'givenName': 'Harry', 'familyName': 'Schell', 'dateOfBirth': '1921-06-29', 'nationality': 'American'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '18', 'laps': '39', 'status': '+3 Laps'}, {'number': '26', 'position': '9', 'positionText': '9', 'points': '0', 'Driver': {'driverId': 'chiron', 'url': 'http://en.wikipedia.org/wiki/Louis_Chiron', 'givenName': 'Louis', 'familyName': 'Chiron', 'dateOfBirth': '1899-08-03', 'nationality': 'Monegasque'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '16', 'laps': '39', 'status': '+3 Laps'}, {'number': '4', 'position': '10', 'positionText': '10', 'points': '0', 'Driver': {'driverId': 'claes', 'url': 'http://en.wikipedia.org/wiki/Johnny_Claes', 'givenName': 'Johnny', 'familyName': 'Claes', 'dateOfBirth': '1916-08-11', 'nationality': 'Belgian'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '14', 'laps': '38', 'status': '+4 Laps'}, {'number': '40', 'position': '11', 'positionText': '11', 'points': '0', 'Driver': {'driverId': 'branca', 'url': 'http://en.wikipedia.org/wiki/Toni_Branca', 'givenName': 'Toni', 'familyName': 'Branca', 'dateOfBirth': '1916-09-15', 'nationality': 'Swiss'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '17', 'laps': '35', 'status': '+7 Laps'}, {'number': '14', 'position': '12', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'fangio', 'url': 'http://en.wikipedia.org/wiki/Juan_Manuel_Fangio', 'givenName': 'Juan', 'familyName': 'Fangio', 'dateOfBirth': '1911-06-24', 'nationality': 'Argentine'}, 'Constructor': {'constructorId': 'alfa', 'url': 'http://en.wikipedia.org/wiki/Alfa_Romeo_in_Formula_One', 'name': 'Alfa Romeo', 'nationality': 'Swiss'}, 'grid': '1', 'laps': '32', 'status': 'Engine'}, {'number': '42', 'position': '13', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'etancelin', 'url': 'http://en.wikipedia.org/wiki/Philippe_%C3%89tancelin', 'givenName': 'Philippe', 'familyName': 'Étancelin', 'dateOfBirth': '1896-12-28', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '6', 'laps': '25', 'status': 'Gearbox'}, {'number': '8', 'position': '14', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'martin', 'url': 'http://en.wikipedia.org/wiki/Eug%C3%A8ne_Martin', 'givenName': 'Eugène', 'familyName': 'Martin', 'dateOfBirth': '1915-03-24', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '9', 'laps': '19', 'status': 'Accident'}, {'number': '20', 'position': '15', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'sommer', 'url': 'http://en.wikipedia.org/wiki/Raymond_Sommer', 'givenName': 'Raymond', 'familyName': 'Sommer', 'dateOfBirth': '1906-08-31', 'nationality': 'French'}, 'Constructor': {'constructorId': 'ferrari', 'url': 'http://en.wikipedia.org/wiki/Scuderia_Ferrari', 'name': 'Ferrari', 'nationality': 'Italian'}, 'grid': '13', 'laps': '19', 'status': 'Suspension'}, {'number': '22', 'position': '16', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'villoresi', 'url': 'http://en.wikipedia.org/wiki/Luigi_Villoresi', 'givenName': 'Luigi', 'familyName': 'Villoresi', 'dateOfBirth': '1909-05-16', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'ferrari', 'url': 'http://en.wikipedia.org/wiki/Scuderia_Ferrari', 'name': 'Ferrari', 'nationality': 'Italian'}, 'grid': '4', 'laps': '9', 'status': 'Engine'}, {'number': '18', 'position': '17', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'ascari', 'url': 'http://en.wikipedia.org/wiki/Alberto_Ascari', 'givenName': 'Alberto', 'familyName': 'Ascari', 'dateOfBirth': '1918-07-13', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'ferrari', 'url': 'http://en.wikipedia.org/wiki/Scuderia_Ferrari', 'name': 'Ferrari', 'nationality': 'Italian'}, 'grid': '5', 'laps': '4', 'status': 'Oil pump'}, {'number': '6', 'position': '18', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'cabantous', 'url': 'http://en.wikipedia.org/wiki/Yves_Giraud_Cabantous', 'givenName': 'Yves', 'familyName': 'Cabantous', 'dateOfBirth': '1904-10-08', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '7', 'laps': '0', 'status': 'Accident'}]\n919                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          [{'number': '10', 'position': '1', 'positionText': '1', 'points': '8', 'Driver': {'driverId': 'fangio', 'url': 'http://en.wikipedia.org/wiki/Juan_Manuel_Fangio', 'givenName': 'Juan', 'familyName': 'Fangio', 'dateOfBirth': '1911-06-24', 'nationality': 'Argentine'}, 'Constructor': {'constructorId': 'alfa', 'url': 'http://en.wikipedia.org/wiki/Alfa_Romeo_in_Formula_One', 'name': 'Alfa Romeo', 'nationality': 'Swiss'}, 'grid': '2', 'laps': '35', 'status': 'Finished', 'Time': {'millis': '10046000', 'time': '2:47:26.0'}}, {'number': '12', 'position': '2', 'positionText': '2', 'points': '6', 'Driver': {'driverId': 'fagioli', 'url': 'http://en.wikipedia.org/wiki/Luigi_Fagioli', 'givenName': 'Luigi', 'familyName': 'Fagioli', 'dateOfBirth': '1898-06-09', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'alfa', 'url': 'http://en.wikipedia.org/wiki/Alfa_Romeo_in_Formula_One', 'name': 'Alfa Romeo', 'nationality': 'Swiss'}, 'grid': '3', 'laps': '35', 'status': 'Finished', 'Time': {'millis': '10060000', 'time': '+14.0'}}, {'number': '14', 'position': '3', 'positionText': '3', 'points': '4', 'Driver': {'driverId': 'rosier', 'url': 'http://en.wikipedia.org/wiki/Louis_Rosier', 'givenName': 'Louis', 'familyName': 'Rosier', 'dateOfBirth': '1905-11-05', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '8', 'laps': '35', 'status': 'Finished', 'Time': {'millis': '10185000', 'time': '+2:19.0'}}, {'number': '8', 'position': '4', 'positionText': '4', 'points': '4', 'Driver': {'driverId': 'farina', 'url': 'http://en.wikipedia.org/wiki/Nino_Farina', 'givenName': 'Nino', 'familyName': 'Farina', 'dateOfBirth': '1906-10-30', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'alfa', 'url': 'http://en.wikipedia.org/wiki/Alfa_Romeo_in_Formula_One', 'name': 'Alfa Romeo', 'nationality': 'Swiss'}, 'grid': '1', 'laps': '35', 'status': 'Finished', 'Time': {'millis': '10291000', 'time': '+4:05.0'}}, {'number': '4', 'position': '5', 'positionText': '5', 'points': '2', 'Driver': {'driverId': 'ascari', 'url': 'http://en.wikipedia.org/wiki/Alberto_Ascari', 'givenName': 'Alberto', 'familyName': 'Ascari', 'dateOfBirth': '1918-07-13', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'ferrari', 'url': 'http://en.wikipedia.org/wiki/Scuderia_Ferrari', 'name': 'Ferrari', 'nationality': 'Italian'}, 'grid': '7', 'laps': '34', 'status': '+1 Lap'}, {'number': '2', 'position': '6', 'positionText': '6', 'points': '0', 'Driver': {'driverId': 'villoresi', 'url': 'http://en.wikipedia.org/wiki/Luigi_Villoresi', 'givenName': 'Luigi', 'familyName': 'Villoresi', 'dateOfBirth': '1909-05-16', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'ferrari', 'url': 'http://en.wikipedia.org/wiki/Scuderia_Ferrari', 'name': 'Ferrari', 'nationality': 'Italian'}, 'grid': '4', 'laps': '33', 'status': '+2 Laps'}, {'number': '22', 'position': '7', 'positionText': '7', 'points': '0', 'Driver': {'driverId': 'levegh', 'url': 'http://en.wikipedia.org/wiki/Pierre_Levegh', 'givenName': 'Pierre', 'familyName': 'Levegh', 'dateOfBirth': '1905-12-22', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '10', 'laps': '33', 'status': '+2 Laps'}, {'number': '24', 'position': '8', 'positionText': '8', 'points': '0', 'Driver': {'driverId': 'claes', 'url': 'http://en.wikipedia.org/wiki/Johnny_Claes', 'givenName': 'Johnny', 'familyName': 'Claes', 'dateOfBirth': '1916-08-11', 'nationality': 'Belgian'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '14', 'laps': '22', 'status': '+3 Laps'}, {'number': '26', 'position': '9', 'positionText': '9', 'points': '0', 'Driver': {'driverId': 'crossley', 'url': 'http://en.wikipedia.org/wiki/Geoff_Crossley', 'givenName': 'Geoff', 'familyName': 'Crossley', 'dateOfBirth': '1921-05-11', 'nationality': 'British'}, 'Constructor': {'constructorId': 'alta', 'url': 'http://en.wikipedia.org/wiki/Alta_auto_racing_team', 'name': 'Alta', 'nationality': 'British'}, 'grid': '12', 'laps': '30', 'status': '+5 Laps'}, {'number': '30', 'position': '10', 'positionText': '10', 'points': '0', 'Driver': {'driverId': 'branca', 'url': 'http://en.wikipedia.org/wiki/Toni_Branca', 'givenName': 'Toni', 'familyName': 'Branca', 'dateOfBirth': '1916-09-15', 'nationality': 'Swiss'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '11', 'laps': '29', 'status': '+6 Laps'}, {'number': '20', 'position': '11', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'chaboud', 'url': 'http://en.wikipedia.org/wiki/Eug%C3%A8ne_Chaboud', 'givenName': 'Eugène', 'familyName': 'Chaboud', 'dateOfBirth': '1907-04-12', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '13', 'laps': '22', 'status': 'Oil pipe'}, {'number': '6', 'position': '12', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'sommer', 'url': 'http://en.wikipedia.org/wiki/Raymond_Sommer', 'givenName': 'Raymond', 'familyName': 'Sommer', 'dateOfBirth': '1906-08-31', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '5', 'laps': '20', 'status': 'Oil pressure'}, {'number': '16', 'position': '13', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'etancelin', 'url': 'http://en.wikipedia.org/wiki/Philippe_%C3%89tancelin', 'givenName': 'Philippe', 'familyName': 'Étancelin', 'dateOfBirth': '1896-12-28', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '6', 'laps': '15', 'status': 'Overheating'}, {'number': '18', 'position': '14', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'cabantous', 'url': 'http://en.wikipedia.org/wiki/Yves_Giraud_Cabantous', 'givenName': 'Yves', 'familyName': 'Cabantous', 'dateOfBirth': '1904-10-08', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '9', 'laps': '2', 'status': 'Oil pipe'}]\n731                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   [{'number': '6', 'position': '1', 'positionText': '1', 'points': '9', 'Driver': {'driverId': 'fangio', 'url': 'http://en.wikipedia.org/wiki/Juan_Manuel_Fangio', 'givenName': 'Juan', 'familyName': 'Fangio', 'dateOfBirth': '1911-06-24', 'nationality': 'Argentine'}, 'Constructor': {'constructorId': 'alfa', 'url': 'http://en.wikipedia.org/wiki/Alfa_Romeo_in_Formula_One', 'name': 'Alfa Romeo', 'nationality': 'Swiss'}, 'grid': '1', 'laps': '64', 'status': 'Finished', 'Time': {'millis': '10672800', 'time': '2:57:52.8'}}, {'number': '4', 'position': '2', 'positionText': '2', 'points': '6', 'Driver': {'driverId': 'fagioli', 'url': 'http://en.wikipedia.org/wiki/Luigi_Fagioli', 'givenName': 'Luigi', 'familyName': 'Fagioli', 'dateOfBirth': '1898-06-09', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'alfa', 'url': 'http://en.wikipedia.org/wiki/Alfa_Romeo_in_Formula_One', 'name': 'Alfa Romeo', 'nationality': 'Swiss'}, 'grid': '3', 'laps': '64', 'status': 'Finished', 'Time': {'millis': '10698500', 'time': '+25.7'}}, {'number': '14', 'position': '3', 'positionText': '3', 'points': '4', 'Driver': {'driverId': 'whitehead', 'url': 'http://en.wikipedia.org/wiki/Peter_Whitehead_(racing_driver)', 'givenName': 'Peter', 'familyName': 'Whitehead', 'dateOfBirth': '1914-11-12', 'nationality': 'British'}, 'Constructor': {'constructorId': 'ferrari', 'url': 'http://en.wikipedia.org/wiki/Scuderia_Ferrari', 'name': 'Ferrari', 'nationality': 'Italian'}, 'grid': '18', 'laps': '61', 'status': '+3 Laps'}, {'number': '44', 'position': '4', 'positionText': '4', 'points': '3', 'Driver': {'driverId': 'manzon', 'url': 'http://en.wikipedia.org/wiki/Robert_Manzon', 'givenName': 'Robert', 'familyName': 'Manzon', 'dateOfBirth': '1917-04-12', 'nationality': 'French'}, 'Constructor': {'constructorId': 'simca', 'url': 'http://en.wikipedia.org/wiki/Simca', 'name': 'Simca', 'nationality': 'French'}, 'grid': '12', 'laps': '61', 'status': '+3 Laps'}, {'number': '16', 'position': '5', 'positionText': '5', 'points': '1', 'Driver': {'driverId': 'etancelin', 'url': 'http://en.wikipedia.org/wiki/Philippe_%C3%89tancelin', 'givenName': 'Philippe', 'familyName': 'Étancelin', 'dateOfBirth': '1896-12-28', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '4', 'laps': '59', 'status': '+5 Laps'}, {'number': '16', 'position': '5', 'positionText': '5', 'points': '1', 'Driver': {'driverId': 'chaboud', 'url': 'http://en.wikipedia.org/wiki/Eug%C3%A8ne_Chaboud', 'givenName': 'Eugène', 'familyName': 'Chaboud', 'dateOfBirth': '1907-04-12', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '4', 'laps': '59', 'status': '+5 Laps'}, {'number': '26', 'position': '6', 'positionText': '6', 'points': '0', 'Driver': {'driverId': 'pozzi', 'url': 'http://en.wikipedia.org/wiki/Charles_Pozzi', 'givenName': 'Charles', 'familyName': 'Pozzi', 'dateOfBirth': '1909-08-27', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '15', 'laps': '56', 'status': '+8 Laps'}, {'number': '26', 'position': '6', 'positionText': '6', 'points': '0', 'Driver': {'driverId': 'rosier', 'url': 'http://en.wikipedia.org/wiki/Louis_Rosier', 'givenName': 'Louis', 'familyName': 'Rosier', 'dateOfBirth': '1905-11-05', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '15', 'laps': '56', 'status': '+8 Laps'}, {'number': '2', 'position': '7', 'positionText': '7', 'points': '0', 'Driver': {'driverId': 'farina', 'url': 'http://en.wikipedia.org/wiki/Nino_Farina', 'givenName': 'Nino', 'familyName': 'Farina', 'dateOfBirth': '1906-10-30', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'alfa', 'url': 'http://en.wikipedia.org/wiki/Alfa_Romeo_in_Formula_One', 'name': 'Alfa Romeo', 'nationality': 'Swiss'}, 'grid': '2', 'laps': '55', 'status': 'Fuel pump'}, {'number': '18', 'position': '8', 'positionText': '8', 'points': '0', 'Driver': {'driverId': 'cabantous', 'url': 'http://en.wikipedia.org/wiki/Yves_Giraud_Cabantous', 'givenName': 'Yves', 'familyName': 'Cabantous', 'dateOfBirth': '1904-10-08', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '5', 'laps': '52', 'status': '+12 Laps'}, {'number': '22', 'position': '9', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'levegh', 'url': 'http://en.wikipedia.org/wiki/Pierre_Levegh', 'givenName': 'Pierre', 'familyName': 'Levegh', 'dateOfBirth': '1905-12-22', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '9', 'laps': '36', 'status': 'Engine'}, {'number': '40', 'position': '10', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'bonetto', 'url': 'http://en.wikipedia.org/wiki/Felice_Bonetto', 'givenName': 'Felice', 'familyName': 'Bonetto', 'dateOfBirth': '1903-06-09', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '10', 'laps': '14', 'status': 'Engine'}, {'number': '42', 'position': '11', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'claes', 'url': 'http://en.wikipedia.org/wiki/Johnny_Claes', 'givenName': 'Johnny', 'familyName': 'Claes', 'dateOfBirth': '1916-08-11', 'nationality': 'Belgian'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '14', 'laps': '11', 'status': 'Overheating'}, {'number': '20', 'position': '12', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'rosier', 'url': 'http://en.wikipedia.org/wiki/Louis_Rosier', 'givenName': 'Louis', 'familyName': 'Rosier', 'dateOfBirth': '1905-11-05', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '6', 'laps': '10', 'status': 'Overheating'}, {'number': '32', 'position': '13', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'reg_parnell', 'url': 'http://en.wikipedia.org/wiki/Reg_Parnell', 'givenName': 'Reg', 'familyName': 'Parnell', 'dateOfBirth': '1911-07-02', 'nationality': 'British'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '11', 'laps': '9', 'status': 'Engine'}, {'number': '28', 'position': '14', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'rol', 'url': 'http://en.wikipedia.org/wiki/Franco_Rol', 'givenName': 'Franco', 'familyName': 'Rol', 'dateOfBirth': '1908-06-05', 'nationality': 'Italian'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '7', 'laps': '6', 'status': 'Engine'}, {'number': '30', 'position': '15', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'chiron', 'url': 'http://en.wikipedia.org/wiki/Louis_Chiron', 'givenName': 'Louis', 'familyName': 'Chiron', 'dateOfBirth': '1899-08-03', 'nationality': 'Monegasque'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '13', 'laps': '6', 'status': 'Engine'}, {'number': '34', 'position': '16', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'hampshire', 'url': 'http://en.wikipedia.org/wiki/David_Hampshire', 'givenName': 'David', 'familyName': 'Hampshire', 'dateOfBirth': '1917-12-29', 'nationality': 'British'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '17', 'laps': '5', 'status': 'Engine'}, {'number': '12', 'position': '17', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'sommer', 'url': 'http://en.wikipedia.org/wiki/Raymond_Sommer', 'givenName': 'Raymond', 'familyName': 'Sommer', 'dateOfBirth': '1906-08-31', 'nationality': 'French'}, 'Constructor': {'constructorId': 'lago', 'url': 'http://en.wikipedia.org/wiki/Talbot-Lago', 'name': 'Talbot-Lago', 'nationality': 'French'}, 'grid': '16', 'laps': '4', 'status': 'Overheating'}, {'number': '36', 'position': '18', 'positionText': 'R', 'points': '0', 'Driver': {'driverId': 'gonzalez', 'url': 'http://en.wikipedia.org/wiki/Jos%C3%A9_Froil%C3%A1n_Gonz%C3%A1lez', 'givenName': 'José Froilán', 'familyName': 'González', 'dateOfBirth': '1922-10-05', 'nationality': 'Argentine'}, 'Constructor': {'constructorId': 'maserati', 'url': 'http://en.wikipedia.org/wiki/Maserati', 'name': 'Maserati', 'nationality': 'Italian'}, 'grid': '8', 'laps': '3', 'status': 'Engine'}]\n    time                  circuitName\n812 <NA>          Silverstone Circuit\n497 <NA>            Circuit de Monaco\n326 <NA>  Indianapolis Motor Speedway\n102 <NA>           Circuit Bremgarten\n919 <NA> Circuit de Spa-Francorchamps\n731 <NA>                  Reims-Gueux\n                                                                                 Location\n812    {'lat': '52.0786', 'long': '-1.01694', 'locality': 'Silverstone', 'country': 'UK'}\n497 {'lat': '43.7347', 'long': '7.42056', 'locality': 'Monte-Carlo', 'country': 'Monaco'}\n326   {'lat': '39.795', 'long': '-86.2347', 'locality': 'Indianapolis', 'country': 'USA'}\n102   {'lat': '46.9589', 'long': '7.40194', 'locality': 'Bern', 'country': 'Switzerland'}\n919        {'lat': '50.4372', 'long': '5.97139', 'locality': 'Spa', 'country': 'Belgium'}\n731       {'lat': '49.2542', 'long': '3.93083', 'locality': 'Reims', 'country': 'France'}\n\n\n\n\nCode\nhead(driver_df)\n\n\n# A tibble: 6 × 9\n  driverId driverRef  number code  forename surname    dob        nation…¹ url  \n     <dbl> <chr>      <chr>  <chr> <chr>    <chr>      <date>     <chr>    <chr>\n1        1 hamilton   \"44\"   HAM   Lewis    Hamilton   1985-01-07 British  http…\n2        2 heidfeld   \"\\\\N\"  HEI   Nick     Heidfeld   1977-05-10 German   http…\n3        3 rosberg    \"6\"    ROS   Nico     Rosberg    1985-06-27 German   http…\n4        4 alonso     \"14\"   ALO   Fernando Alonso     1981-07-29 Spanish  http…\n5        5 kovalainen \"\\\\N\"  KOV   Heikki   Kovalainen 1981-10-19 Finnish  http…\n6        6 nakajima   \"\\\\N\"  NAK   Kazuki   Nakajima   1985-01-11 Japanese http…\n# … with abbreviated variable name ¹​nationality\n\n\nUnzipping Dictionaries from Race results column:\n\n\nCode\ndf$Results_Position_1 =  word(df$Results,1,sep = fixed(\", {'number'\"))\ndf$Results_Position_2 =  word(df$Results,2,sep = fixed(\", {'number'\"))\ndf$Results_Position_3 =  word(df$Results,3,sep = fixed(\", {'number'\"))\ndf$Results_Position_4 =  word(df$Results,4,sep = fixed(\", {'number'\"))\ndf$Results_Position_5 =  word(df$Results,5,sep = fixed(\", {'number'\"))\n\n\n\n\nCode\ndf$Driver_Position_1 = word(df$Results_Position_1,2,sep = fixed(\",\"))\ndf$Driver_Position_2 = word(df$Results_Position_2,2,sep = fixed(\",\"))\ndf$Driver_Position_3 = word(df$Results_Position_3,2,sep = fixed(\",\"))\ndf$Driver_Position_4 = word(df$Results_Position_4,2,sep = fixed(\",\"))\ndf$Driver_Position_5 = word(df$Results_Position_5,2,sep = fixed(\",\"))\n\ndf$Driver_Position_1 = word(df$Driver_Position_1,2,sep = fixed(\": \"))\ndf$Driver_Position_2 = word(df$Driver_Position_2,2,sep = fixed(\": \"))\ndf$Driver_Position_3 = word(df$Driver_Position_3,2,sep = fixed(\": \"))\ndf$Driver_Position_4 = word(df$Driver_Position_4,2,sep = fixed(\": \"))\ndf$Driver_Position_5 = word(df$Driver_Position_5,2,sep = fixed(\": \"))\n\ndf$Driver_Points_1 = word(df$Results_Position_1,4,sep = fixed(\",\"))\ndf$Driver_Points_2 = word(df$Results_Position_2,4,sep = fixed(\",\"))\ndf$Driver_Points_3 = word(df$Results_Position_3,4,sep = fixed(\",\"))\ndf$Driver_Points_4 = word(df$Results_Position_4,4,sep = fixed(\",\"))\ndf$Driver_Points_5 = word(df$Results_Position_5,4,sep = fixed(\",\"))\n\ndf$Driver_Points_1 = word(df$Driver_Points_1,2,sep = fixed(\": \"))\ndf$Driver_Points_2 = word(df$Driver_Points_2,2,sep = fixed(\": \"))\ndf$Driver_Points_3 = word(df$Driver_Points_3,2,sep = fixed(\": \"))\ndf$Driver_Points_4 = word(df$Driver_Points_4,2,sep = fixed(\": \"))\ndf$Driver_Points_5 = word(df$Driver_Points_5,2,sep = fixed(\": \"))\n\ndf$Driver_Info_1 = word(df$Results_Position_1,2,sep = fixed(\"'Driver': {\"))\ndf$Driver_Info_2 = word(df$Results_Position_2,2,sep = fixed(\"'Driver': {\"))\ndf$Driver_Info_3 = word(df$Results_Position_3,2,sep = fixed(\"'Driver': {\"))\ndf$Driver_Info_4 = word(df$Results_Position_4,2,sep = fixed(\"'Driver': {\"))\ndf$Driver_Info_5 = word(df$Results_Position_5,2,sep = fixed(\"'Driver': {\"))\n\ndf$driverRef_1 = word(df$Driver_Info_1,1,sep = fixed(\",\"))\ndf$driverRef_2 = word(df$Driver_Info_2,1,sep = fixed(\",\"))\ndf$driverRef_3 = word(df$Driver_Info_3,1,sep = fixed(\",\"))\ndf$driverRef_4 = word(df$Driver_Info_4,1,sep = fixed(\",\"))\ndf$driverRef_5 = word(df$Driver_Info_5,1,sep = fixed(\",\"))\n\ndf$driverRef_1 = word(df$driverRef_1,2,sep = fixed(\": \"))\ndf$driverRef_2 = word(df$driverRef_2,2,sep = fixed(\": \"))\ndf$driverRef_3 = word(df$driverRef_3,2,sep = fixed(\": \"))\ndf$driverRef_4 = word(df$driverRef_4,2,sep = fixed(\": \"))\ndf$driverRef_5 = word(df$driverRef_5,2,sep = fixed(\": \"))\n\ndf$Constructor_Info_1 = word(df$Results_Position_1,2,sep = fixed(\"'Constructor': {\"))\ndf$Constructor_Info_2 = word(df$Results_Position_2,2,sep = fixed(\"'Constructor': {\"))\ndf$Constructor_Info_3 = word(df$Results_Position_3,2,sep = fixed(\"'Constructor': {\"))\ndf$Constructor_Info_4 = word(df$Results_Position_4,2,sep = fixed(\"'Constructor': {\"))\ndf$Constructor_Info_5 = word(df$Results_Position_5,2,sep = fixed(\"'Constructor': {\"))\n\ndf$constructorRef_1 = word(df$Constructor_Info_1,1,sep = fixed(\",\"))\ndf$constructorRef_2 = word(df$Constructor_Info_2,1,sep = fixed(\",\"))\ndf$constructorRef_3 = word(df$Constructor_Info_3,1,sep = fixed(\",\"))\ndf$constructorRef_4 = word(df$Constructor_Info_4,1,sep = fixed(\",\"))\ndf$constructorRef_5 = word(df$Constructor_Info_5,1,sep = fixed(\",\"))\n\ndf$constructorRef_1 = word(df$constructorRef_1,2,sep = fixed(\": \"))\ndf$constructorRef_2 = word(df$constructorRef_2,2,sep = fixed(\": \"))\ndf$constructorRef_3 = word(df$constructorRef_3,2,sep = fixed(\": \"))\ndf$constructorRef_4 = word(df$constructorRef_4,2,sep = fixed(\": \"))\ndf$constructorRef_5 = word(df$constructorRef_5,2,sep = fixed(\": \"))\n\ndf$otherinfo_1 = word(df$Results_Position_1,2,sep = fixed(\"'grid'\"))\ndf$otherinfo_2 = word(df$Results_Position_2,2,sep = fixed(\"'grid'\"))\ndf$otherinfo_3 = word(df$Results_Position_3,2,sep = fixed(\"'grid'\"))\ndf$otherinfo_4 = word(df$Results_Position_4,2,sep = fixed(\"'grid'\"))\ndf$otherinfo_5 = word(df$Results_Position_5,2,sep = fixed(\"'grid'\"))\n\ndf$grid_pos_1 = word(df$otherinfo_1,1,sep = fixed(\",\"))\ndf$grid_pos_2 = word(df$otherinfo_2,1,sep = fixed(\",\"))\ndf$grid_pos_3 = word(df$otherinfo_3,1,sep = fixed(\",\"))\ndf$grid_pos_4 = word(df$otherinfo_4,1,sep = fixed(\",\"))\ndf$grid_pos_5 = word(df$otherinfo_5,1,sep = fixed(\",\"))\n\ndf$completed_laps_1 = word(df$otherinfo_1,2,sep = fixed(\",\"))\ndf$completed_laps_2 = word(df$otherinfo_2,2,sep = fixed(\",\"))\ndf$completed_laps_3 = word(df$otherinfo_3,2,sep = fixed(\",\"))\ndf$completed_laps_4 = word(df$otherinfo_4,2,sep = fixed(\",\"))\ndf$completed_laps_5 = word(df$otherinfo_5,2,sep = fixed(\",\"))\n\ndf$race_status_1 = word(df$otherinfo_1,3,sep = fixed(\",\"))\ndf$race_status_2 = word(df$otherinfo_2,3,sep = fixed(\",\"))\ndf$race_status_3 = word(df$otherinfo_3,3,sep = fixed(\",\"))\ndf$race_status_4 = word(df$otherinfo_4,3,sep = fixed(\",\"))\ndf$race_status_5 = word(df$otherinfo_5,3,sep = fixed(\",\"))\n\ndf$race_time_1 = word(df$otherinfo_1,2,sep = fixed(\"'Time':\"))\ndf$race_time_2 = word(df$otherinfo_2,2,sep = fixed(\"'Time':\"))\ndf$race_time_3 = word(df$otherinfo_3,2,sep = fixed(\"'Time':\"))\ndf$race_time_4 = word(df$otherinfo_4,2,sep = fixed(\"'Time':\"))\ndf$race_time_5 = word(df$otherinfo_5,2,sep = fixed(\"'Time':\"))\n\ndf$grid_pos_1 = word(df$grid_pos_1,2,sep = fixed(\": \"))\ndf$grid_pos_2 = word(df$grid_pos_2,2,sep = fixed(\": \"))\ndf$grid_pos_3 = word(df$grid_pos_3,2,sep = fixed(\": \"))\ndf$grid_pos_4 = word(df$grid_pos_4,2,sep = fixed(\": \"))\ndf$grid_pos_5 = word(df$grid_pos_5,2,sep = fixed(\": \"))\n\ndf$completed_laps_1 = word(df$completed_laps_1,2,sep = fixed(\": \"))\ndf$completed_laps_2 = word(df$completed_laps_2,2,sep = fixed(\": \"))\ndf$completed_laps_3 = word(df$completed_laps_3,2,sep = fixed(\": \"))\ndf$completed_laps_4 = word(df$completed_laps_4,2,sep = fixed(\": \"))\ndf$completed_laps_5 = word(df$completed_laps_5,2,sep = fixed(\": \"))\n\ndf$race_status_1 = word(df$race_status_1,2,sep = fixed(\": \"))\ndf$race_status_2 = word(df$race_status_2,2,sep = fixed(\": \"))\ndf$race_status_3 = word(df$race_status_3,2,sep = fixed(\": \"))\ndf$race_status_4 = word(df$race_status_4,2,sep = fixed(\": \"))\ndf$race_status_5 = word(df$race_status_5,2,sep = fixed(\": \"))\n\n\nDropping the columns created above:\n\n\nCode\ndrop = c(\"Driver_Info_1\", \"Driver_Info_2\", \"Driver_Info_3\", \"Driver_Info_4\", \"Driver_Info_5\", \"Constructor_Info_1\", \"Constructor_Info_2\", \"Constructor_Info_3\", \"Constructor_Info_4\", \"Constructor_Info_5\", \"Driver_Info_1\", \"Driver_Info_1\", \"Driver_Info_1\", \"Driver_Info_1\", \"Driver_Info_1\", \"Circuit\", \"Results\", \"otherinfo_1\", \"otherinfo_2\", \"otherinfo_3\", \"otherinfo_4\", \"otherinfo_5\", \"Results_Position_1\", \"Results_Position_2\", \"Results_Position_3\", \"Results_Position_4\", \"Results_Position_5\", \"time\")\ndf = df[,!(names(df) %in% drop)]\n\n\n\n\nCode\nnum_cols = c('Driver_Position_1', 'Driver_Position_2', 'Driver_Position_3', 'Driver_Position_4', 'Driver_Position_5',\n             'Driver_Points_1', 'Driver_Points_2', 'Driver_Points_3', 'Driver_Points_4', 'Driver_Points_5',\n             'grid_pos_1', 'grid_pos_2', 'grid_pos_3', 'grid_pos_4', 'grid_pos_5', \n             'completed_laps_1', 'completed_laps_2', 'completed_laps_3', 'completed_laps_4', 'completed_laps_5')\n\n\n\n\nCode\nfn <- function(x) gsub(\"'\", \"\", x)\nfncol <- colwise(fn, .cols=num_cols)\ndf[, num_cols] = fncol(df)\n\n\nConverting columns to numeric:\n\n\nCode\ndf[num_cols] = sapply(df[num_cols], as.numeric)\n\n\n\n\nCode\ncolnames(df)\n\n\n [1] \"circuitId\"         \"season\"            \"round\"            \n [4] \"raceName\"          \"date\"              \"circuitName\"      \n [7] \"Location\"          \"Driver_Position_1\" \"Driver_Position_2\"\n[10] \"Driver_Position_3\" \"Driver_Position_4\" \"Driver_Position_5\"\n[13] \"Driver_Points_1\"   \"Driver_Points_2\"   \"Driver_Points_3\"  \n[16] \"Driver_Points_4\"   \"Driver_Points_5\"   \"driverRef_1\"      \n[19] \"driverRef_2\"       \"driverRef_3\"       \"driverRef_4\"      \n[22] \"driverRef_5\"       \"constructorRef_1\"  \"constructorRef_2\" \n[25] \"constructorRef_3\"  \"constructorRef_4\"  \"constructorRef_5\" \n[28] \"grid_pos_1\"        \"grid_pos_2\"        \"grid_pos_3\"       \n[31] \"grid_pos_4\"        \"grid_pos_5\"        \"completed_laps_1\" \n[34] \"completed_laps_2\"  \"completed_laps_3\"  \"completed_laps_4\" \n[37] \"completed_laps_5\"  \"race_status_1\"     \"race_status_2\"    \n[40] \"race_status_3\"     \"race_status_4\"     \"race_status_5\"    \n[43] \"race_time_1\"       \"race_time_2\"       \"race_time_3\"      \n[46] \"race_time_4\"       \"race_time_5\"      \n\n\n\n\nCode\ndf2 = df[rep(seq_len(nrow(df)), each = 5), ]\nrownames(df2) <- 1:nrow(df2)\nhead(df2)\n\n\n    circuitId season round           raceName       date         circuitName\n1 silverstone   1950     1 British Grand Prix 1950-05-13 Silverstone Circuit\n2 silverstone   1950     1 British Grand Prix 1950-05-13 Silverstone Circuit\n3 silverstone   1950     1 British Grand Prix 1950-05-13 Silverstone Circuit\n4 silverstone   1950     1 British Grand Prix 1950-05-13 Silverstone Circuit\n5 silverstone   1950     1 British Grand Prix 1950-05-13 Silverstone Circuit\n6      monaco   1950     2  Monaco Grand Prix 1950-05-21   Circuit de Monaco\n                                                                               Location\n1    {'lat': '52.0786', 'long': '-1.01694', 'locality': 'Silverstone', 'country': 'UK'}\n2    {'lat': '52.0786', 'long': '-1.01694', 'locality': 'Silverstone', 'country': 'UK'}\n3    {'lat': '52.0786', 'long': '-1.01694', 'locality': 'Silverstone', 'country': 'UK'}\n4    {'lat': '52.0786', 'long': '-1.01694', 'locality': 'Silverstone', 'country': 'UK'}\n5    {'lat': '52.0786', 'long': '-1.01694', 'locality': 'Silverstone', 'country': 'UK'}\n6 {'lat': '43.7347', 'long': '7.42056', 'locality': 'Monte-Carlo', 'country': 'Monaco'}\n  Driver_Position_1 Driver_Position_2 Driver_Position_3 Driver_Position_4\n1                 1                 2                 3                 4\n2                 1                 2                 3                 4\n3                 1                 2                 3                 4\n4                 1                 2                 3                 4\n5                 1                 2                 3                 4\n6                 1                 2                 3                 4\n  Driver_Position_5 Driver_Points_1 Driver_Points_2 Driver_Points_3\n1                 5               9               6               4\n2                 5               9               6               4\n3                 5               9               6               4\n4                 5               9               6               4\n5                 5               9               6               4\n6                 5               9               6               4\n  Driver_Points_4 Driver_Points_5 driverRef_1 driverRef_2   driverRef_3\n1               3               2    'farina'   'fagioli' 'reg_parnell'\n2               3               2    'farina'   'fagioli' 'reg_parnell'\n3               3               2    'farina'   'fagioli' 'reg_parnell'\n4               3               2    'farina'   'fagioli' 'reg_parnell'\n5               3               2    'farina'   'fagioli' 'reg_parnell'\n6               3               2    'fangio'    'ascari'      'chiron'\n  driverRef_4 driverRef_5 constructorRef_1 constructorRef_2 constructorRef_3\n1 'cabantous'    'rosier'           'alfa'           'alfa'           'alfa'\n2 'cabantous'    'rosier'           'alfa'           'alfa'           'alfa'\n3 'cabantous'    'rosier'           'alfa'           'alfa'           'alfa'\n4 'cabantous'    'rosier'           'alfa'           'alfa'           'alfa'\n5 'cabantous'    'rosier'           'alfa'           'alfa'           'alfa'\n6    'sommer'      'bira'           'alfa'        'ferrari'       'maserati'\n  constructorRef_4 constructorRef_5 grid_pos_1 grid_pos_2 grid_pos_3 grid_pos_4\n1           'lago'           'lago'          1          2          4          6\n2           'lago'           'lago'          1          2          4          6\n3           'lago'           'lago'          1          2          4          6\n4           'lago'           'lago'          1          2          4          6\n5           'lago'           'lago'          1          2          4          6\n6        'ferrari'       'maserati'          1          7          8          9\n  grid_pos_5 completed_laps_1 completed_laps_2 completed_laps_3\n1          9               70               70               70\n2          9               70               70               70\n3          9               70               70               70\n4          9               70               70               70\n5          9               70               70               70\n6         15              100               99               98\n  completed_laps_4 completed_laps_5 race_status_1 race_status_2 race_status_3\n1               68               68    'Finished'    'Finished'    'Finished'\n2               68               68    'Finished'    'Finished'    'Finished'\n3               68               68    'Finished'    'Finished'    'Finished'\n4               68               68    'Finished'    'Finished'    'Finished'\n5               68               68    'Finished'    'Finished'    'Finished'\n6               97               95    'Finished'     '+1 Lap'}    '+2 Laps'}\n  race_status_4 race_status_5                                   race_time_1\n1    '+2 Laps'}    '+2 Laps'}   {'millis': '8003600', 'time': '2:13:23.6'}}\n2    '+2 Laps'}    '+2 Laps'}   {'millis': '8003600', 'time': '2:13:23.6'}}\n3    '+2 Laps'}    '+2 Laps'}   {'millis': '8003600', 'time': '2:13:23.6'}}\n4    '+2 Laps'}    '+2 Laps'}   {'millis': '8003600', 'time': '2:13:23.6'}}\n5    '+2 Laps'}    '+2 Laps'}   {'millis': '8003600', 'time': '2:13:23.6'}}\n6    '+3 Laps'}    '+5 Laps'}  {'millis': '11598700', 'time': '3:13:18.7'}}\n                              race_time_2\n1  {'millis': '8006200', 'time': '+2.6'}}\n2  {'millis': '8006200', 'time': '+2.6'}}\n3  {'millis': '8006200', 'time': '+2.6'}}\n4  {'millis': '8006200', 'time': '+2.6'}}\n5  {'millis': '8006200', 'time': '+2.6'}}\n6                                    <NA>\n                               race_time_3 race_time_4 race_time_5\n1  {'millis': '8055600', 'time': '+52.0'}}        <NA>        <NA>\n2  {'millis': '8055600', 'time': '+52.0'}}        <NA>        <NA>\n3  {'millis': '8055600', 'time': '+52.0'}}        <NA>        <NA>\n4  {'millis': '8055600', 'time': '+52.0'}}        <NA>        <NA>\n5  {'millis': '8055600', 'time': '+52.0'}}        <NA>        <NA>\n6                                     <NA>        <NA>        <NA>\n\n\nExporting the CSV for further cleaning:\n\n\nCode\nwrite_csv(df2, '../../data/01-modified-data/data_cleaning_R.csv', na = '') #to python\n\n\nTo be Continued in Python since R does not support handling dictionaries."
  },
  {
    "objectID": "pages/codes/Clustering.html",
    "href": "pages/codes/Clustering.html",
    "title": "Clustering Techniques for Record Data",
    "section": "",
    "text": "I have taken Record History Data from F1 archives (Ergast API) to perform various Clustering Algorithms.\nThe Data is cleaned in the previous sections.\nOverview of Data Cleaning:\n\nF1 archives have important information about races held since 1950 to the current season.\nThere are different tables of data that can be extracted such as Results of the Race, Constructor (Team) Information, Driver Information, Results of the Qualification, Season-End Results and many more.\nThese individual tables have data inside dictionaries inside every row.\nThese dictionaries can be expanded and every dictionary can be transformed into a separate table.\nValuable information from these tables are then concatenated/joined into one master table\n\nThe master table has consists of 26,941 rows and 21 feature variables and 1 label column.\nSome of the feature variables include laps in the race, grid position held, age at time of the race, history of wins in the past, history of laps completed in the past, weather of the race, points gained in the race and many more.\nThe label column is based on the position achieved by a single driver in a particular race. The position range from 1 to 20/21/22 (based on the number of drivers that races in that particular race). For simplification purposes, the positions have been grouped into 3 different sections:\n\nPodium (Top 3 finish)\nTop_10 (Positions 4-10)\nOutside Top_10 (Positions 10 and above)\n\nThe motivation behind these sections were solely based on the fact that Top 3 finish receive more points than Positions 4-10, while Positions 10 and above do not receive any points at all.\nFrom the clustering analysis, I want to see that without the label column (Un-supervised ML), do we still achieve the same number of clusters as the sections that were originally decided upon i.e., 3."
  },
  {
    "objectID": "pages/codes/Clustering.html#clustering-with-random-hyper-parameters",
    "href": "pages/codes/Clustering.html#clustering-with-random-hyper-parameters",
    "title": "Clustering Techniques for Record Data",
    "section": "Clustering with random Hyper-parameters",
    "text": "Clustering with random Hyper-parameters\nFor every algorithm that performs Clustering on a dataset, there are many hyperparameters that needs to be specified beforehand. But these hyper-parameters need to be tuned, keeping in mind our dataset. Giving random hyperparameters that do not match our dataset may result in loss of data and/or wrong datapoints being classified into clusters. Thus before hyper-parameter tuning, let’s see a k-means model with random hyperparameters as follows: - Initializer: kmeans++ - Number of clusters: 6 - Maximum iterations: 300 - Random state: 1973\n\n\nCode\nkmeans = KMeans(\n    init=\"k-means++\",\n    n_clusters=6,\n    max_iter=300,\n    random_state=1973)\n\nlabel = kmeans.fit_predict(X)\n\n\n\n\nCode\ndf['cluster_label'] = label\n\n\n\n\nCode\ndf.columns\n\n\nIndex(['season', 'round', 'driverId', 'circuitId', 'position', 'points',\n       'grid', 'laps', 'age_on_race', 'cumulative_points', 'cumulative_laps',\n       'pole_driverId', 'pole_history', 'win_driverId', 'win_history', 'label',\n       'status_Accident', 'status_Finished', 'status_Lapped',\n       'status_Mechanical_Issue', 'weather_Cloudy', 'weather_Fine',\n       'weather_Not Available', 'weather_Rainy', 'weather_Snowy',\n       'weather_Sunny', 'weather_Windy', 'stop_Five', 'stop_Four',\n       'stop_Not Available', 'stop_One', 'stop_Six', 'stop_Three', 'stop_Two',\n       'cluster_label'],\n      dtype='object')\n\n\n\n\nCode\n# plot the clusters\n\nfeatures = ['season', 'driverId' , 'round' , 'circuitId' , 'pole_driverId' , 'position' ,'points', 'grid',\n                    'laps', 'age_on_race', 'cumulative_points', 'cumulative_laps', 'pole_history', 'win_history', 'win_driverId', 'cluster_label']\n\nsns.pairplot(df[features], hue='cluster_label')\n\n\n<seaborn.axisgrid.PairGrid at 0x10791f8e0>"
  },
  {
    "objectID": "pages/codes/Clustering.html#hyper-parameter-tuning",
    "href": "pages/codes/Clustering.html#hyper-parameter-tuning",
    "title": "Clustering Techniques for Record Data",
    "section": "Hyper-parameter Tuning",
    "text": "Hyper-parameter Tuning\n\nHyper-parameters are variables that you specify while building a machine-learning model. This means that it’s the user that defines the hyper-parameters while building the model. Hyper-parameters control the learning process, while parameters are learned.\nThe performance of a model significantly depends on the value of hyperparameters. Note that there is no way to know in advance the best values for hyperparameters so ideally, we need to try all possible values to know the optimal values.\nDoing this manually could take a considerable amount of time and resources.\nHence we create functions with loops and parameters and try to find the best possible Hyper-parameter(s) based on a metric. This metric changes between algorithm to algorithm and also betwwen methods inside the algorithm. For example, in clustering silhouette method is a very commonly used method to determine hyper-parameters and it is based on maximizing the silhouette score, but there are other methods too to determine hyper-parameters such as Elbow method, Grubbs and so on.\nElbow Method is good for finding optimal hyper-parameters when the dimension of the dataset is low but for high dimension data, Silhouette method is preferred.\n\n\nSilhouette Method\n\nSimilar to Elbow Method, Silhouette method is also a method to find the optimal number of clusters or other hyper-parameters and also to validate the consistency of data inside the clusters.\nThis method calculates silhouette coefficients for each of point, and averages it out for all the samples to get the silhouette score. The set of hyper-parameters with maximum silhouette score is chosen.\nThere are 2 main terms to be known to understand silhouette scores:\n\nCohesion (Similarity to its own cluster)\nSeparation (Similarity to other clusters)\n\nThe silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The silhouette scores lie between the range [-1,1] where a value close to the upper limit shows that a point is well matched to the cluster and a value close to the lower limit shows that it isn’t. After calculating silhouette values of all the points in the data, the silhouette score can be determined by averaging out the silhouette values.\nThe silhouette coefficient, s(i), can be calculated by the following formula: where:  a(i): The average distance of that point with all other points in the same clusters and b(i): The average distance of that point with all the points in the closest cluster to its cluster; \\[ s_{i} = \\frac {b_{i} - a_{i}}{max(b_{i}, a_{i})} \\]\n\n\n\nHyper-Parameter tuning function (maximize_silhouette)\n\nThere are tons of different models that can be used to perform Clustering but for this dataset, we are focusing mainly on the 4 most used methods which are:\n\nK-Means\nDBSCAN\nAgglomerative\nBirch\n\nThe tuning function takes in the dataset (X), algorithm to be used (algo), maximum number of loops to run (nmax) and a boolean function to plot the silhouette scores against hyper-parameter (i_plot). It converts our dataset into a contiguous array and runs models with different value of hyper-parameters using a for loop and computes silhouette scores for each iteration. It then stores the optimal values of hyper-parameter, label and model and returns it out. If the i_plot parameter is set to ‘True’, the function will also a plot a graph of silhouette score v/s hyper-parameter.\n\n\n\nCode\ndef maximize_silhouette(X,algo=\"birch\",nmax=20,i_plot=False):\n\n    # PARAM\n    i_print=False\n\n    #FORCE CONTIGUOUS\n    X=np.ascontiguousarray(X) \n\n    # LOOP OVER HYPER-PARAM\n    params=[]; sil_scores=[]\n    sil_max=-10\n    \n    if algo in [\"kmeans\",\"birch\",\"ag\"]:\n        \n        for param in range(3,nmax+1):\n            \n            if(algo==\"birch\"):\n                \n                model = Birch(n_clusters=param).fit(X)\n                labels=model.predict(X)\n\n            if(algo==\"ag\"):\n                \n                model = AgglomerativeClustering(n_clusters=param).fit(X)\n                labels=model.labels_\n\n            if(algo==\"kmeans\"):\n                \n                model = KMeans(n_clusters=param, init='k-means++').fit(X)\n                labels=model.predict(X)\n\n            try:\n                \n                sil_scores.append(silhouette_score(X,labels))\n                params.append(param)\n                \n            except:\n                \n                continue \n\n            if(i_print): print(param,sil_scores[-1])\n            \n            if(sil_scores[-1]>sil_max):\n                opt_param=param\n                sil_max=sil_scores[-1]\n                opt_labels=labels\n                opt_model=model\n                \n    elif algo in [\"dbscan\"]:\n        \n        for eps in np.arange(0.2, nmax+1, 0.2):\n                \n                model = DBSCAN(eps=eps, min_samples=X.shape[1]*2).fit(X)\n                labels=model.labels_\n                \n                try:\n                        \n                    sil_scores.append(silhouette_score(X,labels))\n                    params.append(eps)\n                    \n                except:\n                            \n                    continue\n            \n                if(i_print): print(eps,sil_scores[-1])\n            \n                if(sil_scores[-1]>sil_max):\n                    opt_param=eps\n                    sil_max=sil_scores[-1]\n                    opt_labels=labels\n                    opt_model=model\n   \n    #print(\"Optimal Parameter for {} algorithm = {}\".format(algo,opt_param))\n\n    if(i_plot):\n        fig, ax = plt.subplots()\n        ax.plot(params, sil_scores, \"-o\")  \n        ax.set(xlabel='Hyper-parameter', ylabel='Silhouette')\n        plt.show()\n\n    return opt_labels, opt_param, opt_model\n\n\n\nK-Means Algorithm (Hyper-Parameter = n_clusters)\n\nK-Means Algorithm is a type of Partition-based or Centroid-based clustering.\nHow does the K-Means algorithm work?\n\nFirst we choose k random data points and assign those as centroids or cluster centres.\nThen for every data point, we see which centroid is nearest to it using some measurement method such as Euclidean (default) or Manhattan.\nWe assign the data point to that centroid.\nThe new centroi is now at the average distance of the centroid chosen before and the data point assigned to it.\nRepeat the previous 3 steps until the centroid stops changing.\nThe algorithm is said to have “converged” once there are no more changes.\n\nThe goal is to minimize the sum of the distances between the data points and the cluster centroid in order to determine which group each data point should belong to.\nK-means follows the same approach as Expectation-Maximization(EM). EM is an iterative method to find the maximum likelihood of parameters where the machine learning model depends on unobserved features. This approach consists of two steps Expectation(E) and Maximization(M) and iterates between these two.\nAdvantages:\n\nRelatively easy to understand and implement.\nScalable to large datasets.\nBetter computation cost.\nEasily warm start the assignments and positions of centroids.\n\nDisadvantages:\n\nChoosing K manually and being dependent on the initial values.\nLacks consistent results for different values of K.\nAlways tries to find circular clusters.\nCentroids get dragged due to outliers in the dataset.\nCurse of dimensionality, K is ineffective when the number of dimensions increases.\n\nFor this exercise, we are choosing the hyper-parameter to tune as n_clusters while the other hyper-parameters are set to default. The tuning function calculates the maximum silhouette score for n_cluster values from 1-10 and outputs the Silhouette v/s N-Clusters graph. The labels are then predicted and appended to our original dataset for plotting clusters using Seaborn’s Pairplot function.\n\n\n\nCode\nalgo_features = ['season', 'driverId' , 'round' , 'circuitId' , 'pole_driverId' , 'position' ,'points', 'grid',\n                    'laps', 'age_on_race', 'cumulative_points', 'cumulative_laps', 'pole_history', 'win_history', 'win_driverId']\n\nstart = time.time()\nprint('KMeans Algorithm')\n\nkmeans_labels, n_clusters, kmeans_model = maximize_silhouette(X, 'kmeans', 10, True)\nprint('The number of clusters for Kmeans algorithm is: ', len(np.unique(kmeans_labels)))\ndf['kmeans_label'] = kmeans_labels\nalgo_features.append('kmeans_label')\nprint('Cluster Plots for Kmeans algorithm')\nsns.pairplot(df[algo_features], hue='kmeans_label')\nplt.show()\nend = time.time()\nprint('Time taken for KMEANS algorithm: {} minutes'.format((end - start)/60))\n\n\nKMeans Algorithm\n\n\n\n\n\nThe number of clusters for Kmeans algorithm is:  3\nCluster Plots for Kmeans algorithm\n\n\n\n\n\nTime taken for KMEANS algorithm: 1.753977100054423 minutes\n\n\n\n\nCode\ndf['kmeans_label'].value_counts()\n\n\n0    17145\n1     7349\n2     2127\nName: kmeans_label, dtype: int64\n\n\n\n\nCode\ndf['label'].value_counts()\n\n\nOutside_Top_10    15364\nTop_10             7866\nPodium             3391\nName: label, dtype: int64\n\n\n\n\nCode\ndf['kmeans_accuracy'] = 0\nfor i in range(len(df)):\n    \n    if df['kmeans_label'][i] == 0 and df['label'][i] == 'Podium':\n        df['kmeans_accuracy'][i] = 1\n    \n    elif df['kmeans_label'][i] == 1 and df['label'][i] == 'Top_10':\n        df['kmeans_accuracy'][i] = 1\n        \n    elif df['kmeans_label'][i] == 2 and df['label'][i] == 'Outside_Top_10':\n        df['kmeans_accuracy'][i] = 1\n        \n    else:\n        df['kmeans_accuracy'][i] = 0\n\nprint('The accuracy of correct clusters formed by K-Means Algorithm = {} %'.format(df['kmeans_accuracy'].mean()*100))\n\n\nThe accuracy of correct clusters formed by K-Means Algorithm = 15.95732692235453 %\n\n\n\nResults for K-Means Algorithm\n\nThe optimal number of clusters formed came out to be 3 which is also equal to the number of labels present in the original dataset.\nBut after comparing proposed cluster labels with the original labels, around 14,800 out of 26,000 values (55.475%) are matching with each other. This is due to the fact that our dataset is historical and contains a lot of inter-dependencies between the features which leads the algorithm to not create clusters properly as it is a simple clustering algorithm. Another reason might be that the dimensionality of the data is very high and this makes it very complex.\n\n\n\n\nDBSCAN Algorithm (Hyper-Parameter = eps)\n\nDBSCAN stands for Density-Based Spatial Clustering of Applications with Noise and it is a Density Based Algorithm.\nIt works on the assumption that clusters are dense regions in space separated by regions of lower density. Densely grouped data points are grouped into a single cluster.\nBy seeing local density of data points in large spatial datasets, the clusters are identified.\nThis algorithm is robust to outliers and the number of clusters to make cannot be specified as a hyper-parameter.\nLet a given dataset of points (dataset D = {xi}), we have to partition the point into the dense region which we will call them as Clusters and sparse region which may contain noise.\nThe 2 main hyper-parameters for this model are:\n\nEpsilon (eps): specifies how close points should be to each other to be considered a part of a cluster. It means that if the distance between two points is lower or equal to this value (eps), these points are considered to be neighbors.\nMinimum number of Points (min_samples): the minimum number of points to form a dense region. For example, if we set the minPoints parameter as 5, then we need at least 5 points to form a dense region.\n\nThere are 3 types of points in this algorithm:\n\nCore Point: The point where, within the specified ‘eps’ radius it has more than the specified min_samples number of points. This points always belongs to a dense region.\nBorder Point: A point where, within the specified ‘eps’ radius it has less than the specified min_samples number of points but it is in the neighborhood of a core point.\nNoise Point: A point that does not belong to both a Core point or a Border Point.\n\nEvery point in the dataset on given min_samples and eps, can categorize every data point into Core point, Border point and Noise point.\nDensity Edge: If p and q both are core points and distance between (p,q) ≤ eps then we can connect p, q vertex in a graph and call it “Density Edge”.\nDensity Connected Points: Two points p and q are said to be density connected points if both p and q are core points and they exist a path formed by density edges connecting point (p) to point(q).\nHow does the DBSCAN algorithm work?\n\nLabel the points as Core, Border and Noise Point.\nGet rid of every Noise point as they do not belong to any density region (cluster).\nFor every core point that has not been assigned to any cluster yet:\n\ncreate a new cluster with the point p and\nadd all the points that are density-connected to p.\n\nAssign each border points to the cluster of the closest core point.\n\nAdvantages:\n\nIt can handle Noise very well.\nDBSCAN can handle clusters of different shapes and sizes.\n\nDisadvantages:\n\nIf your dataset has multiple densities or varying densities, DBSCAN tends to fail. It does not work very well in such cases.\nIt’s extremely sensitive to the hyperparameters. A slight change in hyperparameters can lead to a drastic change in the outcome.\nAs we know, a concept like Density, may not work well in high dimensionality of data. We should avoid using it for text data.\n\nFor this exercise, we are choosing the hyper-parameter to tune as eps while min_samples is taken as twice the dimensionality of the data (33*2 = 66) and other hyper-parameters are set to default.\n\nThe range of values where eps will lie between is calculated using K-distance graph while calculates K-nearest neighbors (n_neighbors = dimensionality of data multiplied by 2) and their distances.\nThe tuning function calculates the maximum silhouette score for eps values between (0.2, 0.4, 0.6, … , 3) and outputs the Silhouette v/s Eps graph. The labels are then predicted and appended to our original dataset for plotting clusters using Seaborn’s Pairplot function.\n\n\n\n\nCode\nneighbors = NearestNeighbors(n_neighbors=66)\nneighbors_fit = neighbors.fit(X)\ndistances, indices = neighbors_fit.kneighbors(X)\n\n\n\n\nCode\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.plot(distances)\nplt.xlabel('Points')\nplt.ylabel('Distance (EPS)')\nplt.title('K-Distance Graph for DBSCAN')\n\n\nText(0.5, 1.0, 'K-Distance Graph for DBSCAN')\n\n\n\n\n\n\n\nCode\nalgo_features = ['season', 'driverId' , 'round' , 'circuitId' , 'pole_driverId' , 'position' ,'points', 'grid',\n                    'laps', 'age_on_race', 'cumulative_points', 'cumulative_laps', 'pole_history', 'win_history', 'win_driverId']\n\nstart = time.time()\nprint('DBSCAN Algorithm')\n\ndbscan_labels, parameter, db_model = maximize_silhouette(X, 'dbscan', 2, True)\nprint('The EPS for DBSCAN algorithm is: {}'.format(parameter))\nprint('The number of clusters for DBSCAN algorithm is: ', len(np.unique(dbscan_labels)))\ndf['dbscan_label'] = dbscan_labels\nalgo_features.append('dbscan_label')\nprint('Cluster Plots for DBSCAN algorithm')\nsns.pairplot(df[algo_features], hue='dbscan_label')\nplt.title('DBSCAN Algorithm')\nplt.show()\nend = time.time()\nprint('Time taken for DBSCAN algorithm: {} minutes'.format((end - start)/60))\n\n\nDBSCAN Algorithm\n\n\n\n\n\nThe EPS for DBSCAN algorithm is: 2.6\nThe number of clusters for DBSCAN algorithm is:  3\nCluster Plots for DBSCAN algorithm\n\n\n\n\n\nTime taken for DBSCAN algorithm: 2.133706529935201 minutes\n\n\n\n\nCode\ndf['dbscan_label'].value_counts()\n\n\n 0    24342\n-1     2130\n 1      149\nName: dbscan_label, dtype: int64\n\n\n\n\nCode\ndf['label'].value_counts()\n\n\nOutside_Top_10    15364\nTop_10             7866\nPodium             3391\nName: label, dtype: int64\n\n\n\n\nCode\ndf['dbscan_accuracy'] = 0\nfor i in range(len(df)):\n    \n    if df['dbscan_label'][i] == 0 and df['label'][i] == 'Outside_Top_10':\n        df['dbscan_accuracy'][i] = 1\n    \n    elif df['dbscan_label'][i] == -1 and df['label'][i] == 'Top_10':\n        df['dbscan_accuracy'][i] = 1\n        \n    elif df['dbscan_label'][i] == 2 and df['label'][i] == 'Podium':\n        df['dbscan_accuracy'][i] = 1\n        \n    else:\n        df['dbscan_accuracy'][i] = 0\n        \nprint('The accuracy of correct clusters formed by DBSCAN Algorithm = {} %'.format(df['dbscan_accuracy'].mean()*100))\n\n\nThe accuracy of correct clusters formed by DBSCAN Algorithm = 56.549340746027575 %\n\n\n\nResults for DBSCAN Algorithm\n\nFrom the K-Distance graph it can be seen that the curve changes between the range 1-3 of eps and hence the optimal value of epsilon should lie between 1 and 3.\nThe tuning algorithm output shows that the optimal eps value for which the silhouette score is maximum is 2.6.\nAfter putting our hyper-parameters eps=2.6 and min_samples=66 the clusters formed came out to be 3 which is also equal to the number of labels present in the original dataset.\nBut after comparing proposed cluster labels with the original labels, around 15,050 out of 26,000 values (56.543%) are matching with each other. This is due to the fact that our dataset is historical and contains a lot of inter-dependencies between the features which leads the algorithm to not create clusters properly as it is a simple clustering algorithm. Another reason might be that the dimensionality of the data is very high and this makes it very complex.\n\n\n\n\nAgglomerative (Hierarchical) Algorithm (Hyper-Parameter = n_clusters)\n\nAgglomerative Model is a type of Hierarchical Clustering.\nIt is also known as bottom-up clustering.\nThe Agglomerative model works as follows: each object is initially considered as a single-element cluster (leaf). At each step of the algorithm, the two clusters that are the most similar are combined into a new bigger cluster (nodes). This procedure is iterated until all points are member of just one single big cluster (root).\nPairs of clusters are merged as one moves up the hierarchy.\nThe clusters are merged by finding the distance between 2 data points. The most famous method is the Euclidean distance.\nEuclidean distance is a method of finding distance between 2 points where distance is calculated by drawing a straight line between the 2 data points and then calculating the length of the line. The formula to calculate the distance between 2 points p and q is: \\[ d\\left( p,q\\right)   = \\sqrt {\\sum _{i=1}^{n}  \\left( q_{i}-p_{i}\\right)^2 } \\]\nHow does Agglomerative Clustering work?\n\nIt starts by treating each observation as a separate cluster.\nDistance Measurement: The distance is calculated between each observation using methods like Euclidean, Manhattan and so on. For example, suppose we have 3 features of 5 rows each. The distance between the Obs. 1 and Obs. 2 can be calculated using the formula above, and same for Obs. 1 and Obs. 3 and Obs. 2 and Obs. 4 and so on. After that, we merge the smallest non-zero distance in the matrix to create our first node. To calculate the distance between this newly created node to other data points, linkage criterion should be set.\nLinkage Criterion: The linkage criterion is where exactly the distance is measured. The simplest linkage criterion is Single Linkage. In a single linkage criterion, we define our distance as the minimum distance between clusters data point. In other words, let’s assume that in our previous example, The node came to be as (Obs. 1, Obs. 3). Now to calculate distance between this node and Obs. 2, we will calculate individual distances (using Euclidean distance formula) between the node and Obs. 2 i.e., Distance between Obs. 1 and Obs. 2 and Obs. 3 and Obs. 2. The minimum of these 2 distances will be taken as the distance between the node (Obs. 1, Obs. 3) and Obs. 2. The same goes for Obs. 4 and Obs. 5 as well.\nThe next step would be again look at distances and merge points with the minimum distance. Continuing the same example, now suppose Obs. 2 and Obs. 4 have the least distance, hence it is merged into a node. Now we are left with 2 nodes: (Obs. 1, Obs. 3) and (Obs. 2, Obs. 4) and 1 other data point: Obs. 5. Again the distances are calculated using the linkage criterion and the nodes are merged again.\nWe keep the merging ongoing until all the data is clustered into one cluster. In the end, we would obtain a dendrogram with all the data that have been merged into one cluster.\n\nAdvantages:\n\nNo need for information about how many numbers of clusters are required.\nEasy to use and implement\n\nDisadvantages:\n\nWe can not take a step back in this algorithm.\nTime complexity is higher at least O(n^2logn)\n\nFor this exercise, we are choosing the hyper-parameter to tune as n_clusters while the other hyper-parameters are set to default. The tuning function calculates the maximum silhouette score for n_cluster values from 1-10 and outputs the Silhouette v/s N-Clusters graph. The labels are then predicted and appended to our original dataset for plotting clusters using Seaborn’s Pairplot function.\n\n\n\nCode\nalgo_features = ['season', 'driverId' , 'round' , 'circuitId' , 'pole_driverId' , 'position' ,'points', 'grid',\n                    'laps', 'age_on_race', 'cumulative_points', 'cumulative_laps', 'pole_history', 'win_history', 'win_driverId']\n\nstart = time.time()\nprint('Agglomerative (Hierarchical) Algorithm')\n\nag_labels, n_clusters, agg_model = maximize_silhouette(X, 'ag', 10, True)\nprint('The number of clusters for Agglomerative (Hierarchical) algorithm is: ', len(np.unique(ag_labels)))\ndf['ag_label'] = ag_labels\nalgo_features.append('ag_label')\nprint('Cluster Plots for Agglomerative (Hierarchical) algorithm')\nsns.pairplot(df[algo_features], hue='ag_label')\nplt.show()\nend = time.time()\nprint('Time taken for Agglomerative (Hierarchical) algorithm: {} minutes'.format((end - start)/60))\n\n\nAgglomerative (Hierarchical) Algorithm\n\n\n\n\n\nThe number of clusters for Agglomerative (Hierarchical) algorithm is:  4\nCluster Plots for Agglomerative (Hierarchical) algorithm\n\n\n\n\n\nTime taken for Agglomerative (Hierarchical) algorithm: 3.548959696292877 minutes\n\n\n\n\nCode\nX1 = df.drop(['label', 'ag_label', 'dbscan_label', 'kmeans_label', 'cluster_label', 'kmeans_accuracy', 'dbscan_accuracy'], axis=1)\n\nZ = linkage(X1, method='ward')\ndend = dendrogram(Z, truncate_mode='lastp',  # show only the last p merged clusters\n    p=4,\n    leaf_rotation=90,\n    leaf_font_size=12,\n    show_contracted=True)\nplt.title('Truncated Hierarchical Clustering Dendrogram')\nplt.xlabel('Cluster Size')\nplt.ylabel('Distance')\nplt.show()\n\n\n\n\n\n\n\nCode\ndf['ag_label'].value_counts()\n\n\n0    17576\n2     3875\n3     2990\n1     2180\nName: ag_label, dtype: int64\n\n\n\n\nCode\ndf['label'].value_counts()\n\n\nOutside_Top_10    15364\nTop_10             7866\nPodium             3391\nName: label, dtype: int64\n\n\n\n\nCode\ndf['ag_accuracy'] = 0\nfor i in range(len(df)):\n    \n    if df['ag_label'][i] == 0 and df['label'][i] == 'Outside_Top_10':\n        df['ag_accuracy'][i] = 1\n    \n    elif df['ag_label'][i] == 2 and df['label'][i] == 'Top_10':\n        df['ag_accuracy'][i] = 1\n        \n    elif df['ag_label'][i] == 3 and df['label'][i] == 'Top_10':\n        df['ag_accuracy'][i] = 1\n        \n    elif df['ag_label'][i] == 1 and df['label'][i] == 'Podium':\n        df['ag_accuracy'][i] = 1\n        \n    else:\n        df['ag_accuracy'][i] = 0\n        \nprint('The accuracy of correct clusters formed by Agglomerative (Hierarchical) Algorithm = {} %'.format(df['ag_accuracy'].mean()*100))\n\n\nThe accuracy of correct clusters formed by Agglomerative (Hierarchical) Algorithm = 51.185154577213474 %\n\n\n\nResults for Agglomerative (Hierarchical) Algorithm\n\nThe optimal number of clusters formed came out to be 4 which is not equal to the number of labels present in the original dataset.\nIt is also seen that if we merge clusters 2 and 3 and then compare the proposed cluster labels with the original labels, around 13,600 out of 26,000 values (51.185%) are matching with each other which is the highest among all the other merges of clusters ([1,3], [2,4], [1,4] and so on).\nThis means that maybe we should consider splitting the Top_10 label into 2 and then run other models.\nBut since the matching score is not high, we will not implement it."
  },
  {
    "objectID": "pages/codes/EDA.html",
    "href": "pages/codes/EDA.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom wordcloud import WordCloud, STOPWORDS\nfrom PIL import Image\nsns.set_theme(style=\"whitegrid\", palette='Set2')"
  },
  {
    "objectID": "pages/codes/EDA.html#record-data",
    "href": "pages/codes/EDA.html#record-data",
    "title": "Exploratory Data Analysis",
    "section": "Record Data",
    "text": "Record Data\n\nVisualizing Drivers with most wins and points\n\n\nCode\nwin_count_df = df[df['position'] == 1.0].groupby('driverRef', as_index=False)['position'].count()\nwin_count_df.sort_values(by = 'position', axis=0, ascending=False, inplace=True)\nwin_count_df_top10 = win_count_df.head(10)\n\nmax_points_df = df.groupby('driverRef', as_index=False)['points'].sum()\nmax_points_df.sort_values(by = 'points', axis=0, ascending=False, inplace=True)\nmax_points_df_top10 = max_points_df.head(10)\n\n\n\n\nCode\nfig, ax = plt.subplots(nrows=1, ncols=2 ,figsize = (18, 10))\n\nsns.barplot(x = win_count_df_top10['driverRef'], y = win_count_df_top10['position'] , ax = ax[0])\nsns.barplot(x = max_points_df_top10['driverRef'], y = max_points_df_top10['points'], ax = ax[1])\n\nFS = 16\n\nax[1].set_xlabel('Driver', fontsize=FS)\nax[1].set_ylabel('Number of Points in Career', fontsize=FS)\nax[1].set_title('Top 10 Drivers with Most Points', fontsize=FS)\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=45, horizontalalignment='right')\n\nax[0].set_xlabel('Driver', fontsize=FS)\nax[0].set_ylabel('Number of Wins in Career', fontsize=FS)\nax[0].set_title('Top 10 Drivers with Most Wins', fontsize=FS)\nax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=45, horizontalalignment='right')\n\nplt.savefig('../../images/visualizations/main_data/top10_drivers_wins_points.png')\nplt.show()\n\n\n\n\n\nWhile Michael Schumacher has the more wins in his entire career than other drivers except Lewis Hamilton, he has lesser number of points as compared to them. By domain knowledge we also know that Schumacher was a part of F1 from the early 90s to 2004, which leads us to the conclusion the point distribution for getting wins has increased over the years. This strange behaviour may also mean that other drivers apart of Schumacher gained more points while not winning (positions 2-10 also has points).\n\n\nVisualizing Constructors (Team) with most wins and points\n\n\nCode\nc_win_count_df = df[df['position'] == 1.0].groupby('constructorRef', as_index=False)['position'].count()\nc_win_count_df.sort_values(by = 'position', axis=0, ascending=False, inplace=True)\nc_win_count_df_top10 = c_win_count_df.head(10)\n\nc_max_points_df = df.groupby('constructorRef', as_index=False)['points'].sum()\nc_max_points_df.sort_values(by = 'points', axis=0, ascending=False, inplace=True)\nc_max_points_df_top10 = c_max_points_df.head(10)\n\n\n\n\nCode\nfig, ax = plt.subplots(nrows=1, ncols=2 ,figsize = (18, 10))\n\nsns.barplot(x = c_win_count_df_top10['constructorRef'], y = c_win_count_df_top10['position'], ax=ax[0])\nsns.barplot(x = c_max_points_df_top10['constructorRef'], y = c_max_points_df_top10['points'], ax=ax[1])\n\nFS = 16\nax[1].set_xlabel('Constructor', fontsize=FS)\nax[1].set_ylabel('Number of Points', fontsize=FS)\nax[1].set_title('Top 10 Constructors with Most Points', fontsize=FS)\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=45, horizontalalignment='right')\n\nax[0].set_xlabel('Constructor', fontsize=FS)\nax[0].set_ylabel('Number of Race Wins', fontsize=FS)\nax[0].set_title('Top 10 Constructors with Most Race Wins', fontsize=FS)\nax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=45, horizontalalignment='right')\n\nplt.savefig('../../images/visualizations/main_data/top10_constructor_wins_points.png')\nplt.show()\n\n\n\n\n\nA team like McLaren has been in F1 since a very early time which justifies it being the second team while seeing race wins. But the points gathered for McLaren have been lesser in the past few years compared to before 2000. Since we already know the point distribution from being 1st in the race was less before 2000 and McLaren is now a mid-tier team it makes sense that it has less overall points despite being second in Race wins.\n\n\nComparison of most famous and successful drivers\n\nSebastian Vettel, Lewis Hamilton and Fernando Alonso\nThese 3 drivers are also called the big 3 of the newer generation. All of them are from rival teams and are rivals on track too. They went head-to-head in a lot of battles on track which makes me want to compare their points , win histories and laps races over the past seasons.\n\n\nCode\ndf1 = df[(df['driverRef'] == 'vettel') | (df['driverRef'] == 'hamilton') | (df['driverRef'] == 'alonso')]\ndf2 = df1[['season', 'round', 'driverRef', 'cumulative_points']]\ndf3 = df1[['season', 'round', 'driverRef', 'win_history']]\ndf4 = df1[['season', 'round', 'driverRef', 'cumulative_laps']]\ndf5 = df1.groupby(['season', 'driverRef'], as_index=False)['points'].sum()\n\n\n\n\nCode\nfig, ax = plt.subplots(nrows=2, ncols=2 ,figsize = (18, 10))\nsns.lineplot(x = 'season', y = 'points', hue = 'driverRef', data = df5, ax=ax[0,0])\nsns.lineplot(x = 'season', y = 'cumulative_points', hue = 'driverRef', data = df2, ax=ax[0,1])\nsns.lineplot(x = 'season', y = 'win_history', hue = 'driverRef', data = df3, ax=ax[1,0])\nsns.lineplot(x = 'season', y = 'cumulative_laps', hue = 'driverRef', data = df4, ax=ax[1,1])\nplt.suptitle('Comparison of Vettel, Alonso and Hamilton Seasonwise', fontsize=16)\n\n\nText(0.5, 0.98, 'Comparison of Vettel, Alonso and Hamilton Seasonwise')\n\n\n\n\n\nThe number of points gained in each season for Alonso and Vettel peaked during 2011-2014 seasons while Hamilton peaked during 2017-2020. The downfall of Alonso resulted in the success of Hamiltion who became more succesful after 2013.\n\n\nLewis Hamilton and Max Verstappen for the 2021 Season\nThe 2021 season went down to the last race and resulted in a very close finish for World Driver’s Championship between Lewish Hamilton (2nd) and Max Verstappen (1st). The following section compares the points, laps completed and grid positions for all the races in the 2021 season.\n\n\nCode\ndf6 = df[(df['driverRef'] == 'max_verstappen') | (df['driverRef'] == 'hamilton')]\ndf6 = df6[df6['season'] == 2021]\ndf7 = df6[['round', 'driverRef', 'position']]\ndf8 = df6[['round', 'driverRef', 'points']]\ndf9 = df6[['round', 'driverRef', 'laps']]\ndf10 = df6[['round', 'driverRef', 'grid']]\n\n\n\n\nCode\nfig, ax = plt.subplots(nrows=2, ncols=2 ,figsize = (18, 10))\nsns.lineplot(x = 'round', y = 'position', hue = 'driverRef', data = df7, ax=ax[0,0])\nsns.lineplot(x = 'round', y = 'points', hue = 'driverRef', data = df8, ax=ax[0,1])\nsns.lineplot(x = 'round', y = 'laps', hue = 'driverRef', data = df9, ax=ax[1,0])\nsns.lineplot(x = 'round', y = 'grid', hue = 'driverRef', data = df10, ax=ax[1,1])\nplt.suptitle('Comparison of Hamilton and Max for 2021 roundwise', fontsize=16)\n\n\nText(0.5, 0.98, 'Comparison of Hamilton and Max for 2021 roundwise')\n\n\n\n\n\nHamilton and Max waere pretty consistent in terms of win results and grid positions with a few exceptions.\n\n\n\nDrivers with Highest Win % and Pole wins\n\n\nCode\ndf['pole_win'] = np.where((df['position'] == 1.0) & (df['grid'] == 1.0), 1, 0)\ndf11 = df['driverRef'][df['pole_win'] == 1]\n\n\n\n\nCode\ndf12 = pd.merge(df.groupby('driverRef', as_index=False)['round'].count(), df.groupby('driverRef', as_index=False)['win_history'].max(), on='driverRef')\ndf12['win_percentage'] = df12['win_history']/df12['round']*100\ndf12.sort_values(by = 'win_percentage', axis=0, ascending=False, inplace=True)\n\n\n\n\nCode\nfig, ax = plt.subplots(nrows=2, ncols=1 ,figsize = (18, 23))\nsns.barplot(x = df12[df12['round']>50].head(20)['driverRef'], y = df12[df12['round']>50].head(20)['win_percentage'], ax=ax[0])\nsns.barplot(x = df11.value_counts().index[:20], y = df11.value_counts().values[:20], ax=ax[1])\nax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=90, horizontalalignment='right')\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90, horizontalalignment='right')\nax[0].set_xlabel('Driver')\nax[0].set_ylabel('Win Percentage')\nax[0].set_title('Top 20 Drivers with Highest Win Percentage', fontsize=16)\nax[1].set_xlabel('Driver')\nax[1].set_ylabel('Number of Pole Wins')\nax[1].set_title('Top 20 Drivers with Most Pole Wins', fontsize=16)\n\n\nText(0.5, 1.0, 'Top 20 Drivers with Most Pole Wins')\n\n\n\n\n\n\n\nDistribution of Labels\n\n\nCode\nfig, ax = plt.subplots(nrows=1, ncols=2 ,figsize = (18, 10))\nax[0].pie(df['label'].value_counts(), labels = df['label'].value_counts().index, autopct='%1.1f%%')\nsns.countplot(x = 'label', data = df, ax=ax[1])\nplt.suptitle('Distribution of Labels')\n\n\nText(0.5, 0.98, 'Distribution of Labels')\n\n\n\n\n\n\n\nPairplot\n\n\nCode\nsns.pairplot(df[['position', 'points', 'laps', 'grid', 'label']], hue='label')\n\n\n<seaborn.axisgrid.PairGrid at 0x1462975b0>"
  },
  {
    "objectID": "pages/codes/EDA.html#twitter-data",
    "href": "pages/codes/EDA.html#twitter-data",
    "title": "Exploratory Data Analysis",
    "section": "Twitter Data",
    "text": "Twitter Data\n\nPie charts to compare Sentiments of tweets written by fans of different teams\n\n\nCode\nteams = ['Ferrari', 'Mercedes', 'Redbull', 'Haas', 'Mclaren', 'Alpine', 'Williams', 'Aston Martin', 'Alpha Tauri', 'Alfa Romeo']\n\nfor team in teams:\n    \n    indiv_df = grouped_df[grouped_df['Team'] == team]\n    indiv_df = indiv_df.rename(columns = {'0': 'Count'})\n    #indiv_df = indiv_df.pivot(index = 'Team', columns = 'sentiment', values = 'Count')\n    #display(indiv_df)\n    \n    positive_percentage = (indiv_df[indiv_df['sentiment'] == 'positive']['Count'].values[0] / indiv_df['Count'].sum()) * 100\n    negative_percentage = (indiv_df[indiv_df['sentiment'] == 'negative']['Count'].values[0] / indiv_df['Count'].sum()) * 100\n    neutral_percentage = (indiv_df[indiv_df['sentiment'] == 'neutral']['Count'].values[0] / indiv_df['Count'].sum()) * 100\n    \n    positive = indiv_df[indiv_df['sentiment'] == 'positive']['Count'].values[0]\n    negative = indiv_df[indiv_df['sentiment'] == 'negative']['Count'].values[0]\n    neutral = indiv_df[indiv_df['sentiment'] == 'neutral']['Count'].values[0]\n    \n    labels = [\"Positive\", \"Neutral\",\"Negative\"]\n    sizes = [positive, neutral, negative]\n    plt.pie(sizes, startangle=90, autopct='%1.1f%%')\n    plt.legend(labels)\n    plt.title(\"Sentiment Analysis Result for keyword=\" + team)\n    plt.axis('equal')\n    plt.savefig('../../images/visualizations/sentiment_analysis/' + team + '_pie.png')\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWorldcloud to check the most used words in tweets for each team\n\n\nCode\ndef create_wordcloud(text):\n    #mask = np.array(Image.open(\"cloud.png\"))\n    stopwords = set(STOPWORDS)\n    wc = WordCloud(background_color=\"white\",\n                    max_words=3000,\n                    stopwords=stopwords,\n                    repeat=True)\n    wc.generate(str(text))\n    wc.to_file(\"../../images/visualizations/sentiment_analysis/\" + team + \"_wordcloud.png\")\n    path=\"../../images/visualizations/sentiment_analysis/\" + team + \"_wordcloud.png\"\n    display(Image.open(path))\n    \nfor team in teams:\n    \n    indiv_df = all_sentiment_df[all_sentiment_df['Team'] == team]\n    create_wordcloud(indiv_df[\"text\"].values)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScatter plot to Compare Polarity and Subjectivity distributions Sentiment-wise\n\n\nCode\nsns.scatterplot(data = all_sentiment_df, x = 'polarity', y = 'subjectivity', hue = 'sentiment')\nplt.title('Polarity vs Subjectivity Sentiment-wise', fontsize=16)\n#plt.savefig('../../501-project-website/images/visualizations/sentiment_analysis/scatterplot_polarity_vs_subjectivity.png')\nplt.show()\n\n\n\n\n\n\n\nPolarity and Subjectivity Values between Teams\n\n\nCode\nfig, ax = plt.subplots(nrows=1, ncols=2 ,figsize = (18, 10))\n\nsns.barplot(x = 'Team', y = 'polarity', data = all_sentiment_df, ax = ax[0])\nsns.barplot(x = 'Team', y = 'subjectivity', data = all_sentiment_df, ax = ax[1])\n\nFS = 18\nax[0].set_title('Team vs Polarity', fontsize = FS)\nax[1].set_title('Team vs Subjectivity', fontsize = FS)\nax[0].set_xlabel('Team', fontsize = FS)\nax[0].set_ylabel('Polarity', fontsize = FS)\nax[1].set_xlabel('Team', fontsize = FS)\nax[1].set_ylabel('Subjectivity', fontsize = FS)\nax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=45, horizontalalignment='right')\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=45, horizontalalignment='right')\n\n#plt.savefig('../../501-project-website/images/visualizations/sentiment_analysis/Team_vs_Polarity_Subjectivity.png')\nplt.show()"
  },
  {
    "objectID": "pages/codes/ARM.html",
    "href": "pages/codes/ARM.html",
    "title": "ARM and Networking",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom apyori import apriori\nimport networkx as nx \nimport netgraph\nimport plotly.graph_objects as go\nsns.set_theme(style=\"whitegrid\", palette='Set2')\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline"
  },
  {
    "objectID": "pages/codes/ARM.html#apriori-algorithm",
    "href": "pages/codes/ARM.html#apriori-algorithm",
    "title": "ARM and Networking",
    "section": "Apriori Algorithm",
    "text": "Apriori Algorithm\n\nThere are 3 ways to measure association:\n\nSupport: It gives the fraction of transactions which contains item A and B. Basically Support tells us about the frequently bought items or the combination of items bought frequently. So with this, we can filter out the items that have a low frequency. \\[Support = \\frac {freq(A,B)}{N}\\]\nConfidence: It tells us how often the items A and B occur together, given the number times A occurs. Typically, when you work with the Apriori Algorithm, you define these terms accordingly. But how do you decide the value? Honestly, there isn’t a way to define these terms. Suppose you’ve assigned the support value as 2. What this means is, until and unless the item/s frequency is not 2%, you will not consider that item/s for the Apriori algorithm. This makes sense as considering items that are bought less frequently is a waste of time. Now suppose, after filtering you still have around 5000 items left. Creating association rules for them is a practically impossible task for anyone. This is where the concept of lift comes into play. \\[Confidence = \\frac {freq(A,B)}{freq(A)}\\]\nLift: Lift indicates the strength of a rule over the random occurrence of A and B. It basically tells us the strength of any rule. Focus on the denominator, it is the probability of the individual support values of A and B and not together. Lift explains the strength of a rule. More the Lift more is the strength. Let’s say for A -> B, the lift value is 4. It means that if you buy A the chances of buying B is 4 times. \\[Lift = \\frac {Support}{Supp(A) * Supp(B)}\\]\n\nApriori algorithm uses frequent itemsets to generate association rules. It is based on the concept that a subset of a frequent itemset must also be a frequent itemset. Frequent Itemset is an itemset whose support value is greater than a threshold value(support).\nGiven a threshold C, the Apriori algorithm identifies the item sets which are subsets of at least C transactions in the database.\nApriori uses a “bottom up” approach, where frequent subsets are extended one item at a time (a step known as candidate generation), and groups of candidates are tested against the data. The algorithm terminates when no further successful extensions are found.\nApriori uses breadth-first search and a Hash tree structure to count candidate item sets efficiently. It generates candidate item sets of length k from item sets of length k-1. Then it prunes the candidates which have an infrequent sub pattern. According to the downward closure lemma, the candidate set contains all frequent k-length item sets. After that, it scans the transaction database to determine frequent item sets among the candidates.\nSteps of the Apriori algorithm:\n\nComputing the support for each individual item.\nDeciding on the support threshold.\nSelecting the frequent items.\nFinding the support of the frequent itemsets.\nRepeat for larger sets.\nGenerate Association Rules and compute confidence.\nCompute lift.\n\n\nRunning the apriori algorithm for min_support=0.05, min_confidence=0.2, min_lift=2, min_length=2 :\n\n\nCode\nassociation_rules = apriori(records, min_support=0.05, min_confidence=0.2, min_lift=2, min_length=2)\nassociation_results = list(association_rules)\n\n\nOutputting the results:\n\n\nCode\nfor item in association_results:\n    \n    # first index of the inner list\n    # Contains base item and add item\n    pair = item[0] \n    items = [x for x in pair]\n    print(\"Rule: \" + items[0] + \" -> \" + items[1])\n    \n    #second index of the inner list\n    print(\"Support: \" + str(item[1]))\n    \n    #third index of the list located at 0th\n    #of the third index of the inner list\n\n    print(\"Confidence: \" + str(item[2][0][2]))\n    print(\"Lift: \" + str(item[2][0][3]))\n    print(\"=====================================\")\n\n\nRule: label_Podium -> constructorRef_mercedes\nSupport: 0.06809024979854955\nConfidence: 0.6926229508196721\nLift: 4.696967661023022\n=====================================\nRule: status_Finished -> label_Podium\nSupport: 0.14746172441579372\nConfidence: 1.0\nLift: 2.135972461273666\n=====================================\nRule: pole_driverRef_leclerc -> season_2019\nSupport: 0.05640612409347301\nConfidence: 0.7777777777777778\nLift: 4.596296296296296\n=====================================\nRule: season_2021 -> pole_driverRef_max_verstappen\nSupport: 0.08058017727639001\nConfidence: 0.7692307692307693\nLift: 4.33916083916084\n=====================================\nRule: weather_Sunny -> pole_driverRef_max_verstappen\nSupport: 0.072522159548751\nConfidence: 0.6923076923076923\nLift: 2.0263062409288826\n=====================================\nRule: win_driverRef_max_verstappen -> pole_driverRef_max_verstappen\nSupport: 0.072522159548751\nConfidence: 0.6923076923076923\nLift: 4.274397244546498\n=====================================\nRule: pole_driverRef_rosberg -> season_2016\nSupport: 0.07091055600322321\nConfidence: 1.0\nLift: 5.372294372294372\n=====================================\nRule: win_driverRef_rosberg -> pole_driverRef_rosberg\nSupport: 0.0531829170024174\nConfidence: 0.7499999999999999\nLift: 9.40151515151515\n=====================================\nRule: weather_Windy -> season_2016\nSupport: 0.088638195004029\nConfidence: 0.47619047619047616\nLift: 3.4761904761904763\n=====================================\nRule: win_driverRef_rosberg -> season_2016\nSupport: 0.07977437550362611\nConfidence: 0.4285714285714286\nLift: 5.372294372294372\n=====================================\nRule: season_2018 -> weather_Not Available\nSupport: 0.10475423045930701\nConfidence: 0.6190476190476191\nLift: 3.4919913419913424\n=====================================\nRule: season_2021 -> win_driverRef_max_verstappen\nSupport: 0.08058017727639001\nConfidence: 0.4545454545454546\nLift: 2.806422433288105\n=====================================\nRule: status_Mechanical_Issue -> stop_Not Available\nSupport: 0.05962933118452861\nConfidence: 0.37279596977329976\nLift: 4.794194802991347\n=====================================\nRule: win_driverRef_rosberg -> weather_Windy\nSupport: 0.0531829170024174\nConfidence: 0.38823529411764707\nLift: 4.866666666666667\n=====================================\nRule: status_Finished -> label_Podium\nSupport: 0.06809024979854955\nConfidence: 0.6926229508196721\nLift: 4.696967661023022\n=====================================\nRule: status_Mechanical_Issue -> pole_driverRef_hamilton\nSupport: 0.08017727639000806\nConfidence: 0.5012594458438288\nLift: 2.2058970648659275\n=====================================\nRule: weather_Not Available -> label_Outside_Top_10\nSupport: 0.052377115229653506\nConfidence: 0.30952380952380953\nLift: 3.4919913419913424\n=====================================\nRule: status_Mechanical_Issue -> label_Outside_Top_10\nSupport: 0.05962933118452861\nConfidence: 0.37279596977329976\nLift: 5.083953829545769\n=====================================\nRule: pole_driverRef_hamilton -> label_Podium\nSupport: 0.0652699435938759\nConfidence: 0.44262295081967207\nLift: 2.2104429857835535\n=====================================\nRule: status_Finished -> label_Podium\nSupport: 0.07131345688960515\nConfidence: 0.4836065573770491\nLift: 2.6037125280039826\n=====================================\nRule: status_Finished -> weather_Sunny\nSupport: 0.05076551168412571\nConfidence: 0.3442622950819672\nLift: 2.163187383274538\n=====================================\nRule: status_Finished -> win_driverRef_hamilton\nSupport: 0.072522159548751\nConfidence: 0.4918032786885246\nLift: 2.2073340645658552\n=====================================\nRule: pole_driverRef_hamilton -> weather_Windy\nSupport: 0.0531829170024174\nConfidence: 0.2857142857142857\nLift: 3.6934523809523805\n=====================================\nRule: pole_driverRef_hamilton -> season_2018\nSupport: 0.05640612409347301\nConfidence: 0.3333333333333333\nLift: 4.5962962962962965\n=====================================\nRule: win_driverRef_max_verstappen -> season_2021\nSupport: 0.05640612409347301\nConfidence: 0.5384615384615384\nLift: 6.682307692307692\n=====================================\nRule: win_driverRef_rosberg -> pole_driverRef_rosberg\nSupport: 0.0531829170024174\nConfidence: 0.7499999999999999\nLift: 9.40151515151515\n=====================================\nRule: win_driverRef_rosberg -> weather_Windy\nSupport: 0.0531829170024174\nConfidence: 0.2857142857142857\nLift: 5.372294372294372\n=====================================\nRule: stop_One -> season_2018\nSupport: 0.06647864625302176\nConfidence: 0.39285714285714285\nLift: 4.131658595641646\n=====================================\nRule: win_driverRef_hamilton -> season_2018\nSupport: 0.05640612409347301\nConfidence: 0.3333333333333333\nLift: 4.136666666666667\n=====================================\nRule: pole_driverRef_hamilton -> win_driverRef_hamilton\nSupport: 0.08783239323126511\nConfidence: 0.3865248226950355\nLift: 2.085553499845822\n=====================================\nRule: win_driverRef_hamilton -> pole_driverRef_hamilton\nSupport: 0.06970185334407736\nConfidence: 0.4576719576719577\nLift: 2.054144301883904\n=====================================\n\n\n\nHelper Functions\n\nreformat_results: Reformats the results from the apriori algorithm into a dataframe with itemsets, support, confidence and lift values.\nconvert_to_network: Converts the apriori dataframe into a Network Graph.\nplot_network: Visualisation of the Network Graph.\n\n\n\nCode\ndef reformat_results(results):\n\n    #CLEAN-UP RESULTS \n    keep=[]\n    for i in range(0,len(results)):\n        for j in range(0,len(list(results[i]))):\n            # print(results)\n            if(j>1):\n                for k in range(0,len(list(results[i][j]))):\n                    if(len(results[i][j][k][0])!=0):\n                        #print(len(results[i][j][k][0]),results[i][j][k][0])\n                        rhs=list(results[i][j][k][0])\n                        lhs=list(results[i][j][k][1])\n                        conf=float(results[i][j][k][2])\n                        lift=float(results[i][j][k][3])\n                        keep.append([rhs,lhs,supp,conf,supp*conf,lift])\n                        # keep.append()\n            if(j==1):\n                supp=results[i][j]\n\n    return pd.DataFrame(keep, columns =[\"rhs\",\"lhs\",\"supp\",\"conf\",\"supp x conf\",\"lift\"])\n\n\n\n\nCode\ndef convert_to_network(df):\n    #print(df)\n\n    #BUILD GRAPH\n    G = nx.DiGraph()  # DIRECTED\n    for row in df.iterrows():\n        # for column in df.columns:\n        lhs=\"_\".join(row[1][0])\n        rhs=\"_\".join(row[1][1])\n        conf=row[1][3]; #print(conf)\n        if(lhs not in G.nodes): \n            G.add_node(lhs)\n        if(rhs not in G.nodes): \n            G.add_node(rhs)\n\n        edge=(lhs,rhs)\n        if edge not in G.edges:\n            G.add_edge(lhs, rhs, weight=conf)\n\n    # print(G.nodes)\n    # print(G.edges)\n    return G\n\n\n\n\nCode\ndef plot_network(G):\n    #SPECIFIY X-Y POSITIONS FOR PLOTTING\n    pos=nx.planar_layout(G)\n\n    #GENERATE PLOT\n    fig, ax = plt.subplots()\n    fig.set_size_inches(45, 45)\n\n    #assign colors based on attributes\n    weights_e   = [G[u][v]['weight'] for u,v in G.edges()]\n\n    #SAMPLE CMAP FOR COLORS \n    cmap=plt.cm.get_cmap('Blues')\n    colors_e    = [cmap(G[u][v]['weight']*10) for u,v in G.edges()]\n\n    #PLOT\n    nx.draw(\n    G,\n    node_color='crimson',\n    edgecolors=\"black\",\n    edge_color=colors_e,\n    node_size=7000,\n    linewidths=2,\n    font_size=12,\n    font_color=\"black\",\n    width=weights_e,\n    with_labels=True,\n    pos=pos,\n    ax=ax\n    )\n    ax.set_title(\"NetworkX Graph for Association Rules\", fontsize=50)\n    \n    plt.show()\n\n# raise\n\n\n\n\nFinal Reformatting and Plotting\n\n\nCode\nresult_df = reformat_results(association_results)\nprint(\"Results\\n\",len(association_results))\n\n\nResults\n 31\n\n\n\n\nCode\nG = convert_to_network(result_df)\nplot_network(G)"
  },
  {
    "objectID": "pages/codes/SVM.html",
    "href": "pages/codes/SVM.html",
    "title": "Support Vector Machine Classifier for Record Data",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix,\\\n    precision_score, recall_score, accuracy_score, f1_score, log_loss,\\\n    roc_curve, roc_auc_score, classification_report\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\nfrom sklearn.compose import ColumnTransformer\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "pages/codes/SVM.html#model-prediction-function",
    "href": "pages/codes/SVM.html#model-prediction-function",
    "title": "Support Vector Machine Classifier for Record Data",
    "section": "Model Prediction function",
    "text": "Model Prediction function\n\nAfter fitting the model it is important to showcase and visualize the model classification results.\nThe model_results function predicts the model results on test data (2021 races). It displays out the 40 results from the test data along with the prediction probabilities.\nThe function also fills the prediction scorecard dictionary which contains:\n\nModel\nAccuracy\nPrecision\nRecall\nBest parameters\n\n\n\n\nCode\nprediction_scorecard = {'model':[],\n                        'accuracy_score':[],\n                        'precision_score':[],\n                        'recall_score':[],\n                        'best_params':[]}\n\n\n\n\nCode\ndef model_results(X_test, model, model_id):\n    # Predict!\n    pred = model.predict(X_test)\n    pred_proba = model.predict_proba(X_test)\n    df_pred = pd.DataFrame(np.around(pred_proba, 4), index=X_test.index, columns=['prob_0', 'prob_1', 'prob_2'])\n    df_pred['prediction'] = list(pred)\n    df_pred['actual'] = y_test['label']\n    df_pred['grid_position'] = X_test['grid']\n\n    # Include row if an 'actual' or 'predicted' podium occured for calculating accuracy\n    # df_pred['sort'] = df_pred['prediction'] + df_pred['actual']\n    # df_pred = df_pred[df_pred['sort'] > 0]\n    # df_pred.reset_index(inplace=True)\n    df_pred = df_pred.groupby(['round']).apply(pd.DataFrame.sort_values, 'prob_1', ascending=False)\n    # df_pred.drop(['sort'], axis=1, inplace=True)\n    # df_pred.reset_index(drop=True, inplace=True) \n    \n    # Save Accuracy, Precision, \n    prediction_scorecard['model'].append(model_id)\n    prediction_scorecard['accuracy_score'].append(accuracy_score(df_pred['actual'], df_pred['prediction']))\n    prediction_scorecard['precision_score'].append(precision_score(df_pred['actual'], df_pred['prediction'], average='micro'))\n    prediction_scorecard['recall_score'].append(recall_score(df_pred['actual'], df_pred['prediction'], average='micro'))\n    prediction_scorecard['best_params'].append(str(model.best_params_))\n    display(df_pred.head(40))"
  },
  {
    "objectID": "pages/codes/SVM.html#grid-search-cv",
    "href": "pages/codes/SVM.html#grid-search-cv",
    "title": "Support Vector Machine Classifier for Record Data",
    "section": "Grid search CV",
    "text": "Grid search CV\n\nHyper-parameters are variables that you specify while building a machine-learning model. This means that it’s the user that defines the hyper-parameters while building the model. Hyper-parameters control the learning process, while parameters are learned.\nThe performance of a model significantly depends on the value of hyperparameters. Note that there is no way to know in advance the best values for hyperparameters so ideally, we need to try all possible values to know the optimal values.\nDoing this manually could take a considerable amount of time and resources and thus we use GridSearchCV to automate the tuning of hyperparameters.\nGrid search CV of the sklearn library is a module for hyperparameter tuning.\nIt runs through all the different parameters that is fed into the parameter grid and produces the best combination of parameters, based on a scoring metric of your choice (accuracy, f1, etc).\nGridSearchCV tries all the combinations of the values passed in the dictionary and evaluates the model for each combination using the Cross-Validation method.\nThe process is time consuming.\n\n\n\nCode\nsvm_params= {'svm__C': [0.1],\n             'svm__kernel': ['linear', 'poly'],\n             'svm__degree': [2, 3],\n             'svm__gamma': [0.01]}\n\n\nTypes of SVM Kernels:  - Linear: These are commonly recommended for text classification because most of these types of classification problems are linearly separable. - Polynomial: The polynomial kernel isn’t used in practice very often because it isn’t as computationally efficient as other kernels and its predictions aren’t as accurate. - Gaussian Radial Basis Function (RBF): One of the most powerful and commonly used kernels in SVMs. Usually the choice for non-linear data.\n\n\nCode\nscoring = ['neg_log_loss', 'accuracy']\n\nsvm_cv = GridSearchCV(prediction_model(SVC(probability=True), 'svm'),\n                      param_grid=svm_params,\n                      scoring=scoring, \n                      refit='neg_log_loss',  \n                      verbose=0)"
  },
  {
    "objectID": "pages/codes/SVM.html#fitting-and-training-the-svm-model",
    "href": "pages/codes/SVM.html#fitting-and-training-the-svm-model",
    "title": "Support Vector Machine Classifier for Record Data",
    "section": "Fitting and Training the SVM model",
    "text": "Fitting and Training the SVM model\n\n\nCode\n# Train Model\nsvm_cv.fit(X_train, y_train)\n\n\nGridSearchCV(estimator=Pipeline(steps=[('prep',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('scaler',\n                                                                                          StandardScaler())]),\n                                                                         ['circuitId',\n                                                                          'position',\n                                                                          'points',\n                                                                          'grid',\n                                                                          'laps',\n                                                                          'age_on_race',\n                                                                          'cumulative_points',\n                                                                          'cumulative_laps',\n                                                                          'pole_driverId',\n                                                                          'pole_history',\n                                                                          'win_driverId',\n                                                                          'win_history']),\n                                                                        ('cat',\n                                                                         Pipeline(steps=[('ohe',\n                                                                                          OneHotEncoder(handle_unknown='ignore'))]),\n                                                                         ['status',\n                                                                          'weather',\n                                                                          'stop'])])),\n                                       ('svm', SVC(probability=True))]),\n             param_grid={'svm__C': [0.1], 'svm__degree': [2, 3],\n                         'svm__gamma': [0.01],\n                         'svm__kernel': ['linear', 'poly']},\n             refit='neg_log_loss', scoring=['neg_log_loss', 'accuracy'])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(estimator=Pipeline(steps=[('prep',\n                                        ColumnTransformer(transformers=[('num',\n                                                                         Pipeline(steps=[('scaler',\n                                                                                          StandardScaler())]),\n                                                                         ['circuitId',\n                                                                          'position',\n                                                                          'points',\n                                                                          'grid',\n                                                                          'laps',\n                                                                          'age_on_race',\n                                                                          'cumulative_points',\n                                                                          'cumulative_laps',\n                                                                          'pole_driverId',\n                                                                          'pole_history',\n                                                                          'win_driverId',\n                                                                          'win_history']),\n                                                                        ('cat',\n                                                                         Pipeline(steps=[('ohe',\n                                                                                          OneHotEncoder(handle_unknown='ignore'))]),\n                                                                         ['status',\n                                                                          'weather',\n                                                                          'stop'])])),\n                                       ('svm', SVC(probability=True))]),\n             param_grid={'svm__C': [0.1], 'svm__degree': [2, 3],\n                         'svm__gamma': [0.01],\n                         'svm__kernel': ['linear', 'poly']},\n             refit='neg_log_loss', scoring=['neg_log_loss', 'accuracy'])estimator: PipelinePipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('num',\n                                                  Pipeline(steps=[('scaler',\n                                                                   StandardScaler())]),\n                                                  ['circuitId', 'position',\n                                                   'points', 'grid', 'laps',\n                                                   'age_on_race',\n                                                   'cumulative_points',\n                                                   'cumulative_laps',\n                                                   'pole_driverId',\n                                                   'pole_history',\n                                                   'win_driverId',\n                                                   'win_history']),\n                                                 ('cat',\n                                                  Pipeline(steps=[('ohe',\n                                                                   OneHotEncoder(handle_unknown='ignore'))]),\n                                                  ['status', 'weather',\n                                                   'stop'])])),\n                ('svm', SVC(probability=True))])prep: ColumnTransformerColumnTransformer(transformers=[('num',\n                                 Pipeline(steps=[('scaler', StandardScaler())]),\n                                 ['circuitId', 'position', 'points', 'grid',\n                                  'laps', 'age_on_race', 'cumulative_points',\n                                  'cumulative_laps', 'pole_driverId',\n                                  'pole_history', 'win_driverId',\n                                  'win_history']),\n                                ('cat',\n                                 Pipeline(steps=[('ohe',\n                                                  OneHotEncoder(handle_unknown='ignore'))]),\n                                 ['status', 'weather', 'stop'])])num['circuitId', 'position', 'points', 'grid', 'laps', 'age_on_race', 'cumulative_points', 'cumulative_laps', 'pole_driverId', 'pole_history', 'win_driverId', 'win_history']StandardScalerStandardScaler()cat['status', 'weather', 'stop']OneHotEncoderOneHotEncoder(handle_unknown='ignore')SVCSVC(probability=True)"
  },
  {
    "objectID": "pages/codes/SVM.html#testing-the-svm-model",
    "href": "pages/codes/SVM.html#testing-the-svm-model",
    "title": "Support Vector Machine Classifier for Record Data",
    "section": "Testing the SVM model",
    "text": "Testing the SVM model\n\n\nCode\n# Test Model\nmodel_results(X_test, svm_cv, 'Support Vector Machines')\nsvm_results = pd.DataFrame(prediction_scorecard)\n\n\n\n\n\n\n  \n    \n      \n      \n      \n      \n      prob_0\n      prob_1\n      prob_2\n      prediction\n      actual\n      grid_position\n    \n    \n      round\n      season\n      round\n      driverRef\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      2021\n      1\n      hamilton\n      0.0000\n      1.0000\n      0.0000\n      Podium\n      Podium\n      2\n    \n    \n      max_verstappen\n      0.0000\n      1.0000\n      0.0000\n      Podium\n      Podium\n      1\n    \n    \n      bottas\n      0.0000\n      0.9989\n      0.0011\n      Podium\n      Podium\n      3\n    \n    \n      norris\n      0.0001\n      0.0106\n      0.9893\n      Top_10\n      Top_10\n      7\n    \n    \n      raikkonen\n      0.9990\n      0.0009\n      0.0001\n      Outside_Top_10\n      Outside_Top_10\n      14\n    \n    \n      giovinazzi\n      0.9998\n      0.0002\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      12\n    \n    \n      ocon\n      0.9999\n      0.0001\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      16\n    \n    \n      ricciardo\n      0.0000\n      0.0000\n      1.0000\n      Top_10\n      Top_10\n      6\n    \n    \n      sainz\n      0.0000\n      0.0000\n      1.0000\n      Top_10\n      Top_10\n      8\n    \n    \n      perez\n      0.0000\n      0.0000\n      1.0000\n      Top_10\n      Top_10\n      0\n    \n    \n      alonso\n      1.0000\n      0.0000\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      9\n    \n    \n      stroll\n      0.0000\n      0.0000\n      1.0000\n      Top_10\n      Top_10\n      10\n    \n    \n      gasly\n      1.0000\n      0.0000\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      5\n    \n    \n      leclerc\n      0.0000\n      0.0000\n      1.0000\n      Top_10\n      Top_10\n      4\n    \n    \n      vettel\n      1.0000\n      0.0000\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      20\n    \n    \n      russell\n      1.0000\n      0.0000\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      15\n    \n    \n      latifi\n      1.0000\n      0.0000\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      17\n    \n    \n      tsunoda\n      0.0000\n      0.0000\n      1.0000\n      Top_10\n      Top_10\n      13\n    \n    \n      mick_schumacher\n      1.0000\n      0.0000\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      18\n    \n    \n      mazepin\n      1.0000\n      0.0000\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      19\n    \n    \n      2\n      2021\n      2\n      hamilton\n      0.0000\n      1.0000\n      0.0000\n      Podium\n      Podium\n      1\n    \n    \n      max_verstappen\n      0.0000\n      1.0000\n      0.0000\n      Podium\n      Podium\n      3\n    \n    \n      norris\n      0.0000\n      0.9954\n      0.0046\n      Podium\n      Podium\n      7\n    \n    \n      leclerc\n      0.0001\n      0.0128\n      0.9871\n      Top_10\n      Top_10\n      4\n    \n    \n      perez\n      0.9987\n      0.0012\n      0.0001\n      Outside_Top_10\n      Outside_Top_10\n      2\n    \n    \n      tsunoda\n      0.9997\n      0.0003\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      20\n    \n    \n      raikkonen\n      0.9999\n      0.0001\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      16\n    \n    \n      gasly\n      0.0000\n      0.0000\n      1.0000\n      Top_10\n      Top_10\n      5\n    \n    \n      mick_schumacher\n      1.0000\n      0.0000\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      18\n    \n    \n      latifi\n      1.0000\n      0.0000\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      14\n    \n    \n      russell\n      1.0000\n      0.0000\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      12\n    \n    \n      giovinazzi\n      1.0000\n      0.0000\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      17\n    \n    \n      stroll\n      0.0000\n      0.0000\n      1.0000\n      Top_10\n      Top_10\n      10\n    \n    \n      alonso\n      0.0000\n      0.0000\n      1.0000\n      Top_10\n      Top_10\n      15\n    \n    \n      ocon\n      0.0000\n      0.0000\n      1.0000\n      Top_10\n      Top_10\n      9\n    \n    \n      sainz\n      0.0000\n      0.0000\n      1.0000\n      Top_10\n      Top_10\n      11\n    \n    \n      bottas\n      1.0000\n      0.0000\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      8\n    \n    \n      ricciardo\n      0.0000\n      0.0000\n      1.0000\n      Top_10\n      Top_10\n      6\n    \n    \n      vettel\n      1.0000\n      0.0000\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      0\n    \n    \n      mazepin\n      1.0000\n      0.0000\n      0.0000\n      Outside_Top_10\n      Outside_Top_10\n      19\n    \n  \n\n\n\n\nThe best parameters for our model are:\n\n\nCode\nsvm_cv.best_params_\n\n\n{'svm__C': 0.1, 'svm__degree': 3, 'svm__gamma': 0.01, 'svm__kernel': 'linear'}\n\n\n\n\nCode\nsvm_results\n\n\n\n\n\n\n  \n    \n      \n      model\n      accuracy_score\n      precision_score\n      recall_score\n      best_params\n    \n  \n  \n    \n      0\n      Support Vector Machines\n      1.0\n      1.0\n      1.0\n      {'svm__C': 0.1, 'svm__degree': 3, 'svm__gamma'..."
  },
  {
    "objectID": "pages/codes/SVM.html#conclusion",
    "href": "pages/codes/SVM.html#conclusion",
    "title": "Support Vector Machine Classifier for Record Data",
    "section": "Conclusion",
    "text": "Conclusion\n\nThe SVM model gives out 100% accuracy, precision and recall values.\nThe ideal hyperparameters are:\n\nC = 0.1\ndegree = 3\ngamma = 0.01\nkernel = linear"
  },
  {
    "objectID": "pages/codes/record_datagathering.html",
    "href": "pages/codes/record_datagathering.html",
    "title": "Record Data Gathering",
    "section": "",
    "text": "Data Gathering and Pre-Processing is a very important step in any Data science project pipeline. It is undeniable that 80% of a data scientist’s time and effort is spent in collecting, cleaning and preparing the data for analysis because datasets come in various sizes and are different in nature. It is extremely important for a data scientist to reshape and refine the datasets into usable datasets, which can be leveraged for analytics.\nKnowledge is power, information is knowledge, and data is information in digitized form, at least as defined in IT. Hence, data is power. But before you can leverage that data into a successful strategy for your organization or business, you need to gather it. That’s your first step.\nBefore we define what is data collection, it’s essential to ask the question, “What is data?” The abridged answer is, data is various kinds of information formatted in a particular way. Therefore, data collection is the process of gathering, measuring, and analyzing accurate data from a variety of relevant sources to find answers to research problems, answer questions, evaluate outcomes, and forecast trends and probabilities.\nOur society is highly dependent on data, which underscores the importance of collecting it. Accurate data collection is necessary to make informed business decisions, ensure quality assurance, and keep research integrity.\nDuring data collection, the researchers must identify the data types, the sources of data, and what methods are being used. We will soon see that there are many different data collection methods. There is heavy reliance on data collection in research, commercial, and government fields.\nWhy Do We Need Data Collection?\n\nBefore a judge makes a ruling in a court case or a general creates a plan of attack, they must have as many relevant facts as possible. The best courses of action come from informed decisions, and information and data are synonymous.\nThe concept of data collection isn’t a new one but the world has changed. There is far more data available today, and it exists in forms that were unheard of a century ago. The data collection process has had to change and grow with the times, keeping pace with technology.\nWhether you’re in the world of academia, trying to conduct research, or part of the commercial sector, thinking of how to promote a new product, you need data collection to help you make better choices.\n\nWhat Are the Different Methods of Data Collection?\n\nSurveys\nTransactional Tracking\nInterviews and Focus Groups\nObservation\nOnline Tracking\nForms\nSocial Media Monitoring\nApplication Programming Interface"
  },
  {
    "objectID": "pages/codes/record_datagathering.html#race-information",
    "href": "pages/codes/record_datagathering.html#race-information",
    "title": "Record Data Gathering",
    "section": "Race Information",
    "text": "Race Information\nThere are 2 main things in a Formula 1. The first is Season and the second is Round. There are multiple rounds (also called as Races) in every season and every Season happends once a year. The data contains the information about every round of every season such as race name, circuit information, date and the results.\n\n\nCode\ndef get_race_results(url, offset, limit=1000):\n    \n    result = requests.get(url + str(limit) + '&offset=' + str(offset))\n    \n    return result.json()\n\n\n\n\nCode\nrecent_race_json = get_race_results(url = 'http://ergast.com/api/f1/results.json?limit=', offset = 25000)\n\nwith open('../../data/00-raw-data/race_data.json', 'w') as outfile:\n    json.dump(recent_race_json, outfile)\n\n\n\n\nCode\ndef all_races():\n    \n    limit = 1000\n    result = []\n    length_per_page = []\n    url = 'http://ergast.com/api/f1/results.json?limit='\n    \n    p = 0\n    \n    while p < 100:\n        \n        page_result_json = get_race_results(url = url, offset = p*1000)\n        list_per_page = page_result_json['MRData']['RaceTable']['Races']\n    \n        if len(list_per_page) == 0:\n            break\n        \n        length_per_page.append(len(list_per_page))\n        \n        for i in range(len(list_per_page)):\n            result.append(list_per_page[i])\n        \n        p = p + 1\n        \n\n        \n    return result, length_per_page\n\n\n\n\nCode\nresult_all_races,length_per_page = all_races()\n\n\n\n\nCode\nrace_result_df = pd.DataFrame(result_all_races)\nrace_result_df.to_csv('../../data/00-raw-data/race_results.csv')"
  },
  {
    "objectID": "pages/codes/record_datagathering.html#qualifying-information",
    "href": "pages/codes/record_datagathering.html#qualifying-information",
    "title": "Record Data Gathering",
    "section": "Qualifying Information",
    "text": "Qualifying Information\nBefore every main Race of a season, there is a Qualifier where each driver races around the whole Circuit to set the fastest lap time. This determines the position that the driver is going to be starting on the Race day. The Qualifiers were fully implemented properly from the 2003 season onwards so there is no data available for seasons before 2003.\n\n\nCode\ndef get_qual_results(url, offset, limit=1000):\n    \n    result = requests.get(url + str(limit) + '&offset=' + str(offset))\n    \n    return result.json()\n\n\n\n\nCode\nrecent_qual_json = get_qual_results(url = 'http://ergast.com/api/f1/qualifying.json?limit=', offset = 9000)\n\nwith open('../../data/00-raw-data/qual_data.json', 'w') as outfile:\n    json.dump(recent_qual_json, outfile)\n\n\n\n\nCode\ndef all_qual():\n    \n    limit = 1000\n    result = []\n    length_per_page = []\n    url = 'http://ergast.com/api/f1/qualifying.json?limit='\n    \n    p = 0\n    \n    while p < 100:\n        \n        page_result_json = get_qual_results(url = url, offset = p*1000)\n        list_per_page = page_result_json['MRData']['RaceTable']['Races']\n    \n        if len(list_per_page) == 0:\n            break\n        \n        length_per_page.append(len(list_per_page))\n        \n        for i in range(len(list_per_page)):\n            result.append(list_per_page[i])\n        \n        p = p + 1\n        \n\n        \n    return result, length_per_page\n\n\n\n\nCode\nresult_all_qual,length_per_page = all_qual()\n\n\n\n\nCode\nqual_result_df = pd.DataFrame(result_all_qual)\n\n\n\n\nCode\nqual_result_df.to_csv('../../data/00-raw-data/qual_results.csv')"
  },
  {
    "objectID": "pages/codes/record_datagathering.html#circuit-information",
    "href": "pages/codes/record_datagathering.html#circuit-information",
    "title": "Record Data Gathering",
    "section": "Circuit Information",
    "text": "Circuit Information\nThe tracks where each race is conducted on are knows as Circuits. The track owners have to renew their contract each year with the FIA to keep their track as one of the tracks where the Races will be held. The data has columns such as id of the circuit, the location and so on.\n\n\nCode\ndef get_circuit_info(url, offset, limit=1000):\n    \n    result = requests.get(url + str(limit) + '&offset=' + str(offset))\n    \n    return result.json()\n\n\n\n\nCode\ncircuit_json = get_qual_results(url = 'http://ergast.com/api/f1/circuits.json?limit=', offset = 0)\n\nwith open('../../data/00-raw-data/circuit_data.json', 'w') as outfile:\n    json.dump(circuit_json, outfile)\n\n\n\n\nCode\ncircuit_list = circuit_json['MRData']['CircuitTable']['Circuits']\n\n\n\n\nCode\ncircuit_df = pd.DataFrame(circuit_list)\n\n\n\n\nCode\ncircuit_df.to_csv('../../data/00-raw-data/circuit_info.csv')"
  },
  {
    "objectID": "pages/codes/record_datagathering.html#driver-standings-information",
    "href": "pages/codes/record_datagathering.html#driver-standings-information",
    "title": "Record Data Gathering",
    "section": "Driver Standings Information",
    "text": "Driver Standings Information\nAfter each Round, points are allocated to winners of the race that follow a set of rules. All the points are combined at the end of every season for the World Driver’s Championship. The driver with the highest points wins the WDC. This data contains the standings and points of each driver that competed in each season at the end of season.\n\n\nCode\ndef get_driverstanding_info(url, offset, limit=1000):\n    \n    result = requests.get(url + str(limit) + '&offset=' + str(offset))\n    \n    return result.json()\n\n\n\n\nCode\ndriverstanding_json = get_driverstanding_info(url = 'http://ergast.com/api/f1/driverStandings.json?limit=', offset = 2000)\n\nwith open('../../data/00-raw-data/driverstanding_data.json', 'w') as outfile:\n    json.dump(driverstanding_json, outfile)\n\n\n\n\nCode\ndef all_driverstandings():\n    \n    limit = 1000\n    result = []\n    length_per_page = []\n    url = 'http://ergast.com/api/f1/driverStandings.json?limit='\n    \n    p = 0\n    \n    while p < 100:\n        \n        page_result_json = get_driverstanding_info(url = url, offset = p*1000)\n        list_per_page = page_result_json['MRData']['StandingsTable']['StandingsLists']\n    \n        if len(list_per_page) == 0:\n            break\n        \n        length_per_page.append(len(list_per_page))\n        \n        for i in range(len(list_per_page)):\n            result.append(list_per_page[i])\n        \n        p = p + 1\n        \n\n        \n    return result, length_per_page\n\n\n\n\nCode\nresult_all_driverstandings,length_per_page = all_driverstandings()\n\n\n\n\nCode\ndriver_standings_df = pd.DataFrame(result_all_driverstandings)\n\n\n\n\nCode\ndriver_standings_df.to_csv('../../data/00-raw-data/driver_standings.csv')"
  },
  {
    "objectID": "pages/codes/record_datagathering.html#constructor-standings",
    "href": "pages/codes/record_datagathering.html#constructor-standings",
    "title": "Record Data Gathering",
    "section": "Constructor Standings",
    "text": "Constructor Standings\nA Constructor in F1 is a term for teams. Each constructor has 2 drivers with identical cars competing in a season. At the end of the season the points of both the drivers are combined for the Constructor’s Cup. The team with the highest points wins that cup. This data contains the standings and points of every Constructor that competed in each season at the end of season.\n\n\nCode\ndef get_constructorstanding_info(url, offset, limit=1000):\n    \n    result = requests.get(url + str(limit) + '&offset=' + str(offset))\n    \n    return result.json()\n\n\n\n\nCode\nconstructortanding_json = get_constructorstanding_info(url = 'http://ergast.com/api/f1/constructorStandings.json?limit=', offset = 0)\n\nwith open('../../data/00-raw-data/constructorstanding_data.json', 'w') as outfile:\n    json.dump(constructortanding_json, outfile)\n\n\n\n\nCode\nconstructortanding_list = constructortanding_json['MRData']['StandingsTable']['StandingsLists']\n\n\n\n\nCode\nconstructortanding_df = pd.DataFrame(constructortanding_list)\n\n\n\n\nCode\nconstructortanding_df.to_csv('../../data/00-raw-data/constructor_standings.csv')"
  },
  {
    "objectID": "pages/codes/record_datagathering.html#season-information",
    "href": "pages/codes/record_datagathering.html#season-information",
    "title": "Record Data Gathering",
    "section": "Season Information",
    "text": "Season Information\nEvery season there is either new set of drivers or tracks or rules. This data has all the information of the tracks of all the rounds of every season.\n\n\nCode\ndef get_season_info(url, offset, limit=1000):\n    \n    result = requests.get(url + str(limit) + '&offset=' + str(offset))\n    \n    return result.json()\n\n\n\n\nCode\ndef get_each_season_info(url, offset, season, limit=1000):\n    \n    result = requests.get(url + season + '.json?limit=' + str(limit) + '&offset=' + str(offset))\n    \n    return result.json()\n\n\n\n\nCode\nseason_json = get_season_info(url = 'http://ergast.com/api/f1/seasons.json?limit=', offset = 0)\n\nwith open('../../data/00-raw-data/season_data.json', 'w') as outfile:\n    json.dump(season_json, outfile)\n\n\n\n\nCode\nseason_list = season_json['MRData']['SeasonTable']['Seasons']\n\n\n\n\nCode\nseason_df = pd.DataFrame(season_list)\n\n\n\n\nCode\nseason_df.to_csv('../../data/00-raw-data/season_info.csv')\n\n\n\n\nCode\nall_season_list = []\n\nfor i in range(len(season_df)):\n    \n    temp_json = get_each_season_info(url = 'http://ergast.com/api/f1/', season = season_df['season'][i], offset = 0)\n    temp_list = temp_json['MRData']['RaceTable']['Races']\n    \n    for j in range(len(temp_list)):\n        \n        all_season_list.append(temp_list[j])\n\n\n\n\nCode\nall_season_df = pd.DataFrame(all_season_list)\n\n\n\n\nCode\nall_season_df\n\n\n\n\n\n\n  \n    \n      \n      season\n      round\n      url\n      raceName\n      Circuit\n      date\n      time\n      FirstPractice\n      SecondPractice\n      ThirdPractice\n      Qualifying\n      Sprint\n    \n  \n  \n    \n      0\n      1950\n      1\n      http://en.wikipedia.org/wiki/1950_British_Gran...\n      British Grand Prix\n      {'circuitId': 'silverstone', 'url': 'http://en...\n      1950-05-13\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      1950\n      2\n      http://en.wikipedia.org/wiki/1950_Monaco_Grand...\n      Monaco Grand Prix\n      {'circuitId': 'monaco', 'url': 'http://en.wiki...\n      1950-05-21\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      1950\n      3\n      http://en.wikipedia.org/wiki/1950_Indianapolis...\n      Indianapolis 500\n      {'circuitId': 'indianapolis', 'url': 'http://e...\n      1950-05-30\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      1950\n      4\n      http://en.wikipedia.org/wiki/1950_Swiss_Grand_...\n      Swiss Grand Prix\n      {'circuitId': 'bremgarten', 'url': 'http://en....\n      1950-06-04\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      4\n      1950\n      5\n      http://en.wikipedia.org/wiki/1950_Belgian_Gran...\n      Belgian Grand Prix\n      {'circuitId': 'spa', 'url': 'http://en.wikiped...\n      1950-06-18\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1097\n      2023\n      19\n      https://en.wikipedia.org/wiki/2023_United_Stat...\n      United States Grand Prix\n      {'circuitId': 'americas', 'url': 'http://en.wi...\n      2023-10-22\n      19:00:00Z\n      {'date': '2023-10-20', 'time': '17:30:00Z'}\n      {'date': '2023-10-21', 'time': '18:00:00Z'}\n      NaN\n      {'date': '2023-10-20', 'time': '21:00:00Z'}\n      {'date': '2023-10-21', 'time': '22:00:00Z'}\n    \n    \n      1098\n      2023\n      20\n      https://en.wikipedia.org/wiki/2023_Mexico_City...\n      Mexico City Grand Prix\n      {'circuitId': 'rodriguez', 'url': 'http://en.w...\n      2023-10-29\n      20:00:00Z\n      {'date': '2023-10-27', 'time': '18:30:00Z'}\n      {'date': '2023-10-27', 'time': '22:00:00Z'}\n      {'date': '2023-10-28', 'time': '17:30:00Z'}\n      {'date': '2023-10-28', 'time': '21:00:00Z'}\n      NaN\n    \n    \n      1099\n      2023\n      21\n      https://en.wikipedia.org/wiki/2023_S%C3%A3o_Pa...\n      São Paulo Grand Prix\n      {'circuitId': 'interlagos', 'url': 'http://en....\n      2023-11-05\n      17:00:00Z\n      {'date': '2023-11-03', 'time': '14:30:00Z'}\n      {'date': '2023-11-04', 'time': '14:30:00Z'}\n      NaN\n      {'date': '2023-11-03', 'time': '18:00:00Z'}\n      {'date': '2023-11-04', 'time': '18:30:00Z'}\n    \n    \n      1100\n      2023\n      22\n      https://en.wikipedia.org/wiki/2023_Las_Vegas_G...\n      Las Vegas Grand Prix\n      {'circuitId': 'vegas', 'url': 'https://en.wiki...\n      2023-11-19\n      06:00:00Z\n      {'date': '2023-11-17', 'time': '04:30:00Z'}\n      {'date': '2023-11-17', 'time': '08:00:00Z'}\n      {'date': '2023-11-18', 'time': '04:30:00Z'}\n      {'date': '2023-11-18', 'time': '08:00:00Z'}\n      NaN\n    \n    \n      1101\n      2023\n      23\n      https://en.wikipedia.org/wiki/2023_Abu_Dhabi_G...\n      Abu Dhabi Grand Prix\n      {'circuitId': 'yas_marina', 'url': 'http://en....\n      2023-11-26\n      13:00:00Z\n      {'date': '2023-11-24', 'time': '09:30:00Z'}\n      {'date': '2023-11-24', 'time': '13:00:00Z'}\n      {'date': '2023-11-25', 'time': '10:30:00Z'}\n      {'date': '2023-11-25', 'time': '14:00:00Z'}\n      NaN\n    \n  \n\n1102 rows × 12 columns\n\n\n\n\n\nCode\nall_season_df.to_csv('../../data/00-raw-data/all_season_info.csv')"
  },
  {
    "objectID": "pages/codes/twitter_datagathering.html",
    "href": "pages/codes/twitter_datagathering.html",
    "title": "Twitter Data Gathering",
    "section": "",
    "text": "Data Gathering\n\nData Gathering and Pre-Processing is a very important step in any Data science project pipeline. It is undeniable that 80% of a data scientist’s time and effort is spent in collecting, cleaning and preparing the data for analysis because datasets come in various sizes and are different in nature. It is extremely important for a data scientist to reshape and refine the datasets into usable datasets, which can be leveraged for analytics.\nKnowledge is power, information is knowledge, and data is information in digitized form, at least as defined in IT. Hence, data is power. But before you can leverage that data into a successful strategy for your organization or business, you need to gather it. That’s your first step.\nBefore we define what is data collection, it’s essential to ask the question, “What is data?” The abridged answer is, data is various kinds of information formatted in a particular way. Therefore, data collection is the process of gathering, measuring, and analyzing accurate data from a variety of relevant sources to find answers to research problems, answer questions, evaluate outcomes, and forecast trends and probabilities.\nOur society is highly dependent on data, which underscores the importance of collecting it. Accurate data collection is necessary to make informed business decisions, ensure quality assurance, and keep research integrity.\nDuring data collection, the researchers must identify the data types, the sources of data, and what methods are being used. We will soon see that there are many different data collection methods. There is heavy reliance on data collection in research, commercial, and government fields.\nWhy Do We Need Data Collection?\n\nBefore a judge makes a ruling in a court case or a general creates a plan of attack, they must have as many relevant facts as possible. The best courses of action come from informed decisions, and information and data are synonymous.\nThe concept of data collection isn’t a new one but the world has changed. There is far more data available today, and it exists in forms that were unheard of a century ago. The data collection process has had to change and grow with the times, keeping pace with technology.\nWhether you’re in the world of academia, trying to conduct research, or part of the commercial sector, thinking of how to promote a new product, you need data collection to help you make better choices.\n\nWhat Are the Different Methods of Data Collection?\n\nSurveys\nTransactional Tracking\nInterviews and Focus Groups\nObservation\nOnline Tracking\nForms\nSocial Media Monitoring\nApplication Programming Interface\n\n\n\n\nTwitter API\n\nTwitter is what’s happening in the world and what people are talking about right now. You can access Twitter via the web or your mobile device. To share information on Twitter as widely as possible, we also provide companies, developers, and users with programmatic access to Twitter data through our APIs (application programming interfaces).\nAt the end of 2020, Twitter introduced a new Twitter API built from the ground up. Twitter API v2 comes with more features and data you can pull and analyze, new endpoints, and a lot of functionalities.\nWith the introduction of that new API, Twitter also introduced a new powerful free product for academics: The Academic Research product track.\nThe track grants free access to full-archive search and other v2 endpoints, with a volume cap of 10,000,000 tweets per month! If you want to know if you qualify for the track or not, check this link.\nYet since v2 of the API is fairly new, fewer resources exist if you run into issues through the process of collecting data for your research.\nTwitter data is unique from data shared by most other social platforms because it reflects information that users choose to share publicly. The API platform provides broad access to public Twitter data that users have chosen to share with the world. It also support APIs that allow users to manage their own non-public Twitter information (e.g., Direct Messages) and provide this information to developers whom they have authorized to do so.\n\n\n\nImport Libraries\n\n\nCode\nimport pandas as pd\nimport os\nimport time\nimport requests\nimport json\nimport csv\nfrom tqdm import tqdm\n\nimport tweepy\n\nimport requests\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\nfrom turtle import color\nfrom collections import Counter\n\n\n\n\nSet Twitter API Keys\n\n\nCode\nconsumer_key        = 'mvpqVTwR7IBgL4wLAF6VSV8Fd'\nconsumer_secret     = 'MPTgWHei1DLcLSI9BixrufChLU1t2L63a2Z01wqPPWeKnuquGR'\naccess_token        = '1567741611135717376-P5V6SK5VpbNK5E9sPqYjBJBNuYXSsw'\naccess_token_secret = 'ih0sZBZDmCieDXNAXjdPvxmjeLBvV9J6nkM8DfxhWwdPp'\nbearer_token        = 'AAAAAAAAAAAAAAAAAAAAAOJigwEAAAAA85lnBTZlhhTy84L4U2c%2BR8T4e7c%3DPNtcxQs4Lybq3eeN8CTBjyxCuPbPRv2DaZ3H5IgnkzbXj3WbPb'\n\n\n\n\nSet the Twitter authentication and bearer token\n\n\nCode\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\n\napi = tweepy.API(auth, wait_on_rate_limit=True)\nheaders = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n\n\nExtraction function\n\n\nCode\ndef search_twitter(query, max_results, bearer_token, start_time, end_time, tweet_fields):\n    \n    client = tweepy.Client(bearer_token = bearer_token)\n    tweets = tweepy.Paginator(client.search_recent_tweets, query=query, tweet_fields=tweet_fields,\n                                start_time=start_time, end_time=end_time).flatten(limit = max_results)\n    \n    tweet_search = []\n    for tweet in tweets:\n        tweet_search.append((tweet.text, tweet.author_id, tweet.created_at, tweet.lang))\n        \n    return tweet_search\n\n\nStoring the tweets to a CSV file\n\n\nCode\ndef store_tweets_f1(query_list):\n    \n    max_results = 1000\n    start_time = '2023-03-02T00:00:00Z'\n    end_time = '2023-03-07T00:00:00Z'\n    tweet_fields = 'text,author_id,created_at,lang'\n    \n    for query in query_list:\n        tweet_search = search_twitter(query + \" f1 -is:retweet\", max_results, bearer_token, start_time, end_time, tweet_fields)\n        df = pd.DataFrame(tweet_search, columns = ['text', 'author_id', 'created_at', 'lang'])\n        df = df[df['lang'] == 'en']\n        df = df[['text', 'lang']]\n        df.to_csv('../../data/00-raw-data/' + query + '_f1_tweets_2023.csv')\n        \n    return df\n\n\n\n\nCode\nquery_list = ['ferrari', 'mercedes', 'redbull', 'mclaren', 'aston martin', 'alpha tauri', 'alpine', 'williams', 'haas', 'alfa romeo']\n\n\n```{python}\nstore_tweets_f1(query_list)\n```"
  },
  {
    "objectID": "pages/codes/record_NB.html",
    "href": "pages/codes/record_NB.html",
    "title": "Naive Bayes’ for Record Data",
    "section": "",
    "text": "Import Libraries\n\n\nCode\nlibrary(tidyr)\nlibrary(tidyverse)\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ dplyr   1.0.10\n✔ tibble  3.1.8      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n✔ purrr   0.3.4      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\n\nCode\nlibrary(dplyr)\nlibrary(caret)\n\n\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n\n\nCode\nlibrary(e1071)\nlibrary(caTools)\nlibrary(yardstick)\n\n\nFor binary classification, the first factor level is assumed to be the event.\nUse the argument `event_level = \"second\"` to alter this as needed.\n\nAttaching package: 'yardstick'\n\nThe following objects are masked from 'package:caret':\n\n    precision, recall, sensitivity, specificity\n\nThe following object is masked from 'package:readr':\n\n    spec\n\n\nCode\nlibrary(naivebayes)\n\n\nnaivebayes 0.9.7 loaded\n\n\nCode\nlibrary(ggplot2)\nlibrary(psych)\n\n\n\nAttaching package: 'psych'\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n\n\nCode\nlibrary(sjPlot)\n\n\nInstall package \"strengejacke\" from GitHub (`devtools::install_github(\"strengejacke/strengejacke\")`) to load all sj-packages at once!\n\n\nCode\nlibrary(klaR)\n\n\nLoading required package: MASS\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\n\n\nImport Data\n\n\nCode\ndf = read_csv(\"../../data/02-model-data/data_cleaned.csv\")\n\n\nRows: 26941 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): season_round, status, constructorRef, weather, stop, label\ndbl (16): season, round, driverId, raceId, circuitId, position, points, grid...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nCode\nhead(df)\n\n\n# A tibble: 6 × 22\n  season round season…¹ drive…² raceId circu…³ posit…⁴ points  grid  laps status\n   <dbl> <dbl> <chr>      <dbl>  <dbl>   <dbl>   <dbl>  <dbl> <dbl> <dbl> <chr> \n1   1950     1 1950_1       642    833       9       1      9     1    70 Finis…\n2   1950     1 1950_1       786    833       9       2      6     2    70 Finis…\n3   1950     1 1950_1       686    833       9       3      4     4    70 Finis…\n4   1950     1 1950_1       704    833       9       4      3     6    68 Lapped\n5   1950     1 1950_1       627    833       9       5      2     9    68 Lapped\n6   1950     1 1950_1       619    833       9       6      0    13    67 Lapped\n# … with 11 more variables: constructorRef <chr>, weather <chr>, stop <chr>,\n#   age_on_race <dbl>, cumulative_points <dbl>, cumulative_laps <dbl>,\n#   pole_driverId <dbl>, pole_history <dbl>, win_driverId <dbl>,\n#   win_history <dbl>, label <chr>, and abbreviated variable names\n#   ¹​season_round, ²​driverId, ³​circuitId, ⁴​position\n\n\n\n\nData Pre-Processing and Visualization\n\n\nCode\nbarplot(table(df$label), col = 'lightblue', main = 'Distribution of Labels', xlab = 'Labels', ylab = 'Count')\n\n\n\n\n\n\n\nCode\nplot(df$cumulative_points, df$win_history, col = 'lightblue', main = 'Points vs Win History', xlab = 'Cumulative Points', ylab = 'Win History')\n\n\n\n\n\nConverting some numeric variables to factors:\n\n\nCode\na = factor(df$status, levels = unique(df$status))\ndf$status = as.integer(a)\n\n\n\n\nCode\na = factor(df$constructorRef, levels = unique(df$constructorRef))\ndf$constructorRef = as.integer(a)\n\n\n\n\nCode\na = factor(df$weather, levels = unique(df$weather))\ndf$weather = as.integer(a)\n\n\n\n\nCode\na = factor(df$stop, levels = unique(df$stop))\ndf$stop = as.integer(a)\n\n\n\n\nCode\na = factor(df$label, levels = unique(df$label))\ndf$label = as.integer(a)\n\n\nDropping unnecessary columns:\n\n\nCode\ndf = df[-c(1:3)]\n\n\n\n\nCode\ndf$label = as.factor(df$label)\ndf\n\n\n# A tibble: 26,941 × 19\n   driverId raceId circuitId position points  grid  laps status constr…¹ weather\n      <dbl>  <dbl>     <dbl>    <dbl>  <dbl> <dbl> <dbl>  <int>    <int>   <int>\n 1      642    833         9        1      9     1    70      1        1       1\n 2      786    833         9        2      6     2    70      1        1       1\n 3      686    833         9        3      4     4    70      1        1       1\n 4      704    833         9        4      3     6    68      2        2       1\n 5      627    833         9        5      2     9    68      2        2       1\n 6      619    833         9        6      0    13    67      2        3       1\n 7      787    833         9        7      0    15    67      2        3       1\n 8      741    833         9        8      0    14    65      2        2       1\n 9      784    833         9        9      0    16    64      2        4       1\n10      778    833         9       10      0    20    64      2        4       1\n# … with 26,931 more rows, 9 more variables: stop <int>, age_on_race <dbl>,\n#   cumulative_points <dbl>, cumulative_laps <dbl>, pole_driverId <dbl>,\n#   pole_history <dbl>, win_driverId <dbl>, win_history <dbl>, label <fct>, and\n#   abbreviated variable name ¹​constructorRef\n\n\nSplitting data into train and test set:\n\n\nCode\nset.seed(1973)\n\nsample <- sample(c(TRUE, FALSE), nrow(df), replace=TRUE, prob=c(0.8,0.2))\ntrain  <- df[sample, ]\ntest   <- df[!sample, ]\n\n\n\n\nCode\nnrow(train)\n\n\n[1] 21547\n\n\nCode\nnrow(test)\n\n\n[1] 5394\n\n\n\n\nNaive Bayes Model\n\nBayes’ Theorem: In probability theory and statistics, Bayes’ theorem (alternatively Bayes’ law or Bayes’ rule), named after Thomas Bayes, describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if the risk of developing health problems is known to increase with age, Bayes’ theorem allows the risk to an individual of a known age to be assessed more accurately (by conditioning it on their age) than simply assuming that the individual is typical of the population as a whole.\nNaive Bayes Algorithm is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\nA fruit might be categorized as an apple, for instance, if it is red, rounded, and around 3 inches in diameter. Even if these characteristics depend on one another or on the presence of other characteristics, each of these traits separately increases the likelihood that this fruit is an apple, which is why it is called “Naive.”\nNaive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\nBayes’ Theorem can be shown by this equation: \\[ P(C|X) = \\frac {P(X|C) * P(C)}{P(X)} \\]\nIn the above equation:\n\nP(C|X) is the posterior probability of class (C, target) given predictor (X, attributes).\nP(C) is the prior probability of class.\nP(X|C) is the likelihood which is the probability of predictor given class.\nP(X) is the prior probability of predictor.\n\nHow does Bayes Theorem work?\n\nLet’s take an example: A Path Lab is performing a Test of disease say “D” with two results “Positive” & “Negative.” They guarantee that their test result is 99% accurate: if you have the disease, they will give test positive 99% of the time. If you don’t have the disease, they will test negative 99% of the time. If 3% of all the people have this disease and test gives “positive” result, what is the probability that you actually have the disease?\nFor solving the above problem, we will have to use conditional probability.\n\nProbability of people suffering from Disease D, P(D) = 0.03 = 3%\nProbability that test gives “positive” result and patient have the disease, P(Pos | D) = 0.99 =99%\nProbability of people not suffering from Disease D, P(~D) = 0.97 = 97%\nProbability that test gives “positive” result and patient does have the disease, P(Pos | ~D) = 0.01 =1%\n\nFor calculating the probability that the patient actually have the disease i.e, P( D | Pos) we will use Bayes theorem.\nP(Pos) = P(D, pos) + P( ~D, pos) = P(pos|D) * P(D) + P(pos|~D) * P(~D) = 0.99 * 0.03 + 0.01 * 0.97 = 0.0394\nHence, P( D | Pos) = (P(Pos | D) * P(D)) / P(Pos) = (0.99 * 0.03) / 0.0394 = 0.753807107\nSo, Approximately 75% chances are there that the patient is actually suffering from disease.\nThis is how Bayes’ Theorem works.  Reference \n\nTypes of Naive Bayes Algorithms:\n\nGaussian Naïve Bayes Classifier: In Gaussian Naïve Bayes, continuous values associated with each feature are assumed to be distributed according to a Gaussian distribution (Normal distribution). When plotted, it gives a bell-shaped curve which is symmetric about the mean of the feature values.\nMultinomial Naïve Bayes Classifier: Feature vectors represent the frequencies with which certain events have been generated by a multinomial distribution. This is the event model typically used for document classification.\nBernoulli Naïve Bayes Classifier: In the multivariate Bernoulli event model, features are independent booleans (binary variables) describing inputs. Like the multinomial model, this model is popular for document classification tasks, where binary term occurrence (i.e. a word occurs in a document or not) features are used rather than term frequencies (i.e. frequency of a word in the document).\n\nApplications of Naive Bayes Algorithm:\n\nReal time Prediction: Naive Bayes is an eager learning classifier and it is sure fast. Thus, it could be used for making predictions in real time.\nMulti class Prediction: This algorithm is also well known for multi class prediction feature. Here we can predict the probability of multiple classes of target variable.\nText classification/ Spam Filtering/ Sentiment Analysis: Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments)\nRecommendation System: Naive Bayes Classifier and Collaborative Filtering together builds a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not\n\nAdvantages:\n\nIt is easy and fast to predict class of test data set. It also perform well in multi class prediction\nWhen assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\nIt performs well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).\n\nDisadvatages:\n\nIf categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation.\nOn the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\nAnother limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.\n\n\n\n\nCode\nset.seed(1973)\nmodel1=NaiveBayes(label ~., data=train)\nmodel = naive_bayes(label ~., data=train)\n\n\n\n\nCode\nplot(model1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAbove are the density line plots for all the feature variables for all 3 label values.\n\n\nCode\ntrain_pred=predict(model,train)\n\n\nWarning: predict.naive_bayes(): more features in the newdata are provided as\nthere are probability tables in the object. Calculation is performed based on\nfeatures to be found in the tables.\n\n\nCode\ntrain_cm = table(train_pred,train$label)\nconfusionMatrix(train_cm)\n\n\nConfusion Matrix and Statistics\n\n          \ntrain_pred     1     2     3\n         1  2375   327     0\n         2   310  3468   256\n         3     0  2569 12242\n\nOverall Statistics\n                                          \n               Accuracy : 0.8393          \n                 95% CI : (0.8344, 0.8442)\n    No Information Rate : 0.58            \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.6971          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: 1 Class: 2 Class: 3\nSensitivity            0.8845   0.5449   0.9795\nSpecificity            0.9827   0.9627   0.7161\nPos Pred Value         0.8790   0.8597   0.8265\nNeg Pred Value         0.9836   0.8346   0.9620\nPrevalence             0.1246   0.2954   0.5800\nDetection Rate         0.1102   0.1610   0.5682\nDetection Prevalence   0.1254   0.1872   0.6874\nBalanced Accuracy      0.9336   0.7538   0.8478\n\n\n\n\nCode\ntest_pred=predict(model,test)\n\n\nWarning: predict.naive_bayes(): more features in the newdata are provided as\nthere are probability tables in the object. Calculation is performed based on\nfeatures to be found in the tables.\n\n\nCode\ntest_cm = table(test_pred,test$label)\nconfusionMatrix(test_cm)\n\n\nConfusion Matrix and Statistics\n\n         \ntest_pred    1    2    3\n        1  669   89    1\n        2   85  864   68\n        3    0  661 2957\n\nOverall Statistics\n                                          \n               Accuracy : 0.8324          \n                 95% CI : (0.8222, 0.8423)\n    No Information Rate : 0.561           \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.694           \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n\nStatistics by Class:\n\n                     Class: 1 Class: 2 Class: 3\nSensitivity            0.8873   0.5353   0.9772\nSpecificity            0.9806   0.9595   0.7209\nPos Pred Value         0.8814   0.8496   0.8173\nNeg Pred Value         0.9817   0.8286   0.9611\nPrevalence             0.1398   0.2992   0.5610\nDetection Rate         0.1240   0.1602   0.5482\nDetection Prevalence   0.1407   0.1885   0.6707\nBalanced Accuracy      0.9339   0.7474   0.8490\n\n\nWe get training accuracy from our model as 83.93% and balanced accuracy as 93% for Podium, 75% for Top_10 and 84% for Outisde_Top_10. Test accuracy from our model as 83.24% and balanced accuracy as 93% for Podium, 74% for Top_10 and 85% for Outisde_Top_10. There is a not a lot of difference between train and test accuracy which means our model is not over fitted.\nConfusion Matrix for Train and Test Data:\n\n\nCode\ntrain_cm_df = data.frame(train_cm)\ncolnames(train_cm_df) = c('pred', 'truth', 'y')\n\nggplot(data = train_cm_df, mapping = aes(x = truth , y = pred)) +\n  geom_tile(aes(fill = y), colour = \"white\") +\n  labs(title = 'Confusion Matrix of Train Data') +\n  scale_x_discrete(labels=c(\"1\" = \"Podium\", \"2\" = \"Top_10\", \"3\" = \"Outide_Top_10\")) +\n  scale_y_discrete(labels=c(\"1\" = \"Podium\", \"2\" = \"Top_10\", \"3\" = \"Outide_Top_10\")) +\n  geom_text(aes(label = sprintf(\"%1.0f\", y)), vjust = 1, colour = 'white') +\n  #scale_fill_gradient(low = \"lightblue\", high = \"yellow\") +\n  theme_bw() + theme(legend.position = \"none\")\n\n\n\n\n\n\n\nCode\ntest_cm_df = data.frame(test_cm)\ncolnames(test_cm_df) = c('pred', 'truth', 'y')\n\nggplot(data = test_cm_df, mapping = aes(x = truth , y = pred)) +\n  geom_tile(aes(fill = y), colour = \"white\") +\n  labs(title = 'Confusion Matrix of Test Data') +\n  scale_x_discrete(labels=c(\"1\" = \"Podium\", \"2\" = \"Top_10\", \"3\" = \"Outide_Top_10\")) +\n  scale_y_discrete(labels=c(\"1\" = \"Podium\", \"2\" = \"Top_10\", \"3\" = \"Outide_Top_10\")) +\n  geom_text(aes(label = sprintf(\"%1.0f\", y)), vjust = 1, colour = 'white') +\n  # scale_fill_gradient(low = \"cyan\", high = \"darkgoldenrod1\") +\n  theme_bw() + theme(legend.position = \"none\")"
  },
  {
    "objectID": "pages/codes/twitter_NB.html",
    "href": "pages/codes/twitter_NB.html",
    "title": "Naive Bayes’ for Twitter Data",
    "section": "",
    "text": "I have taken English Tweets from Twitter of the 10 teams in Formula One from 1 week to create different Naive Bayes model for predicting which tweet belongs to which team.\nThe Data is cleaned in the previous sections.\nOverview of Data Cleaning:\n\nF1 tweets provide context about sentiments about fans all over the world. Since some fans do not speak English, they tend to tweet in languages other than English. After extracting 1000 tweets each for every team from Twitter. I have only taken tweets of the English language for better understandibility.\nVarious Pre-Processing tasks were applied on the tweet text like excess blank spaces, stopwords, numbers and punctuations were removed.\nFurthermore the tweets were tokenized and lemmatized for further analysis.\nI also calculated sentiments of tweets in order to understand the emotions of fans behind writing these tweets and to better understand the need of this project.\n\nThe master table has consists of 3,928 rows, 9 columns and 1 label column.\nThe label column is based on the teams that are racing in the current season (2022) that are:\n\nFerrari\nMercedes\nRedbull\nWilliams\nAlpha Tauri\nAlfa Romeo\nMcLaren\nAlpine\nHaas\nAston Martin"
  },
  {
    "objectID": "pages/codes/twitter_NB.html#count-vectorizer",
    "href": "pages/codes/twitter_NB.html#count-vectorizer",
    "title": "Naive Bayes’ for Twitter Data",
    "section": "Count Vectorizer",
    "text": "Count Vectorizer\n\nWhenever we work on any NLP related problem, we process a lot of textual data. The textual data after processing needs to be fed into the model.\nCharacters and words are incomprehensible to machines. So, when dealing with text data, we must represent it numerically so that the machine can understand it.\nThe Count Vectorizer method converts text to numerical data.\nCountVectorizer tokenizes (tokenization means dividing the sentences in words) the text along with performing very basic preprocessing. It removes the punctuation marks and converts all the words to lowercase.\nCountVectorizer creates a matrix in which each unique word is represented by a column of the matrix, and each text sample from the document is a row in the matrix. The value of each cell is nothing but the count of the word in that particular text sample.\nInside CountVectorizer, these words are not stored as strings. Rather, they are given a particular index value. This way of representation is known as a Sparse Matrix\nIn our dataset we are taking tweets of 10 different teams and vectorizing the data so that it can fed into our Naive Bayes’ Models.\n\n\n\nCode\nimport random as rd\nMyCV_content=CountVectorizer(input='content',\n                        stop_words='english'\n                        #max_features=100\n                        )\n\nMy_DTM2=MyCV_content.fit_transform(X)\nColNames=MyCV_content.get_feature_names()\nMy_DF_content=pd.DataFrame(My_DTM2.toarray(),columns=ColNames)\n\n\nMy_DF_content['LABEL'] = pd.DataFrame(y,columns=['LABEL'])\nrd.seed(1973)\nTrainDF, TestDF = train_test_split(My_DF_content, test_size=0.25)\nTrainLabels=TrainDF[\"LABEL\"]\nTestLabels=TestDF[\"LABEL\"]\n\nTrainDF = TrainDF.drop([\"LABEL\"], axis=1)\nTestDF = TestDF.drop([\"LABEL\"], axis=1)\n\nfrom collections import Counter\nCounter(y).keys()\nCounter(y).values()\n\n\n/Users/rd/opt/anaconda3/envs/anly503/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning:\n\nFunction get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n\n\n\ndict_values([453, 534, 322, 419, 550, 523, 574, 267, 106, 180])\n\n\n\n\nCode\nMy_DF_content.to_csv('../../data/02-model-data/twitter_data_count_vectorizer.csv')"
  },
  {
    "objectID": "pages/codes/twitter_NB.html#multinomial-naïve-bayes-classifier",
    "href": "pages/codes/twitter_NB.html#multinomial-naïve-bayes-classifier",
    "title": "Naive Bayes’ for Twitter Data",
    "section": "Multinomial Naïve Bayes Classifier",
    "text": "Multinomial Naïve Bayes Classifier\n\nThere are thousands of softwares or tools for the analysis of numerical data but there are very few for texts. Multinomial Naive Bayes is one of the most popular supervised learning classifications that is used for the analysis of the categorical text data.\nText data classification is gaining popularity because there is an enormous amount of information available in email, documents, websites, etc. that needs to be analyzed. Knowing the context around a certain type of text helps in finding the perception of a software or product to users who are going to use it.\nMultinomial Naive Bayes algorithm is a probabilistic learning method that is mostly used in Natural Language Processing (NLP). The algorithm is based on the Bayes theorem and predicts the tag of a text such as a piece of email or newspaper article. It calculates the probability of each tag for a given sample and then gives the tag with the highest probability as output.\nNaive Bayes classifier is a collection of many algorithms where all the algorithms share one common principle, and that is each feature being classified is not related to any other feature. The presence or absence of a feature does not affect the presence or absence of the other feature.\nSince we are dealing with text data (tweets) converted into numerical using Count Vectorizer, Multinomial Naive Bayes will be useful here.\nLaplace smoothing is a smoothing technique that handles the problem of zero probability in Naïve Bayes. It is controlled by the parameter ‘alpha’ in sklearn’s MultinomialNB. For this exercise we will take models with alpha as 1,5 and 10.\n\n\nModel 1 (alpha = 1)\n\n\nCode\nMyModelNB= MultinomialNB(alpha = 1)\n\nNB1=MyModelNB.fit(TrainDF, TrainLabels)\nPreds = MyModelNB.predict(TestDF)\nPred_Proba = MyModelNB.predict_proba(TestDF)\nprint(metrics.classification_report(TestLabels, Preds))\ncnf_matrix1 = confusion_matrix(TestLabels, Preds)\n\n##Visualise Confusion Matrix\nlabels = label_list\nax1=plt.subplot()\nsns.heatmap(confusion_matrix(TestLabels, Preds), annot=True, fmt='g', ax=ax1)\n\n# labels, title and ticks\nax1.set_xlabel('Predicted labels');ax1.set_ylabel('True labels');\nax1.set_title('Confusion Matrix for Model 1') \nax1.xaxis.set_ticklabels(labels)\nax1.yaxis.set_ticklabels(labels)\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, horizontalalignment='right')\nax1.set_yticklabels(ax1.get_yticklabels(), rotation=45, horizontalalignment='right')\nplt.savefig('../../images/Confusion Matrix for Model 1.png')\nplt.show()\nplt.close()\n\n\n              precision    recall  f1-score   support\n\n           0       0.75      0.66      0.70        32\n           1       0.89      0.31      0.46        26\n           2       0.61      0.66      0.63       140\n           3       0.85      0.88      0.86        64\n           4       0.81      0.72      0.76       119\n           5       0.47      0.57      0.52        89\n           6       0.76      0.83      0.79       136\n           7       0.74      0.82      0.78       140\n           8       0.89      0.64      0.75        90\n           9       0.77      0.75      0.76       146\n\n    accuracy                           0.72       982\n   macro avg       0.75      0.68      0.70       982\nweighted avg       0.74      0.72      0.72       982\n\n\n\n\n\n\n\n\nModel 2 (alpha = 5)\n\n\nCode\nMyModelNB2= MultinomialNB(alpha =5)\n\nNB2=MyModelNB2.fit(TrainDF, TrainLabels)\nPreds2 = MyModelNB2.predict(TestDF)\nPred_Proba2 = MyModelNB2.predict_proba(TestDF)\nprint(metrics.classification_report(TestLabels, Preds2))\ncnf_matrix1 = confusion_matrix(TestLabels, Preds2)\n\n##Visualise Confusion Matrix\nlabels = label_list\nax1=plt.subplot()\nsns.heatmap(confusion_matrix(TestLabels, Preds2), annot=True, fmt='g', ax=ax1);\n\n# labels, title and ticks\nax1=plt.subplot()\nsns.heatmap(confusion_matrix(TestLabels, Preds), annot=True, fmt='g', ax=ax1)\n\n# labels, title and ticks\nax1.set_xlabel('Predicted labels');ax1.set_ylabel('True labels');\nax1.set_title('Confusion Matrix for Model 2') \nax1.xaxis.set_ticklabels(labels)\nax1.yaxis.set_ticklabels(labels)\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, horizontalalignment='right')\nax1.set_yticklabels(ax1.get_yticklabels(), rotation=45, horizontalalignment='right')\nplt.savefig('../../images/Confusion Matrix for Model 2.png')\nplt.show()\nplt.close()\n\n\n              precision    recall  f1-score   support\n\n           0       0.87      0.62      0.73        32\n           1       1.00      0.15      0.27        26\n           2       0.66      0.74      0.69       140\n           3       0.86      0.86      0.86        64\n           4       0.93      0.73      0.82       119\n           5       0.58      0.63      0.61        89\n           6       0.78      0.84      0.81       136\n           7       0.72      0.88      0.79       140\n           8       0.92      0.63      0.75        90\n           9       0.74      0.83      0.78       146\n\n    accuracy                           0.75       982\n   macro avg       0.80      0.69      0.71       982\nweighted avg       0.77      0.75      0.75       982\n\n\n\n/var/folders/80/kkd433150p52z36v9dx3c_p00000gn/T/ipykernel_48916/2323361071.py:15: MatplotlibDeprecationWarning:\n\nAuto-removal of overlapping axes is deprecated since 3.6 and will be removed two minor releases later; explicitly call ax.remove() as needed.\n\n\n\n\n\n\n\n\nModel 3 (alpha = 10)\n\n\nCode\nMyModelNB3= MultinomialNB(alpha =10)\n\nNB3=MyModelNB3.fit(TrainDF, TrainLabels)\nPreds3 = MyModelNB3.predict(TestDF)\nPred_Proba3 = MyModelNB3.predict_proba(TestDF)\nprint(metrics.classification_report(TestLabels, Preds3))\ncnf_matrix1 = confusion_matrix(TestLabels, Preds3)\n\n##Visualise Confusion Matrix\nlabels = label_list\nax1=plt.subplot()\nsns.heatmap(confusion_matrix(TestLabels, Preds3), annot=True, fmt='g', ax=ax1);\n\n# labels, title and ticks\nax1.set_xlabel('Predicted labels');ax1.set_ylabel('True labels');\nax1.set_title('Confusion Matrix for Model 3') \nax1.xaxis.set_ticklabels(labels)\nax1.yaxis.set_ticklabels(labels)\nax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, horizontalalignment='right')\nax1.set_yticklabels(ax1.get_yticklabels(), rotation=45, horizontalalignment='right')\nplt.savefig('../../images/Confusion Matrix for Model 3.png')\nplt.show()\nplt.close()\n\n\n              precision    recall  f1-score   support\n\n           0       0.95      0.59      0.73        32\n           1       1.00      0.15      0.27        26\n           2       0.67      0.74      0.71       140\n           3       0.88      0.88      0.88        64\n           4       0.95      0.72      0.82       119\n           5       0.60      0.65      0.63        89\n           6       0.76      0.86      0.81       136\n           7       0.71      0.89      0.79       140\n           8       0.96      0.60      0.74        90\n           9       0.74      0.84      0.79       146\n\n    accuracy                           0.76       982\n   macro avg       0.82      0.69      0.71       982\nweighted avg       0.78      0.76      0.75       982"
  },
  {
    "objectID": "pages/codes/record_datacleaningPython.html",
    "href": "pages/codes/record_datacleaningPython.html",
    "title": "Record Data Cleaning in Python",
    "section": "",
    "text": "Data Cleaning\n\nIn the era of big data, cleaning or scrubbing your data has become an essential part of the data management process. Even though data cleaning can be tedious at times, it is absolutely crucial for getting accurate business intelligence (BI) that can drive your strategic decisions.\nIncorrect or inconsistent data leads to false conclusions. And so, how well you clean and understand the data has a high impact on the quality of the results.\nData cleaning involve different techniques based on the problem and the data type. Different methods can be applied with each has its own trade-offs. Overall, incorrect data is either removed, corrected, or imputed.\nData cleaning is the process of removing incorrect, duplicate, or otherwise erroneous data from a dataset. These errors can include incorrectly formatted data, redundant entries, mislabeled data, and other issues; they often arise when two or more datasets are combined together. Data cleaning improves the quality of your data as well as any business decisions that you draw based on the data.\nThere is no one right way to clean a dataset, as every set is different and presents its own unique slate of errors that need to be corrected. Many data cleaning techniques can now be automated with the help of dedicated software, but some portion of the work must be done manually to ensure the greatest accuracy. Usually this work is done by data quality analysts, BI analysts, and business users.\nEvery organization’s data cleaning methods will vary according to their individual needs as well as the particular constraints of the dataset. However, most data cleaning steps follow a standard framework:\n\nDetermine the critical data values you need for your analysis.\nCollect the data you need, then sort and organize it.\nIdentify duplicate or irrelevant values and remove them.\nSearch for missing values and fill them in, so you have a complete dataset.\nFix any remaining structural or repetitive errors in the dataset.\nIdentify outliers and remove them, so they will not interfere with your analysis.\nValidate your dataset to ensure it is ready for data transformation and analysis.\nOnce the set has been validated, perform your transformation and analysis.\n\nData Cleaning vs. Data Cleansing vs. Data Scrubbing\n\nYou might sometimes hear the terms data cleansing or data scrubbing used instead of data cleaning. In most situations, these terms are all being used interchangeably and refer to the exact same thing. Data scrubbing may sometimes be used to refer to a specific aspect of data cleaning—namely, removing duplicate or bad data from datasets.\nYou should also know that data scrubbing can have a slightly different meaning within the specific context of data storage; in this case, it refers to an automated function that evaluates storage systems and disk drives to identify any bad sectors or blocks and to confirm the data in them can be read.\nNote that all three of these terms—data cleaning, data cleansing, and data scrubbing—are different from data transformation, which is the act of taking clean data and converting it into a new format or structure. Data transformation is a separate process that comes after data cleaning.\n\nBenefits of Data Cleaning:\n\nNot having clean data exacts a high price: IBM estimates that bad data costs the U.S. over $3 trillion each year. That’s because data-driven decisions are only as good as the data you are relying on. Bad quality data leads to equally bad quality decisions. If the data you are basing your strategy on is inaccurate, then your strategy will have the same issues present in the data, even if it seems sound. In fact, sometimes no data at all is better than bad data.\nCleaning your data results in many benefits for your organization in both the short- and long-term. It leads to better decision making, which can boost your efficiency and your customer satisfaction, in turn giving your business a competitive edge. Over time, it also reduces your costs of data management by preemptively removing errors and other mistakes that would necessitate performing analysis over and over again.\n\n\n\n\nImport Libraries\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport ast\nimport json\nimport requests\nfrom bs4 import BeautifulSoup\nfrom lxml import html\nimport requests\nfrom numpy import NaN\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n\n\nImport Data\n\n\nCode\ndf = pd.read_csv('../../data/01-modified-data/data_cleaning_R.csv')\ndf.head()\n\n\n\n\nCleaning the Data\n\n\nCode\na = 0\nb = 1\nc = 2\nd = 3\ne = 4\n\ndf['Driver_Position'] = 0\ndf['Driver_Points'] = 0\ndf['driverRef'] = 0\ndf['constructorRef'] = 0\ndf['grid_pos'] = 0\ndf['race_status'] = 0\ndf['race_time'] = 0\ndf['completed_laps'] = 0\n\nfor i in range(1096):\n    \n    df['Driver_Position'][a] = df['Driver_Position_1'][a]\n    df['Driver_Position'][b] = df['Driver_Position_2'][b]\n    df['Driver_Position'][c] = df['Driver_Position_3'][c]\n    df['Driver_Position'][d] = df['Driver_Position_4'][d]\n    df['Driver_Position'][e] = df['Driver_Position_5'][e]\n    \n    df['Driver_Points'][a] = df['Driver_Points_1'][a]\n    df['Driver_Points'][b] = df['Driver_Points_2'][b]\n    df['Driver_Points'][c] = df['Driver_Points_3'][c]\n    df['Driver_Points'][d] = df['Driver_Points_4'][d]\n    df['Driver_Points'][e] = df['Driver_Points_5'][e]\n    \n    df['driverRef'][a] = df['driverRef_1'][a]\n    df['driverRef'][b] = df['driverRef_2'][b]\n    df['driverRef'][c] = df['driverRef_3'][c]\n    df['driverRef'][d] = df['driverRef_4'][d]\n    df['driverRef'][e] = df['driverRef_5'][e]\n    \n    df['constructorRef'][a] = df['constructorRef_1'][a]\n    df['constructorRef'][b] = df['constructorRef_2'][b]\n    df['constructorRef'][c] = df['constructorRef_3'][c]\n    df['constructorRef'][d] = df['constructorRef_4'][d]\n    df['constructorRef'][e] = df['constructorRef_5'][e]\n    \n    df['grid_pos'][a] = df['grid_pos_1'][a]\n    df['grid_pos'][b] = df['grid_pos_2'][b]\n    df['grid_pos'][c] = df['grid_pos_3'][c]\n    df['grid_pos'][d] = df['grid_pos_4'][d]\n    df['grid_pos'][e] = df['grid_pos_5'][e]\n    \n    df['race_status'][a] = df['race_status_1'][a]\n    df['race_status'][b] = df['race_status_2'][b]\n    df['race_status'][c] = df['race_status_3'][c]\n    df['race_status'][d] = df['race_status_4'][d]\n    df['race_status'][e] = df['race_status_5'][e]\n    \n    df['race_time'][a] = df['race_time_1'][a]\n    df['race_time'][b] = df['race_time_2'][b]\n    df['race_time'][c] = df['race_time_3'][c]\n    df['race_time'][d] = df['race_time_4'][d]\n    df['race_time'][e] = df['race_time_5'][e]\n    \n    df['completed_laps'][a] = df['completed_laps_1'][a]\n    df['completed_laps'][b] = df['completed_laps_2'][b]\n    df['completed_laps'][c] = df['completed_laps_3'][c]\n    df['completed_laps'][d] = df['completed_laps_4'][d]\n    df['completed_laps'][e] = df['completed_laps_5'][e]\n    \n    a = a+5\n    b = b+5\n    c = c+5\n    d = d+5\n    e = e+5\n    \ndrop = ['Driver_Position_1', 'Driver_Position_2', 'Driver_Position_3', 'Driver_Position_4', 'Driver_Position_5',\n             'Driver_Points_1', 'Driver_Points_2', 'Driver_Points_3', 'Driver_Points_4', 'Driver_Points_5',\n             'driverRef_1', 'driverRef_2', 'driverRef_3', 'driverRef_4', 'driverRef_5',\n             'constructorRef_1', 'constructorRef_2', 'constructorRef_3', 'constructorRef_4', 'constructorRef_5',\n             'grid_pos_1', 'grid_pos_2', 'grid_pos_3', 'grid_pos_4', 'grid_pos_5', \n             'completed_laps_1', 'completed_laps_2', 'completed_laps_3', 'completed_laps_4', 'completed_laps_5',\n             'race_status_1', 'race_status_2', 'race_status_3', 'race_status_4', 'race_status_5',\n             'race_time_1', 'race_time_2', 'race_time_3', 'race_time_4', 'race_time_5']\n\ndf.drop(drop, axis = 1, inplace=True)\n\ndf.to_csv('../../data/01-modified-data/race_result_cleaned.csv')\n\n\nGetting and Un-packing Driver standings Information from our Raw data to merge it in the master data:\n\n\nCode\nds_df = pd.read_csv('../../data/00-raw-data/driver_standings.csv')\nast.literal_eval(ds_df['DriverStandings'][0])[0]\n\n\n\n\nCode\nds_df\n\n\n\n\nCode\nseason_list = list(df['season'].unique())\nds_df_1 = pd.DataFrame(columns=['driverRef'] + season_list)\nds_df_1['driverRef'] = ds_df['driverRef']\nds_df_1.fillna(0, inplace=True)\n\n\n\n\nCode\nds_df_1\n\n\n\n\nCode\nfor i in range(len(ds_df)):\n    \n    for j in range(len(ast.literal_eval(ds_df['DriverStandings'][i]))):\n        \n        points = ast.literal_eval(ds_df['DriverStandings'][i])[j]['points']\n        driver = ast.literal_eval(ds_df['DriverStandings'][i])[j]['Driver']['driverId']\n        \n        if points == '':\n            points = 0\n            \n        for k in range(len(ds_df_1)):\n            \n            if driver == ds_df_1['driverRef'][k]:\n                ds_df_1[int(ds_df['season'][i])][k] = points\n\n\n\n\nCode\nds_df_1\n\n\n\n\nCode\nrace_results_df = pd.read_csv('../../data/00-raw-data/race_results.csv')\n\n\n\n\nCode\nrace_results_df\n\n\n\n\nCode\ndriver_list = list(driver_df['driverRef'].unique())\n\n\nFinding the Cumulative Points of a driver for every race he has raced:\n\n\nCode\npoint_history_dict = {i : 0 for i in driver_list}\n\n\n\n\nCode\ndf['Cumulative_Points'] = 0\n\nfor i in range(len(race_results_df)):\n    \n    res_dict = ast.literal_eval(race_results_df['Results'][i])\n    \n    for j in range(len(res_dict)):\n        \n        for k in range(len(df)):\n        \n            if (race_results_df['season'][i] == df['season'][k]) & (race_results_df['round'][i] == df['round'][k]) & (res_dict[j]['Driver']['driverId'] == df['driverRef'][k]):\n                    \n                if res_dict[j]['points'] == '':\n                    res_dict[j]['points'] = 0\n                \n                df['Cumulative_Points'][k] = point_history_dict[res_dict[j]['Driver']['driverId']] + float(res_dict[j]['points'])    \n                point_history_dict[res_dict[j]['Driver']['driverId']] += float(res_dict[j]['points'])    \n\n\n\n\nCode\ndriver_df\n\n\nFinding the year of birth of each driver at the time of the race:\n\n\nCode\ndf['yob'] = 0\nfor i in range(len(df)):\n    \n    for j in range(len(driver_df)):\n        \n        if df['driverRef'][i] == driver_df['driverRef'][j]:\n            df['yob'][i] = int(driver_df['dob'][j][:4])\n\n\nFinding the cumulative wins of each driver up until each race:\n\n\nCode\nwin_history_dict = {i : 0 for i in driver_list}    \n\n\n\n\nCode\ndf['Cumulative_Wins'] = 0\n\nfor i in range(len(df)):\n    \n    if df['Driver_Position'][i] == 1:\n        \n        df['Cumulative_Wins'][i] = win_history_dict[df['driverRef'][i]] + 1\n        win_history_dict[df['driverRef'][i]] += 1\n    \n    else:\n        df['Cumulative_Wins'][i] = win_history_dict[df['driverRef'][i]]\n\n\n\n\nCode\ndf.tail(40)\n\n\n\n\nCode\npodium_per_race = {}\n\nfor i in range(len(race_results_df)):\n    \n    result = ast.literal_eval(race_results_df['Results'][i])\n     \n    for j in range(len(result)):\n         \n        if result[j]['grid'] == '1':\n            \n            driver = result[j]['Driver']['driverId']\n             \n            podium_per_race[str(race_results_df['season'][i]) + '_' + str(race_results_df['round'][i])] = driver\n\n\n\n\nCode\ndf['Podiums'] = ''\n\nfor i in range(len(df)):\n    \n    for keys in podium_per_race:\n    \n        if str(df['season'][i]) + '_' + str(df['round'][i]) == keys:\n            \n            df['Podiums'][i] = podium_per_race[keys]\n\n\n\n\nCode\ndf1 = df.groupby(['season', 'round'], as_index = False)['Podiums'].max()\n\n\n\n\nCode\ndf1['counter'] = ''\n\nfor i in range(len(df1)):\n    \n    df1['counter'][i] = dict(Counter(df1['Podiums'][:i+1]))\n\n\n\n\nCode\ndf = pd.merge(df, df1, on = ['season', 'round'], how = 'left')\n\n\n\n\nCode\ndf.drop(['Podiums_y'], axis = 1, inplace=True)\n\n\n\n\nCode\ndf.rename(columns = {'Podiums_x' : 'Podiums'}, inplace=True)\n\n\n\n\nCode\ndf['podium_count'] = 0\n\nfor i in range(len(df)):\n    \n    if df['driverRef'][i] in df['counter'][i].keys():\n        df['podium_count'][i] = df['counter'][i][df['driverRef'][i]]\n    \n    else:\n        df['podium_count'][i] = 0\n\n\nMerging the season and round into one column for easy computations:\n\n\nCode\nseason_info = df.groupby(['season', 'round'], as_index = False).count().iloc[:, :2]\n\n\n\n\nCode\nrace_results_df['season_round'] = ''\n\nfor i in range(len(race_results_df)):\n    \n    race_results_df['season_round'][i] = str(race_results_df['season'][i]) + '_' + str(race_results_df['round'][i])\n\n\n\n\nCode\nseason_info['season_round'] = ''\n\nfor i in range(len(season_info)):\n    \n    season_info['season_round'][i] = str(season_info['season'][i]) + '_' + str(season_info['round'][i])\n\n\n\n\nCode\ndf1 = pd.merge(season_info, race_results_df, on = 'season_round', how = 'inner')\n\n\n\n\nCode\ndf1.drop(['season_y', 'round_y', 'Unnamed: 0'], axis = 1, inplace=True)\n\n\n\n\nCode\ndf1\n\n\nUnzipping the results of each driver in each race to create our main dataframe:\n\n\nCode\nmain_df = pd.DataFrame()\n\nfor i in range(len(df1)):\n    \n    data1 = pd.DataFrame(ast.literal_eval(df1['Results'][i]))\n    data1['season'] = df1['season_x'][i]\n    data1['round'] = df1['round_x'][i]\n    data1['season_round'] = df1['season_round'][i]\n    data1['circuitId'] = ast.literal_eval(df1['Circuit'][i])['circuitId']\n    data1['date'] = df1['date'][i]\n    data1['url'] = df1['url'][i]\n    data1['raceName '] = df1['raceName'][i]\n    \n    main_df = pd.concat([main_df, data1], axis = 0)\n\n\n\n\nCode\nmain_df.reset_index(inplace=True)\n\n\n\n\nCode\nmain_df\n\n\n\n\nCode\nmain_df['driverRef'] = ''\nmain_df['yob'] = 0\nmain_df['constructorRef'] = ''\n\nfor i in range(len(main_df)):\n    \n    main_df['driverRef'][i] = main_df['Driver'][i]['driverId']\n    main_df['yob'][i] = int(main_df['Driver'][i]['dateOfBirth'][:4])\n    main_df['constructorRef'][i] = main_df['Constructor'][i]['constructorId']\n\n\nDropping useless columns:\n\n\nCode\nmain_df.drop(['Driver', 'Constructor', 'index', 'number', 'positionText', 'Time'], axis = 1, inplace=True)\n\n\n\n\nCode\nmain_df\n\n\nWebscraping weather information of each race of each season from wikipedia infobox:\n\n\nCode\ndef get_infobox(url):\n    response = requests.get(url)\n    bs = BeautifulSoup(response.text)\n\n    table = bs.find('table', {'class' :'infobox vevent'})\n    result = {}\n    row_count = 0\n    if table is None:\n        pass\n    else:\n        for tr in table.find_all('tr'):\n            if tr.find('th'):\n                key = tr.find('th').text.strip()\n                if tr.find('td'):\n                    value = tr.find('td').text.strip()\n                    result[key] = value\n                else:\n                    result[key] = ''\n            else:\n                pass\n    return result\n\n\n\n\nCode\nurl_df = main_df.groupby(['season_round'], as_index = False)['url'].max()\n\n\n\n\nCode\nurl_df['weather'] = np.nan\n\nfor i in range(0, len(url_df)):\n    infobox = get_infobox(url_df['url'][i])\n    infobox_keys = get_infobox(url_df['url'][i]).keys()\n    \n    if 'Weather' in infobox_keys:\n        url_df['weather'][i] = infobox['Weather']\n    else:\n        url_df['weather'][i] = np.nan\n\n\n\n\nCode\nmain_df = pd.merge(main_df, url_df, on = 'season_round', how = 'left')\n\n\n\n\nCode\nmain_df\n\n\nClassifying weather into Rainy, Snowy, Windy, Cloudy, Fine, Sunny:\n\n\nCode\nfor i in range(len(main_df)):\n    \n    if main_df['weather'][i] is not NaN:\n        \n        if ('rain' in main_df['weather'][i].lower()) | ('wet' in main_df['weather'][i].lower()) | ('showers' in main_df['weather'][i].lower()) | ('drizzle' in main_df['weather'][i].lower()) | ('rainy' in main_df['weather'][i].lower()) | ('drizzly' in main_df['weather'][i].lower()):\n            main_df['weather'][i] = 'Rainy'\n        \n        elif ('snow' in main_df['weather'][i].lower()) | ('cold' in main_df['weather'][i].lower()) | ('frost' in main_df['weather'][i].lower()) | ('icy' in main_df['weather'][i].lower()):\n            main_df['weather'][i] = 'Snowy'\n        \n        elif ('windy' in main_df['weather'][i].lower()) | ('wind' in main_df['weather'][i].lower()) | ('gusty' in main_df['weather'][i].lower()) | ('blustery' in main_df['weather'][i].lower()):\n            main_df['weather'][i] = 'Windy'\n        \n        elif ('cloudy' in main_df['weather'][i].lower()) | ('overcast' in main_df['weather'][i].lower()) | ('partly cloudy' in main_df['weather'][i].lower()) | ('partly sunny' in main_df['weather'][i].lower()) | ('hazy' in main_df['weather'][i].lower()) | ('foggy' in main_df['weather'][i].lower()) | ('fog' in main_df['weather'][i].lower()) | ('misty' in main_df['weather'][i].lower()) | ('mist' in main_df['weather'][i].lower()) | ('cloud' in main_df['weather'][i].lower()):\n            main_df['weather'][i] = 'Cloudy'\n            \n        elif ('fine' in main_df['weather'][i].lower()) | ('fair' in main_df['weather'][i].lower()) | ('mild' in main_df['weather'][i].lower()):\n            main_df['weather'][i] = 'Fine'\n        \n        elif ('sunny' in main_df['weather'][i].lower()) | ('clear' in main_df['weather'][i].lower()) | ('warm' in main_df['weather'][i].lower()) | ('hot' in main_df['weather'][i].lower()) | ('dry' in main_df['weather'][i].lower()):\n            main_df['weather'][i] = 'Sunny'\n            \n        else:\n            main_df['weather'][i] = 'Sunny'\n    \n    else:\n        pass\n\n\n\n\nCode\nmain_df['weather'].value_counts()\n\n\n\n\nCode\nmain_df.drop(['url_y', 'raceName\\t', 'FastestLap'], axis = 1, inplace=True)\n\n\n\n\nCode\nmain_df\n\n\nImporting other raw datas for further cleaning and engineering:\n\n\nCode\npitstop_df = pd.read_csv('../../data/00-raw-data/pit_stops.csv')\npitstop_count = pitstop_df.groupby(['raceId', 'driverId'], as_index=False)['stop'].count()\n\n\n\n\nCode\nrace_df = pd.read_csv('../../data/00-raw-data/races.csv')\nrace_df.rename(columns={'year':'season'}, inplace=True)\nrace_df =  race_df[['raceId', 'season', 'round', 'circuitId']]\nrace_df.head()\n\n\n\n\nCode\ndriver_df = pd.read_csv('../../data/00-raw-data/drivers.csv')\ndriver_df.head()\n\n\n\n\nCode\npitstop_count\n\n\nMerging raw datasets with main data to get additional feature columns:\n\n\nCode\nmain_df = pd.merge(main_df, driver_df, on = ['driverRef'], how = 'left')\n\n\n\n\nCode\nmain_df.drop(['url_x', 'driverRef', 'code', 'number', 'nationality', 'forename', 'surname', 'dob'], axis = 1, inplace=True)\n\n\n\n\nCode\nmain_df = pd.merge(main_df, race_df, on = ['season', 'round'], how = 'left')\n\n\n\n\nCode\nmain_df.rename(columns={'circuitId_x' : 'circuitRef', 'circuitId_y' : 'circuitId'}, inplace=True)\nmain_df\n\n\n\n\nCode\nmain_df = pd.merge(main_df, pitstop_count, on = ['raceId', 'driverId'], how = 'left')\n\n\n\n\nCode\nmain_df\n\n\n\n\nCode\nfor i in range(len(main_df)):\n    \n    if ('Lap' in main_df['status'][i]) | ('lap' in main_df['status'][i]):\n        main_df['status'][i] = 'Lapped'\n    \n    elif 'Accident' in main_df['status'][i]:\n        main_df['status'][i] = 'Accident'\n        \n    elif 'Finished' in main_df['status'][i]:\n        main_df['status'][i] = 'Finished'\n        \n    else:\n        main_df['status'][i] = 'Mechanical_Issue'\n\n\n\n\nCode\nmain_df\n\n\n\n\nCode\nmain_df['age_on_race'] = main_df['season'] - main_df['yob']\nmain_df.drop('yob', axis = 1, inplace=True)\n\n\nReplacing null values in Pit stop and Weather columns with string “Not Available”:\n\n\nCode\nmain_df['stop'].fillna('Not Available', inplace=True)\nmain_df['weather'].fillna('Not Available', inplace=True)\n\n\nConverting other numerical values in stop column to text:\n\n\nCode\nfor i in range(len(main_df)):\n    \n    if main_df['stop'][i] == 1.0:\n        main_df['stop'][i] = 'One'\n    \n    elif main_df['stop'][i] == 2.0:\n        main_df['stop'][i] = 'Two'\n    \n    elif main_df['stop'][i] == 3.0:\n        main_df['stop'][i] = 'Three'\n    \n    elif main_df['stop'][i] == 4.0:\n        main_df['stop'][i] = 'Four'\n    \n    elif main_df['stop'][i] == 5.0:\n        main_df['stop'][i] = 'Five'\n    \n    elif main_df['stop'][i] == 6.0:\n        main_df['stop'][i] = 'Six'\n        \n    elif main_df['stop'][i] == 7.0:\n        main_df['stop'][i] = 'Seven'\n        \n    else:\n        pass\n\n\n\n\nCode\nmain_df\n\n\n\n\nCode\nmain_df[['position', 'points', 'grid', 'laps', 'season', 'round', 'driverId', 'raceId', 'circuitId', 'age_on_race']] = main_df[['position', 'points', 'grid', 'laps', 'season', 'round', 'driverId', 'raceId', 'circuitId', 'age_on_race']].apply(pd.to_numeric)\n\n\nGetting the Cumulative sum of points and laps completed by each driver for every race:\n\n\nCode\nmain_df['cumulative_points'] = main_df.groupby(['driverId'])['points'].cumsum()\nmain_df['cumulative_laps'] = main_df.groupby(['driverId'])['laps'].cumsum()\n\n\n\n\nCode\nmain_df\n\n\nGetting the number of poles and wins achieved till a specific race for every driver in every race:\n\n\nCode\ngrid_df = main_df[['season', 'round', 'driverId']][main_df['grid'] == 1]\n\n\n\n\nCode\nmain_df = pd.merge(main_df, grid_df, on = ['season', 'round'], how = 'left')\nmain_df.rename(columns={'driverId_x' : 'driverId', 'driverId_y' : 'pole_driverId'}, inplace=True)\n\n\n\n\nCode\nhistory_dict = {i : 0 for i in list(main_df['driverId'].unique())} \n\n\n\n\nCode\nmain_df['pole_history'] = 0\n    \nfor j in range(len(main_df)):\n        \n    if (main_df['driverId'][j] == main_df['pole_driverId'][j]):\n            \n        history_dict[main_df['driverId'][j]] += 1\n        main_df['pole_history'][j] = history_dict[main_df['driverId'][j]]\n        \n    else:\n        main_df['pole_history'][j] = history_dict[main_df['driverId'][j]]\n\n\n\n\nCode\nwin_df = main_df[['season_round', 'driverId']][main_df['position'] == 1]\n\n\n\n\nCode\nmain_df = pd.merge(main_df, win_df, on = ['season_round'], how = 'left')\n\n\n\n\nCode\nmain_df.rename(columns={'driverId_x' : 'driverId', 'driverId_y' : 'win_driverId'}, inplace=True)\n\n\n\n\nCode\nmain_df\n\n\n\n\nCode\nhistory_dict = {i : 0 for i in list(main_df['driverId'].unique())} \n\nmain_df['win_history'] = 0\n    \n    \nfor j in range(len(main_df)):\n        \n    if (main_df['driverId'][j] == main_df['win_driverId'][j]):\n            \n        history_dict[main_df['driverId'][j]] += 1\n        main_df['win_history'][j] = history_dict[main_df['driverId'][j]]\n        \n    else:\n        main_df['win_history'][j] = history_dict[main_df['driverId'][j]]\n\n\n\n\nCode\nmain_df.drop(['circuitRef', 'url', 'date'], axis = 1, inplace=True)\n\n\n\n\nCode\nmain_df.columns\n\n\n\n\nCode\nmain_df = main_df[['season', 'round', 'season_round', 'driverId', 'raceId',\n                    'circuitId', 'position', 'points', 'grid', 'laps', 'status',\n                    'constructorRef', 'weather', 'stop', 'age_on_race', 'cumulative_points',\n                    'cumulative_laps', 'pole_driverId', 'pole_history', 'win_driverId',\n                    'win_history']]\n\n\n\n\nCode\nmain_df\n\n\nCreating a label column for Supervised Machine learning with 3 label variables - Podium (Top 3 finish), Top_10 (4-10 finish) and Outisde_Top_10 (10+ finish):\n\n\nCode\nmain_df['label'] = ''\n\nfor i in range(len(main_df)):\n    \n    if main_df['position'][i] in [1, 2, 3]:\n        main_df['label'][i] = 'Podium'\n        \n    elif main_df['position'][i] in [4, 5, 6, 7, 8, 9, 10]:\n        main_df['label'][i] = 'Top_10'\n    \n    else:\n        main_df['label'][i] = 'Outside_Top_10'\n\n\n\n\nCode\nmain_df\n\n\nExporting the dataframe to a csv file:\n\n\nCode\nmain_df.to_csv('../../data/01-modified-data/data_cleaned.csv', index=False)"
  },
  {
    "objectID": "pages/codes/DT.html",
    "href": "pages/codes/DT.html",
    "title": "Decision Tree Classifier for Twitter Data",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as  pd\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split\n\nimport nltk\nfrom PIL import Image\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.tree import plot_tree\nfrom sklearn.utils import resample\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\n\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\n\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\nimport re"
  },
  {
    "objectID": "pages/codes/DT.html#data-preparation",
    "href": "pages/codes/DT.html#data-preparation",
    "title": "Decision Tree Classifier for Twitter Data",
    "section": "Data Preparation",
    "text": "Data Preparation\n\nCount Vectorizer: It is used to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text. This is helpful when we have multiple such texts, and we wish to convert each word in each text into vectors (for using in further text analysis). CountVectorizer creates a matrix in which each unique word is represented by a column of the matrix, and each text sample from the document is a row in the matrix. The value of each cell is nothing but the count of the word in that particular text sample.\nThe data is first split into train and test with a split ratio of 80(train) - 20(test). The tweets are then transformed to number vectors using sklearn CountVectorizer function.\n\n\n\nCode\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n\n\n\n\nCode\nvectorizer = CountVectorizer()\nX_train1 = vectorizer.fit_transform(X_train.astype('U')) \nX_test1 = vectorizer.transform(X_test.astype('U'))"
  },
  {
    "objectID": "pages/codes/DT.html#decision-trees-with-random-hyperparameters",
    "href": "pages/codes/DT.html#decision-trees-with-random-hyperparameters",
    "title": "Decision Tree Classifier for Twitter Data",
    "section": "Decision Trees with random Hyperparameters",
    "text": "Decision Trees with random Hyperparameters\n\nDecision Tree 1\nHyperparameters:  - Criterion: Entropy  - Splitter: Best  - Maximum depth of the decision tree: 4\n\n\nCode\ndt1 = DecisionTreeClassifier(random_state=1973,criterion = \"entropy\", splitter = \"best\", max_depth = 4)\ndt1.fit(X_train1, y_train)\ny_pred = dt1.predict(X_test1)\nClassification_report_1 = classification_report(y_test, y_pred)\nconf_matrix_1 = confusion_matrix(y_test, y_pred)\n\n\nDecision Tree 1 Plot after fitting:\n\n\nCode\nplt.figure(figsize = (30,30))\ndec_tree_1 = plot_tree(decision_tree=dt1, class_names=label_list, filled=True, rounded=True, fontsize=10, max_depth=4)\n\n\n\n\n\nClassification Report for Decision Tree 1:\n\n\nCode\n#Confusion matrix seaborn\nsns.heatmap(conf_matrix_1,annot=True)\nplt.xlabel('Original')\nplt.ylabel('Predicted')\nplt.title('Heatmap of Confusion Matrix 1: Entropy, Best')\n#plt.savefig('Confusion_Matrix1.png')\n\n#Accuracy\nprint('Accuracy',metrics.accuracy_score(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\n\nAccuracy 0.5381679389312977\n              precision    recall  f1-score   support\n\n           0       0.50      0.02      0.05        41\n           1       0.16      0.12      0.14        24\n           2       0.61      0.87      0.72        77\n           3       0.00      0.00      0.00        55\n           4       0.87      0.83      0.85        90\n           5       0.00      0.00      0.00        91\n           6       0.87      0.85      0.86       113\n           7       0.90      0.77      0.83        99\n           8       0.60      0.11      0.19        81\n           9       0.27      0.83      0.40       115\n\n    accuracy                           0.54       786\n   macro avg       0.48      0.44      0.40       786\nweighted avg       0.53      0.54      0.48       786\n\n\n\n\n\n\nInference from Decision Tree 1:  - There are lot of mis-classifications for 6 out of 10 Teams (classes). - Teams such as Alpine, Ferrari, Mclaren and Mercedes are getting classified with a high F1 score. - A majority of tweets are classified into label 9 (Williams). One of the reasons can be since the samples of Williams are higher than other classes.\n\n\nDecision Tree 2\nHyperparameters: - Criterion: Gini Impurity - Splitter: Random - Maximum depth of the decision tree: 5\n\n\nCode\ndt2 = DecisionTreeClassifier(random_state=10,criterion = \"gini\", splitter = \"random\",max_depth = 5)\ndt2.fit(X_train1, y_train)\ny_pred = dt2.predict(X_test1)\nClassification_report_2 = classification_report(y_test, y_pred)\nconf_matrix_2 = confusion_matrix(y_test, y_pred)\n\n\nDecision Tree 2 Plot after fitting:\n\n\nCode\nplt.figure(figsize = (30,30))\ndec_tree_2 = plot_tree(decision_tree=dt2, class_names=label_list, filled=True, rounded=True, fontsize=10, max_depth=5)\n\n\n\n\n\nClassification Report for Decision Tree 2:\n\n\nCode\n#Confusion matrix seaborn\nsns.heatmap(conf_matrix_2,annot=True)\nplt.xlabel('Original')\nplt.ylabel('Predicted')\nplt.title('Heatmap of Confusion Matrix 2: Gini, Random')\n#plt.savefig('Confusion_Matrix2.png')\n\n#Accuracy\nprint('Accuracy',metrics.accuracy_score(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\n\nAccuracy 0.5928753180661578\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        41\n           1       0.00      0.00      0.00        24\n           2       0.59      0.84      0.70        77\n           3       0.00      0.00      0.00        55\n           4       0.28      0.89      0.42        90\n           5       0.20      0.05      0.09        91\n           6       0.88      0.82      0.85       113\n           7       0.89      0.75      0.81        99\n           8       0.87      0.89      0.88        81\n           9       0.88      0.67      0.76       115\n\n    accuracy                           0.59       786\n   macro avg       0.46      0.49      0.45       786\nweighted avg       0.57      0.59      0.55       786\n\n\n\n\n\n\nInference from Decision Tree 2:  - There are lot of mis-classifications for 5 out of 10 Teams (classes). - Teams such as Alpine, Mclaren, Mercedes, Redbull and Williams are getting classified with a high F1 score. - A majority of tweets are classified into label 4 (Ferrari). - As I increased our maximum depth by 1, we can see a significant change in the classification for Williams label as well as for some other classes.\n\n\nDecision Tree 3\nHyperparameters: - Criterion: Entropy - Splitter: Random - Maximum depth of the decision tree: 4\n\n\nCode\ndt3 = DecisionTreeClassifier(random_state=0,criterion = \"entropy\", splitter = \"random\",max_depth = 4)\ndt3.fit(X_train1, y_train)\ny_pred = dt3.predict(X_test1)\nClassification_report_3 = classification_report(y_test, y_pred)\nconf_matrix_3 = confusion_matrix(y_test, y_pred)\n\n\nDecision Tree 3 Plot after fitting:\n\n\nCode\nplt.figure(figsize = (60,40))\ndec_tree_3 = plot_tree(decision_tree=dt3, class_names=label_list, filled=True, rounded=True, fontsize=10, max_depth=4)\n\n\n\n\n\nClassification Report for Decision Tree 3:\n\n\nCode\n#Confusion matrix seaborn\nsns.heatmap(conf_matrix_3,annot=True)\nplt.xlabel('Original')\nplt.ylabel('Predicted')\nplt.title('Heatmap of Confusion Matrix 3: Gini, Random')\n#plt.savefig('Confusion_Matrix3.png')\n\n#Accuracy\nprint('Accuracy',metrics.accuracy_score(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\n\nAccuracy 0.5190839694656488\n              precision    recall  f1-score   support\n\n           0       0.00      0.00      0.00        41\n           1       0.16      0.12      0.14        24\n           2       0.61      0.87      0.72        77\n           3       0.00      0.00      0.00        55\n           4       0.00      0.00      0.00        90\n           5       0.88      0.69      0.77        91\n           6       0.87      0.85      0.86       113\n           7       0.90      0.77      0.83        99\n           8       0.60      0.11      0.19        81\n           9       0.25      0.82      0.39       115\n\n    accuracy                           0.52       786\n   macro avg       0.43      0.42      0.39       786\nweighted avg       0.50      0.52      0.47       786\n\n\n\n\n\n\nInference from Decision Tree 3: - There are lot of mis-classifications for 6 out of 10 Teams (classes). - Teams such as Alpine, Haas, Mclaren and Mercedes are getting classified with a high F1 score. - A majority of tweets are classified into label 9 (Williams). - Comparing with Decision tree 1, after changing our splitter to random from best our accuracy has decreased so the splitter that was chosen earlier in the first tree is more suitable."
  },
  {
    "objectID": "pages/codes/DT.html#hyperparameter-tuning",
    "href": "pages/codes/DT.html#hyperparameter-tuning",
    "title": "Decision Tree Classifier for Twitter Data",
    "section": "Hyperparameter Tuning",
    "text": "Hyperparameter Tuning\n\nSince the accuracy scores of the Decision tree models with random hyperparameters were not up to the mark we have to tune our hyperparameters accordingly.\nFrom above random models, it is concluded that even if Entropy performs better feature selection, the difference is less when we compare to Gini Impurity. This is not the case for computational time, which is higher when we take Entropy as our criterion.\nIt is also observed that increasing the depth of the decision tree leads to better results but it also increases the complexity of the model and thus may lead to overfitting. Hence we need to find an optimal depth for our decision tree model.\nThis can be achieved by iterating with the help of loops over depth ranging from 0-20 and finding the accuracy and recall for all the depths.\n\n\n\nCode\ntest_results=[]\ntrain_results=[]\n\nfor num_layer in range(1,20):\n    model = DecisionTreeClassifier(max_depth=num_layer)\n    model = model.fit(X_train1, y_train)\n\n    yp_train=model.predict(X_train1)\n    yp_test=model.predict(X_test1)\n\n    # print(y_pred.shape)\n    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test,pos_label=0, average='micro'),recall_score(y_test, yp_test,pos_label=1, average='micro'),\n                         recall_score(y_test, yp_test,pos_label=2, average='micro'),recall_score(y_test, yp_test,pos_label=3, average='micro'),\n                         recall_score(y_test, yp_test,pos_label=4, average='micro'),recall_score(y_test, yp_test,pos_label=5, average='micro'),\n                         recall_score(y_test, yp_test,pos_label=6, average='micro'),recall_score(y_test, yp_test,pos_label=7, average='micro'),\n                         recall_score(y_test, yp_test,pos_label=8, average='micro'),recall_score(y_test, yp_test,pos_label=9, average='micro')])\n    train_results.append([num_layer,accuracy_score(y_train, yp_train),recall_score(y_train, yp_train,pos_label=0, average='micro'),recall_score(y_train, yp_train,pos_label=1, average='micro'),\n                          recall_score(y_train, yp_train,pos_label=2, average='micro'),recall_score(y_train, yp_train,pos_label=3, average='micro'),\n                          recall_score(y_train, yp_train,pos_label=4, average='micro'),recall_score(y_train, yp_train,pos_label=5, average='micro'),\n                          recall_score(y_train, yp_train,pos_label=6, average='micro'),recall_score(y_train, yp_train,pos_label=7, average='micro'),\n                          recall_score(y_train, yp_train,pos_label=8, average='micro'),recall_score(y_train, yp_train,pos_label=9, average='micro')])\n\n\n\n\nCode\nplt.plot([x[0] for x in test_results],[x[1] for x in test_results],label='test', color='red', marker='o')\nplt.plot([x[0] for x in train_results],[x[1] for x in train_results],label='train', color='blue', marker='o')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('ACCURACY : Training (blue) and Test (red)')\nplt.show()\n\nplt.plot([x[0] for x in test_results],[x[2] for x in test_results],label='test', color='red', marker='o')\nplt.plot([x[0] for x in train_results],[x[2] for x in train_results],label='train', color='blue', marker='o')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('RECALL (Y=0): Training (blue) and Test (red)')\nplt.show()\n\nplt.plot([x[0] for x in test_results],[x[3] for x in test_results],label='test', color='red', marker='o')\nplt.plot([x[0] for x in train_results],[x[3] for x in train_results],label='train', color='blue', marker='o')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('RECALL (Y=1): Training (blue) and Test (red)')\nplt.show()\n\nplt.plot([x[0] for x in test_results],[x[4] for x in test_results],label='test', color='red', marker='o')\nplt.plot([x[0] for x in train_results],[x[4] for x in train_results],label='train', color='blue', marker='o')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('RECALL (Y=2): Training (blue) and Test (red)')\nplt.show()\n\nplt.plot([x[0] for x in test_results],[x[5] for x in test_results],label='test', color='red', marker='o')\nplt.plot([x[0] for x in train_results],[x[5] for x in train_results],label='train', color='blue', marker='o')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('RECALL (Y=3): Training (blue) and Test (red)')\nplt.show()\n\nplt.plot([x[0] for x in test_results],[x[6] for x in test_results],label='test', color='red', marker='o')\nplt.plot([x[0] for x in train_results],[x[6] for x in train_results],label='train', color='blue', marker='o')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('RECALL (Y=4): Training (blue) and Test (red)')\nplt.show()\n\nplt.plot([x[0] for x in test_results],[x[7] for x in test_results],label='test', color='red', marker='o')\nplt.plot([x[0] for x in train_results],[x[7] for x in train_results],label='train', color='blue', marker='o')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('RECALL (Y=5): Training (blue) and Test (red)')\nplt.show()\n\nplt.plot([x[0] for x in test_results],[x[8] for x in test_results],label='test', color='red', marker='o')\nplt.plot([x[0] for x in train_results],[x[8] for x in train_results],label='train', color='blue', marker='o')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('RECALL (Y=6): Training (blue) and Test (red)')\nplt.show()\n\nplt.plot([x[0] for x in test_results],[x[9] for x in test_results],label='test', color='red', marker='o')\nplt.plot([x[0] for x in train_results],[x[9] for x in train_results],label='train', color='blue', marker='o')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('RECALL (Y=7): Training (blue) and Test (red)')\nplt.show()\n\nplt.plot([x[0] for x in test_results],[x[10] for x in test_results],label='test', color='red', marker='o')\nplt.plot([x[0] for x in train_results],[x[10] for x in train_results],label='train', color='blue', marker='o')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('RECALL (Y=8): Training (blue) and Test (red)')\nplt.show()\n\nplt.plot([x[0] for x in test_results],[x[11] for x in test_results],label='test', color='red', marker='o')\nplt.plot([x[0] for x in train_results],[x[11] for x in train_results],label='train', color='blue', marker='o')\nplt.xlabel('Number of layers in decision tree (max_depth)')\nplt.ylabel('RECALL (Y=9): Training (blue) and Test (red)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInference for Hyperparameter tuning - As the depth increases, the accuracy and recall scores are increasing. - The accuracy and recall scores of test data reach their maximum at max_depth value = 10. - The accuracy and recall scores of train data keep on increasing as the depth increases. - Hence I choose my optimal max_depth value as 10."
  },
  {
    "objectID": "pages/codes/DT.html#optimal-decision-tree",
    "href": "pages/codes/DT.html#optimal-decision-tree",
    "title": "Decision Tree Classifier for Twitter Data",
    "section": "Optimal Decision Tree",
    "text": "Optimal Decision Tree\n\n\nCode\noptimal_dt = DecisionTreeClassifier(random_state=1973, max_depth = 10, criterion = 'gini')\noptimal_dt.fit(X_train1, y_train)\ny_pred = optimal_dt.predict(X_test1)\nClassification_report = classification_report(y_test, y_pred)\nconf_matrix = confusion_matrix(y_test, y_pred)\n\n\nOptimal Decision Tree Plot after fitting:\n\n\nCode\nplt.figure(figsize = (40,40))\noptimal_dec_tree = plot_tree(decision_tree=optimal_dt, class_names=label_list, filled=True, rounded=True, fontsize=10, max_depth=10)\n#plt.savefig('dec_tree_4.png')\n\n\n\n\n\nClassification report for Optimal Decision Tree:\n\n\nCode\nsns.heatmap(conf_matrix,annot=True)\nplt.xlabel('Original')\nplt.ylabel('Predicted')\nplt.title('Heatmap of Optimal Decision Tree')\n#plt.savefig('Confusion_Matrix3.png')\n\n#Accuracy\nprint('Accuracy',metrics.accuracy_score(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\n\n\nAccuracy 0.7417302798982188\n              precision    recall  f1-score   support\n\n           0       0.70      0.73      0.71        41\n           1       0.48      0.58      0.53        24\n           2       0.62      0.71      0.66        77\n           3       0.98      0.75      0.85        55\n           4       0.78      0.79      0.78        90\n           5       0.79      0.67      0.73        91\n           6       0.87      0.75      0.81       113\n           7       0.83      0.69      0.75        99\n           8       0.85      0.84      0.84        81\n           9       0.58      0.78      0.67       115\n\n    accuracy                           0.74       786\n   macro avg       0.75      0.73      0.73       786\nweighted avg       0.76      0.74      0.75       786"
  },
  {
    "objectID": "pages/codes/DT.html#conclusion",
    "href": "pages/codes/DT.html#conclusion",
    "title": "Decision Tree Classifier for Twitter Data",
    "section": "Conclusion",
    "text": "Conclusion\nThe accuracy and recall scores have increased to around 74% by changing our max_depth to 10 and criterion as gini with best splitter."
  },
  {
    "objectID": "pages/codes/twitter_datacleaning.html",
    "href": "pages/codes/twitter_datacleaning.html",
    "title": "Twitter Data Cleaning",
    "section": "",
    "text": "In the era of big data, cleaning or scrubbing your data has become an essential part of the data management process. Even though data cleaning can be tedious at times, it is absolutely crucial for getting accurate business intelligence (BI) that can drive your strategic decisions.\nIncorrect or inconsistent data leads to false conclusions. And so, how well you clean and understand the data has a high impact on the quality of the results.\nData cleaning involve different techniques based on the problem and the data type. Different methods can be applied with each has its own trade-offs. Overall, incorrect data is either removed, corrected, or imputed.\nData cleaning is the process of removing incorrect, duplicate, or otherwise erroneous data from a dataset. These errors can include incorrectly formatted data, redundant entries, mislabeled data, and other issues; they often arise when two or more datasets are combined together. Data cleaning improves the quality of your data as well as any business decisions that you draw based on the data.\nThere is no one right way to clean a dataset, as every set is different and presents its own unique slate of errors that need to be corrected. Many data cleaning techniques can now be automated with the help of dedicated software, but some portion of the work must be done manually to ensure the greatest accuracy. Usually this work is done by data quality analysts, BI analysts, and business users.\nEvery organization’s data cleaning methods will vary according to their individual needs as well as the particular constraints of the dataset. However, most data cleaning steps follow a standard framework:\n\nDetermine the critical data values you need for your analysis.\nCollect the data you need, then sort and organize it.\nIdentify duplicate or irrelevant values and remove them.\nSearch for missing values and fill them in, so you have a complete dataset.\nFix any remaining structural or repetitive errors in the dataset.\nIdentify outliers and remove them, so they will not interfere with your analysis.\nValidate your dataset to ensure it is ready for data transformation and analysis.\nOnce the set has been validated, perform your transformation and analysis.\n\nData Cleaning vs. Data Cleansing vs. Data Scrubbing\n\nYou might sometimes hear the terms data cleansing or data scrubbing used instead of data cleaning. In most situations, these terms are all being used interchangeably and refer to the exact same thing. Data scrubbing may sometimes be used to refer to a specific aspect of data cleaning—namely, removing duplicate or bad data from datasets.\nYou should also know that data scrubbing can have a slightly different meaning within the specific context of data storage; in this case, it refers to an automated function that evaluates storage systems and disk drives to identify any bad sectors or blocks and to confirm the data in them can be read.\nNote that all three of these terms—data cleaning, data cleansing, and data scrubbing—are different from data transformation, which is the act of taking clean data and converting it into a new format or structure. Data transformation is a separate process that comes after data cleaning.\n\nBenefits of Data Cleaning:\n\nNot having clean data exacts a high price: IBM estimates that bad data costs the U.S. over $3 trillion each year. That’s because data-driven decisions are only as good as the data you are relying on. Bad quality data leads to equally bad quality decisions. If the data you are basing your strategy on is inaccurate, then your strategy will have the same issues present in the data, even if it seems sound. In fact, sometimes no data at all is better than bad data.\nCleaning your data results in many benefits for your organization in both the short- and long-term. It leads to better decision making, which can boost your efficiency and your customer satisfaction, in turn giving your business a competitive edge. Over time, it also reduces your costs of data management by preemptively removing errors and other mistakes that would necessitate performing analysis over and over again."
  },
  {
    "objectID": "pages/codes/twitter_datacleaning.html#natural-language-processing-terms",
    "href": "pages/codes/twitter_datacleaning.html#natural-language-processing-terms",
    "title": "Twitter Data Cleaning",
    "section": "Natural Language Processing terms:",
    "text": "Natural Language Processing terms:\n\nStopwords:\n\nThe words which are generally filtered out before processing a natural language are called stop words. These are actually the most common words in any language (like articles, prepositions, pronouns, conjunctions, etc) and does not add much information to the text. Examples of a few stop words in English are “the”, “a”, “an”, “so”, “what”.\nStop words are available in abundance in any human language. By removing these words, we remove the low-level information from our text in order to give more focus to the important information. In order words, we can say that the removal of such words does not show any negative consequences on the model we train for our task. Removal of stop words definitely reduces the dataset size and thus reduces the training time due to the fewer number of tokens involved in the training.\n\nStemming:\n\nStemming is the process of reducing a word to its stem that affixes to suffixes and prefixes or to the roots of words known as “lemmas”. Stemming is important in natural language understanding (NLU) and natural language processing (NLP).\nStemming is a part of linguistic studies in morphology as well as artificial intelligence (AI) information retrieval and extraction. Stemming and AI knowledge extract meaningful information from vast sources like big data or the internet since additional forms of a word related to a subject may need to be searched to get the best results. Stemming is also a part of queries and internet search engines.\nRecognizing, searching and retrieving more forms of words returns more results. When a form of a word is recognized, it’s possible to return search results that otherwise might have been missed. That additional information retrieved is why stemming is integral to search queries and information retrieval.\n\nTokenization:\n\nTokenization is a way of separating a piece of text into smaller units called tokens. Here, tokens can be either words, characters, or subwords. Hence, tokenization can be broadly classified into 3 types\n\nword\ncharacter\nsubword (n-gram characters)\n\nAs tokens are the building blocks of Natural Language, the most common way of processing the raw text happens at the token level.\nTokenization is performed on the corpus to obtain tokens. The following tokens are then used to prepare a vocabulary. Vocabulary refers to the set of unique tokens in the corpus. Remember that vocabulary can be constructed by considering each unique token in the corpus or by considering the top K Frequently Occurring Words."
  },
  {
    "objectID": "pages/codes/twitter_datacleaning.html#helper-functions",
    "href": "pages/codes/twitter_datacleaning.html#helper-functions",
    "title": "Twitter Data Cleaning",
    "section": "Helper functions:",
    "text": "Helper functions:\n\npercentage: computes the percentage of any number.\nremove_punct: Removes punctuations and numbers from the text.\ntokenization: Tokenizes the text data.\nremove_stopwords: Removes stopwords from the data\nstemming: Performs stemming on the text.\n\n\n\nCode\ndef percentage(part,whole):\n    \n    return 100 * float(part)/float(whole)\n\n\n\n\nCode\ndef remove_punct(text):\n    \n    text  = \"\".join([char for char in text if char not in string.punctuation])\n    text = re.sub('[0-9]+', '', text) #removes numbers from text\n    return text\n\n\n\n\nCode\ndef tokenization(text):\n    \n    text = re.split('\\W+', text)\n    return text\n\nstopword = nltk.corpus.stopwords.words('english')\ndef remove_stopwords(text):\n    \n    text = [word for word in text if word not in stopword]\n    return text\n\nps = nltk.PorterStemmer()\ndef stemming(text):\n    \n    text = [ps.stem(word) for word in text]\n    return text\n\ndef clean_text(text):\n    \n    text_lc = \"\".join([word.lower() for word in text if word not in string.punctuation]) # remove puntuation\n    text_rc = re.sub('[0-9]+', '', text_lc)\n    tokens = re.split('\\W+', text_rc)    # tokenization\n    text = [ps.stem(word) for word in tokens if word not in stopword]  # remove stopwords and stemming\n    \n    return text"
  },
  {
    "objectID": "pages/codes/twitter_datacleaning.html#pipeline-function",
    "href": "pages/codes/twitter_datacleaning.html#pipeline-function",
    "title": "Twitter Data Cleaning",
    "section": "Pipeline function:",
    "text": "Pipeline function:\n\nSentiment Analysis: Sentiment analysis (or opinion mining) is a natural language processing (NLP) technique used to determine whether data is positive, negative or neutral. Sentiment analysis is often performed on textual data to help businesses monitor brand and product sentiment in customer feedback, and understand customer needs.\nSince humans express their thoughts and feelings more openly than ever before, sentiment analysis is fast becoming an essential tool to monitor and understand sentiment in all types of data.\nThe pipeline function computes the sentiments of all our tweets for different teams using polarity scores before and after applying the above helper functions. It also creates 3 separate dataframes for Positive, Neutral and Negative Sentiments for further analysis\n\n\n\nCode\ndef pipeline(df):\n    \n    positive = 0\n    negative = 0\n    neutral = 0\n    polarity = 0\n    tweet_list = []\n    neutral_list = []\n    negative_list = []\n    positive_list = []\n\n    for tweet in df['text']:\n        \n        tweet_list.append(tweet)\n        analysis = TextBlob(tweet)\n        score = SentimentIntensityAnalyzer().polarity_scores(tweet)\n        neg = score['neg']\n        neu = score['neu']\n        pos = score['pos']\n        comp = score['compound']\n        polarity += analysis.sentiment.polarity\n        \n        if neg > pos:\n            \n            negative_list.append(tweet)\n            negative += 1\n            \n        elif pos > neg:\n            \n            positive_list.append(tweet)\n            positive += 1\n            \n        elif pos == neg:\n            \n            neutral_list.append(tweet)\n            neutral = neutral + 1\n            \n    \n    tweet_list = pd.DataFrame(tweet_list)\n    neutral_list = pd.DataFrame(neutral_list)\n    negative_list = pd.DataFrame(negative_list)\n    positive_list = pd.DataFrame(positive_list)\n\n    positive_percentage = percentage(positive,len(tweet_list))\n    negative_percentage = percentage(negative,len(tweet_list))\n    neutral_percentage = percentage(neutral,len(tweet_list))\n    \n    tweet_list.drop_duplicates(inplace = True)\n    \n    tw_list = pd.DataFrame(tweet_list)\n    tw_list['text'] = tw_list[0]\n    #Removing RT, Punctuation etc\n    remove_rt = lambda x: re.sub('@\\w+: ', \"\", x)\n    tw_list[\"text\"] = tw_list.text.map(remove_rt)\n    tw_list[\"text\"] = tw_list.text.map(remove_punct)\n    tw_list[\"text\"] = tw_list.text.str.lower()\n    \n    tw_list[['polarity', 'subjectivity']] = tw_list['text'].apply(lambda Text: pd.Series(TextBlob(Text).sentiment))\n\n    for index, row in tw_list['text'].iteritems():\n        score = SentimentIntensityAnalyzer().polarity_scores(row)\n        neg = score['neg']\n        neu = score['neu']\n        pos = score['pos']\n        comp = score['compound']\n        if neg > pos:\n            tw_list.loc[index, 'sentiment'] = \"negative\"\n        elif pos > neg:\n            tw_list.loc[index, \"sentiment\"] = \"positive\"\n        else:\n            tw_list.loc[index, \"sentiment\"] = \"neutral\"\n    \n    tw_list_negative = tw_list[tw_list[\"sentiment\"]==\"negative\"]\n    tw_list_positive = tw_list[tw_list[\"sentiment\"]==\"positive\"]\n    tw_list_neutral = tw_list[tw_list[\"sentiment\"]==\"neutral\"]\n    \n    tw_list[\"text\"] = tw_list.text.replace(\" f \", \" f1 \")        \n    tw_list['tokenized'] = tw_list['text'].apply(lambda x: tokenization(x.lower()))\n    tw_list['nonstop'] = tw_list['tokenized'].apply(lambda x: remove_stopwords(x))\n    tw_list['stemmed'] = tw_list['nonstop'].apply(lambda x: stemming(x))\n    \n    countVectorizer = CountVectorizer(analyzer=clean_text) \n    countVector = countVectorizer.fit_transform(tw_list['text'])\n    \n    count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names_out())\n    \n    count = pd.DataFrame(count_vect_df.sum())\n    top_20_count = count.sort_values(0,ascending=False).head(20)\n    top_20_count.columns = ['count']\n    \n    return tw_list, top_20_count\n\n\nApplying the pipeline function to tweets from different teams:\n\n\nCode\nferrari_sentiment_df, ferrari_top_20_count = pipeline(ferrari_df)\nmercedes_sentiment_df, mercedes_top_20_count = pipeline(mercedes_df)\nredbull_sentiment_df, redbull_top_20_count = pipeline(redbull_df)\nhaas_sentiment_df, haas_top_20_count = pipeline(haas_df)\nmclaren_sentiment_df, mclaren_top_20_count = pipeline(mclaren_df)\nalpine_sentiment_df, alpine_top_20_count = pipeline(alpine_df)\nwilliams_sentiment_df, williams_top_20_count = pipeline(williams_df)\nastonmartin_sentiment_df, astonmartin_top_20_count = pipeline(aston_martin_df)\nalphatauri_sentiment_df, alphatauri_top_20_count = pipeline(alpha_tauri_df)\nalfa_romeo_sentiment_df, alfa_romeo_top_20_count = pipeline(alfa_romeo_df)\n\n\n\n\nCode\nferrari_sentiment_df.to_csv('../../data/01-modified-data/sentiment_analysis/ferrari_sentiment_df.csv')\nmercedes_sentiment_df.to_csv('../../data/01-modified-data/sentiment_analysis/mercedes_sentiment_df.csv')\nredbull_sentiment_df.to_csv('../../data/01-modified-data/sentiment_analysis/redbull_sentiment_df.csv')\nhaas_sentiment_df.to_csv('../../data/01-modified-data/sentiment_analysis/haas_sentiment_df.csv')\nmclaren_sentiment_df.to_csv('../../data/01-modified-data/sentiment_analysis/mclaren_sentiment_df.csv')\nalpine_sentiment_df.to_csv('../../data/01-modified-data/sentiment_analysis/alpine_sentiment_df.csv')\nwilliams_sentiment_df.to_csv('../../data/01-modified-data/sentiment_analysis/williams_sentiment_df.csv')\nastonmartin_sentiment_df.to_csv('../../data/01-modified-data/sentiment_analysis/astonmartin_sentiment_df.csv')\nalphatauri_sentiment_df.to_csv('../../data/01-modified-data/sentiment_analysis/alphatauri_sentiment_df.csv')\nalfa_romeo_sentiment_df.to_csv('../../data/01-modified-data/sentiment_analysis/alfa_romeo_sentiment_df.csv')\n\n\n\n\nCode\nall_teams_top20_count = pd.concat([ferrari_top_20_count, mercedes_top_20_count, redbull_top_20_count, haas_top_20_count, mclaren_top_20_count, alpine_top_20_count, williams_top_20_count, astonmartin_top_20_count, alphatauri_top_20_count, alfa_romeo_top_20_count], axis=1)\nall_teams_top20_count.reset_index(inplace=True)\nall_teams_top20_count.columns = ['words', 'ferrari', 'mercedes', 'redbull', 'haas', 'mclaren', 'alpine', 'williams', 'aston martin', 'alpha tauri', 'alfa romeo']\nall_teams_top20_count.to_csv('../../data/01-modified-data/all_teams_top20_wordcount.csv')\n\n\n\n\nCode\nall_teams_top20_count\n\n\nSaving all the different tweets of each team into a single dataframe:\n\n\nCode\nkeys = ['Ferrari', 'Mercedes', 'Redbull', 'Haas', 'Mclaren', 'Alpine', 'Williams', 'Aston Martin', 'Alpha Tauri', 'Alfa Romeo']\nall_teams_sentiment_df = pd.concat([ferrari_sentiment_df, mercedes_sentiment_df, redbull_sentiment_df, haas_sentiment_df, mclaren_sentiment_df, alpine_sentiment_df, williams_sentiment_df, astonmartin_sentiment_df, alphatauri_sentiment_df, alfa_romeo_sentiment_df], keys=keys, names=['Team', None], axis=0).reset_index(level = 'Team')\n\n\n\n\nCode\nall_teams_sentiment_df.to_csv('../../data/01-modified-data/all_teams_sentiment_df.csv')\n\n\n\nWordclouds:\n\nWord clouds (also known as text clouds or tag clouds) work in a simple way: the more a specific word appears in a source of textual data (such as a speech, blog post, or database), the bigger and bolder it appears in the word cloud.\nAlso known as tag clouds or text clouds, these are ideal ways to pull out the most pertinent parts of textual data, from blog posts to databases. They can also help business users compare and contrast two different pieces of text to find the wording similarities between the two.\n\nWorldcloud function:\n\n\nCode\ndef create_wordcloud(text):\n    #mask = np.array(Image.open(\"cloud.png\"))\n    stopwords = set(STOPWORDS)\n    wc = WordCloud(background_color=\"white\",\n                    max_words=3000,\n                    stopwords=stopwords,\n                    repeat=True)\n    wc.generate(str(text))\n    wc.to_file(\"wc.png\")\n    print(\"Word Cloud Saved Successfully\")\n    path=\"wc.png\"\n    display(Image.open(path))\n\n\nWordclouds for different sentiments of Ferrari:\n\n\nCode\ncreate_wordcloud(tw_list[\"text\"].values)\n\n\n\n\nCode\ncreate_wordcloud(tw_list_positive[\"text\"].values)\n\n\n\n\nCode\ncreate_wordcloud(tw_list_negative[\"text\"].values)\n\n\n\n\nCode\ntw_list[\"text_len\"] = tw_list[\"text\"].astype(str).apply(len)\ntw_list[\"text_word_count\"] = tw_list[\"text\"].apply(lambda x: len(str(x).split()))\nround(pd.DataFrame(tw_list.groupby(\"sentiment\").text_len.mean()),2)\n\n\n\n\nCode\nround(pd.DataFrame(tw_list.groupby(\"sentiment\").text_word_count.mean()),2)"
  }
]