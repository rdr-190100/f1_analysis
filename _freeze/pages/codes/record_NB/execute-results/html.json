{
  "hash": "4fdfee475092b15155650234f5c83e0a",
  "result": {
    "markdown": "---\ntitle: <b>Naive Bayes' for Record Data</b>\nformat:\n  html:\n    theme: lumen\n    toc: true\n    self-contained: true\n    embed-resources: true\n    page-layout: full\n    code-fold: true\n    code-tools: true\n---\n\n\n\n\n# Import Libraries\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyr)\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ ggplot2 3.4.0      ✔ dplyr   1.0.10\n✔ tibble  3.1.8      ✔ stringr 1.4.1 \n✔ readr   2.1.3      ✔ forcats 0.5.2 \n✔ purrr   0.3.4      \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n```\n:::\n\n```{.r .cell-code}\nlibrary(dplyr)\nlibrary(caret)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: lattice\n\nAttaching package: 'caret'\n\nThe following object is masked from 'package:purrr':\n\n    lift\n```\n:::\n\n```{.r .cell-code}\nlibrary(e1071)\nlibrary(caTools)\nlibrary(yardstick)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nFor binary classification, the first factor level is assumed to be the event.\nUse the argument `event_level = \"second\"` to alter this as needed.\n\nAttaching package: 'yardstick'\n\nThe following objects are masked from 'package:caret':\n\n    precision, recall, sensitivity, specificity\n\nThe following object is masked from 'package:readr':\n\n    spec\n```\n:::\n\n```{.r .cell-code}\nlibrary(naivebayes)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nnaivebayes 0.9.7 loaded\n```\n:::\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(psych)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'psych'\n\nThe following objects are masked from 'package:ggplot2':\n\n    %+%, alpha\n```\n:::\n\n```{.r .cell-code}\nlibrary(sjPlot)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nInstall package \"strengejacke\" from GitHub (`devtools::install_github(\"strengejacke/strengejacke\")`) to load all sj-packages at once!\n```\n:::\n\n```{.r .cell-code}\nlibrary(klaR)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: MASS\n\nAttaching package: 'MASS'\n\nThe following object is masked from 'package:dplyr':\n\n    select\n```\n:::\n:::\n\n\n# Import Data\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf = read_csv(\"../../data/02-model-data/data_cleaned.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 26941 Columns: 22\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (6): season_round, status, constructorRef, weather, stop, label\ndbl (16): season, round, driverId, raceId, circuitId, position, points, grid...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n\n```{.r .cell-code}\nhead(df)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 22\n  season round season…¹ drive…² raceId circu…³ posit…⁴ points  grid  laps status\n   <dbl> <dbl> <chr>      <dbl>  <dbl>   <dbl>   <dbl>  <dbl> <dbl> <dbl> <chr> \n1   1950     1 1950_1       642    833       9       1      9     1    70 Finis…\n2   1950     1 1950_1       786    833       9       2      6     2    70 Finis…\n3   1950     1 1950_1       686    833       9       3      4     4    70 Finis…\n4   1950     1 1950_1       704    833       9       4      3     6    68 Lapped\n5   1950     1 1950_1       627    833       9       5      2     9    68 Lapped\n6   1950     1 1950_1       619    833       9       6      0    13    67 Lapped\n# … with 11 more variables: constructorRef <chr>, weather <chr>, stop <chr>,\n#   age_on_race <dbl>, cumulative_points <dbl>, cumulative_laps <dbl>,\n#   pole_driverId <dbl>, pole_history <dbl>, win_driverId <dbl>,\n#   win_history <dbl>, label <chr>, and abbreviated variable names\n#   ¹​season_round, ²​driverId, ³​circuitId, ⁴​position\n```\n:::\n:::\n\n\n# Data Pre-Processing and Visualization\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbarplot(table(df$label), col = 'lightblue', main = 'Distribution of Labels', xlab = 'Labels', ylab = 'Count')\n```\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(df$cumulative_points, df$win_history, col = 'lightblue', main = 'Points vs Win History', xlab = 'Cumulative Points', ylab = 'Win History')\n```\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\nConverting some numeric variables to factors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\na = factor(df$status, levels = unique(df$status))\ndf$status = as.integer(a)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\na = factor(df$constructorRef, levels = unique(df$constructorRef))\ndf$constructorRef = as.integer(a)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\na = factor(df$weather, levels = unique(df$weather))\ndf$weather = as.integer(a)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\na = factor(df$stop, levels = unique(df$stop))\ndf$stop = as.integer(a)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\na = factor(df$label, levels = unique(df$label))\ndf$label = as.integer(a)\n```\n:::\n\n\nDropping unnecessary columns:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndf = df[-c(1:3)]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndf$label = as.factor(df$label)\ndf\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 26,941 × 19\n   driverId raceId circuitId position points  grid  laps status constr…¹ weather\n      <dbl>  <dbl>     <dbl>    <dbl>  <dbl> <dbl> <dbl>  <int>    <int>   <int>\n 1      642    833         9        1      9     1    70      1        1       1\n 2      786    833         9        2      6     2    70      1        1       1\n 3      686    833         9        3      4     4    70      1        1       1\n 4      704    833         9        4      3     6    68      2        2       1\n 5      627    833         9        5      2     9    68      2        2       1\n 6      619    833         9        6      0    13    67      2        3       1\n 7      787    833         9        7      0    15    67      2        3       1\n 8      741    833         9        8      0    14    65      2        2       1\n 9      784    833         9        9      0    16    64      2        4       1\n10      778    833         9       10      0    20    64      2        4       1\n# … with 26,931 more rows, 9 more variables: stop <int>, age_on_race <dbl>,\n#   cumulative_points <dbl>, cumulative_laps <dbl>, pole_driverId <dbl>,\n#   pole_history <dbl>, win_driverId <dbl>, win_history <dbl>, label <fct>, and\n#   abbreviated variable name ¹​constructorRef\n```\n:::\n:::\n\n\nSplitting data into train and test set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1973)\n\nsample <- sample(c(TRUE, FALSE), nrow(df), replace=TRUE, prob=c(0.8,0.2))\ntrain  <- df[sample, ]\ntest   <- df[!sample, ]\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nnrow(train)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 21547\n```\n:::\n\n```{.r .cell-code}\nnrow(test)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5394\n```\n:::\n:::\n\n\n# Naive Bayes Model\n\n- Bayes' Theorem: In probability theory and statistics, Bayes' theorem (alternatively Bayes' law or Bayes' rule), named after Thomas Bayes, describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if the risk of developing health problems is known to increase with age, Bayes' theorem allows the risk to an individual of a known age to be assessed more accurately (by conditioning it on their age) than simply assuming that the individual is typical of the population as a whole.\n- Naive Bayes Algorithm is a classification technique based on Bayes’ Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n- A fruit might be categorized as an apple, for instance, if it is red, rounded, and around 3 inches in diameter. Even if these characteristics depend on one another or on the presence of other characteristics, each of these traits separately increases the likelihood that this fruit is an apple, which is why it is called \"Naive.\"\n- Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods.\n- Bayes' Theorem can be shown by this equation:\n$$ P(C|X) = \\frac {P(X|C) * P(C)}{P(X)} $$\n- In the above equation:\n    - P(C|X) is the posterior probability of class (C, target) given predictor (X, attributes).\n    - P(C) is the prior probability of class.\n    - P(X|C) is the likelihood which is the probability of predictor given class.\n    - P(X) is the prior probability of predictor.\n- How does Bayes Theorem work?\n    - Let's take an example: A Path Lab is performing a Test of disease say “D” with two results “Positive” & “Negative.” They guarantee that their test result is 99% accurate: if you have the disease, they will give test positive 99% of the time. If you don’t have the disease, they will test negative 99% of the time. If 3% of all the people have this disease and test gives “positive” result, what is the probability that you actually have the disease?\n    - For solving the above problem, we will have to use conditional probability.\n        - Probability of people suffering from Disease D, P(D) = 0.03 = 3%\n        - Probability that test gives “positive” result and patient have the disease, P(Pos | D) = 0.99 =99%\n        - Probability of people not suffering from Disease D, P(~D) = 0.97 = 97%\n        - Probability that test gives “positive” result and patient does have the disease, P(Pos | ~D) = 0.01 =1%\n    - For calculating the probability that the patient actually have the disease i.e, P( D | Pos) we will use Bayes theorem.\n    - P(Pos) = P(D, pos) + P( ~D, pos) = P(pos|D) * P(D) + P(pos|~D) * P(~D) = 0.99 * 0.03 + 0.01 * 0.97 = 0.0394\n    - Hence, P( D | Pos) = (P(Pos | D) * P(D)) / P(Pos) = (0.99 * 0.03) / 0.0394 = 0.753807107\n    - So, Approximately 75% chances are there that the patient is actually suffering from disease.\n    - This is how Bayes' Theorem works. <a href='https://dataaspirant.com/naive-bayes-classifier-machine-learning'> Reference </a>\n- Types of Naive Bayes Algorithms:\n    1. Gaussian Naïve Bayes Classifier: In Gaussian Naïve Bayes, continuous values associated with each feature are assumed to be distributed according to a Gaussian distribution (Normal distribution). When plotted, it gives a bell-shaped curve which is symmetric about the mean of the feature values.\n    2. Multinomial Naïve Bayes Classifier: Feature vectors represent the frequencies with which certain events have been generated by a multinomial distribution. This is the event model typically used for document classification.\n    3. Bernoulli Naïve Bayes Classifier: In the multivariate Bernoulli event model, features are independent booleans (binary variables) describing inputs. Like the multinomial model, this model is popular for document classification tasks, where binary term occurrence (i.e. a word occurs in a document or not) features are used rather than term frequencies (i.e. frequency of a word in the document).\n- Applications of Naive Bayes Algorithm:\n    1. Real time Prediction: Naive Bayes is an eager learning classifier and it is sure fast. Thus, it could be used for making predictions in real time.\n    2. Multi class Prediction: This algorithm is also well known for multi class prediction feature. Here we can predict the probability of multiple classes of target variable.\n    3. Text classification/ Spam Filtering/ Sentiment Analysis: Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering (identify spam e-mail) and Sentiment Analysis (in social media analysis, to identify positive and negative customer sentiments)\n    4. Recommendation System: Naive Bayes Classifier and Collaborative Filtering together builds a Recommendation System that uses machine learning and data mining techniques to filter unseen information and predict whether a user would like a given resource or not\n- Advantages:\n    - It is easy and fast to predict class of test data set. It also perform well in multi class prediction\n    - When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\n    - It performs well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).\n- Disadvatages:\n    - If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation.\n    - On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\n    - Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1973)\nmodel1=NaiveBayes(label ~., data=train)\nmodel = naive_bayes(label ~., data=train)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(model1)\n```\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-3.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-4.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-5.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-6.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-7.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-8.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-9.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-10.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-11.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-12.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-13.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-14.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-15.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-16.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-17.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-15-18.png){width=672}\n:::\n:::\n\n\nAbove are the density line plots for all the feature variables for all 3 label values.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_pred=predict(model,train)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: predict.naive_bayes(): more features in the newdata are provided as\nthere are probability tables in the object. Calculation is performed based on\nfeatures to be found in the tables.\n```\n:::\n\n```{.r .cell-code}\ntrain_cm = table(train_pred,train$label)\nconfusionMatrix(train_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n          \ntrain_pred     1     2     3\n         1  2375   327     0\n         2   310  3468   256\n         3     0  2569 12242\n\nOverall Statistics\n                                          \n               Accuracy : 0.8393          \n                 95% CI : (0.8344, 0.8442)\n    No Information Rate : 0.58            \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.6971          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: 1 Class: 2 Class: 3\nSensitivity            0.8845   0.5449   0.9795\nSpecificity            0.9827   0.9627   0.7161\nPos Pred Value         0.8790   0.8597   0.8265\nNeg Pred Value         0.9836   0.8346   0.9620\nPrevalence             0.1246   0.2954   0.5800\nDetection Rate         0.1102   0.1610   0.5682\nDetection Prevalence   0.1254   0.1872   0.6874\nBalanced Accuracy      0.9336   0.7538   0.8478\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_pred=predict(model,test)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: predict.naive_bayes(): more features in the newdata are provided as\nthere are probability tables in the object. Calculation is performed based on\nfeatures to be found in the tables.\n```\n:::\n\n```{.r .cell-code}\ntest_cm = table(test_pred,test$label)\nconfusionMatrix(test_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nConfusion Matrix and Statistics\n\n         \ntest_pred    1    2    3\n        1  669   89    1\n        2   85  864   68\n        3    0  661 2957\n\nOverall Statistics\n                                          \n               Accuracy : 0.8324          \n                 95% CI : (0.8222, 0.8423)\n    No Information Rate : 0.561           \n    P-Value [Acc > NIR] : < 2.2e-16       \n                                          \n                  Kappa : 0.694           \n                                          \n Mcnemar's Test P-Value : < 2.2e-16       \n\nStatistics by Class:\n\n                     Class: 1 Class: 2 Class: 3\nSensitivity            0.8873   0.5353   0.9772\nSpecificity            0.9806   0.9595   0.7209\nPos Pred Value         0.8814   0.8496   0.8173\nNeg Pred Value         0.9817   0.8286   0.9611\nPrevalence             0.1398   0.2992   0.5610\nDetection Rate         0.1240   0.1602   0.5482\nDetection Prevalence   0.1407   0.1885   0.6707\nBalanced Accuracy      0.9339   0.7474   0.8490\n```\n:::\n:::\n\n\nWe get training accuracy from our model as 83.93% and balanced accuracy as 93% for Podium, 75% for Top_10 and 84% for Outisde_Top_10.\nTest accuracy from our model as 83.24% and balanced accuracy as 93% for Podium, 74% for Top_10 and 85% for Outisde_Top_10. There is a not a lot of difference between train and test accuracy which means our model is not over fitted.\n\nConfusion Matrix for Train and Test Data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain_cm_df = data.frame(train_cm)\ncolnames(train_cm_df) = c('pred', 'truth', 'y')\n\nggplot(data = train_cm_df, mapping = aes(x = truth , y = pred)) +\n  geom_tile(aes(fill = y), colour = \"white\") +\n  labs(title = 'Confusion Matrix of Train Data') +\n  scale_x_discrete(labels=c(\"1\" = \"Podium\", \"2\" = \"Top_10\", \"3\" = \"Outide_Top_10\")) +\n  scale_y_discrete(labels=c(\"1\" = \"Podium\", \"2\" = \"Top_10\", \"3\" = \"Outide_Top_10\")) +\n  geom_text(aes(label = sprintf(\"%1.0f\", y)), vjust = 1, colour = 'white') +\n  #scale_fill_gradient(low = \"lightblue\", high = \"yellow\") +\n  theme_bw() + theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntest_cm_df = data.frame(test_cm)\ncolnames(test_cm_df) = c('pred', 'truth', 'y')\n\nggplot(data = test_cm_df, mapping = aes(x = truth , y = pred)) +\n  geom_tile(aes(fill = y), colour = \"white\") +\n  labs(title = 'Confusion Matrix of Test Data') +\n  scale_x_discrete(labels=c(\"1\" = \"Podium\", \"2\" = \"Top_10\", \"3\" = \"Outide_Top_10\")) +\n  scale_y_discrete(labels=c(\"1\" = \"Podium\", \"2\" = \"Top_10\", \"3\" = \"Outide_Top_10\")) +\n  geom_text(aes(label = sprintf(\"%1.0f\", y)), vjust = 1, colour = 'white') +\n  # scale_fill_gradient(low = \"cyan\", high = \"darkgoldenrod1\") +\n  theme_bw() + theme(legend.position = \"none\")\n```\n\n::: {.cell-output-display}\n![](record_NB_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "record_NB_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}