{
  "hash": "9ca5f956f43e97a1f35b7d2650e0013c",
  "result": {
    "markdown": "---\ntitle: <b>Twitter Data Gathering</b>\nformat:\n  html:\n    theme: lumen\n    toc: true\n    self-contained: true\n    embed-resources: true\n    page-layout: full\n    code-fold: true\n    code-tools: true\n---\n\n# Data Gathering\n\n- Data Gathering and Pre-Processing is a very important step in any Data science project pipeline. It is undeniable that 80% of a data scientist's time and effort is spent in collecting, cleaning and preparing the data for analysis because datasets come in various sizes and are different in nature. It is extremely important for a data scientist to reshape and refine the datasets into usable datasets, which can be leveraged for analytics.\n- Knowledge is power, information is knowledge, and data is information in digitized form, at least as defined in IT. Hence, data is power. But before you can leverage that data into a successful strategy for your organization or business, you need to gather it. That’s your first step.\n- Before we define what is data collection, it’s essential to ask the question, “What is data?” The abridged answer is, data is various kinds of information formatted in a particular way. Therefore, data collection is the process of gathering, measuring, and analyzing accurate data from a variety of relevant sources to find answers to research problems, answer questions, evaluate outcomes, and forecast trends and probabilities.\n- Our society is highly dependent on data, which underscores the importance of collecting it. Accurate data collection is necessary to make informed business decisions, ensure quality assurance, and keep research integrity.\n- During data collection, the researchers must identify the data types, the sources of data, and what methods are being used. We will soon see that there are many different data collection methods. There is heavy reliance on data collection in research, commercial, and government fields.\n- `Why Do We Need Data Collection?`\n    - Before a judge makes a ruling in a court case or a general creates a plan of attack, they must have as many relevant facts as possible. The best courses of action come from informed decisions, and information and data are synonymous.\n    - The concept of data collection isn’t a new one but the world has changed. There is far more data available today, and it exists in forms that were unheard of a century ago. The data collection process has had to change and grow with the times, keeping pace with technology.\n    - Whether you’re in the world of academia, trying to conduct research, or part of the commercial sector, thinking of how to promote a new product, you need data collection to help you make better choices.\n- `What Are the Different Methods of Data Collection?`\n    - Surveys\n    - Transactional Tracking\n    - Interviews and Focus Groups\n    - Observation\n    - Online Tracking\n    - Forms\n    - Social Media Monitoring\n    - Application Programming Interface\n\n# Twitter API\n- Twitter is what’s happening in the world and what people are talking about right now. You can access Twitter via the web or your mobile device. To share information on Twitter as widely as possible, we also provide companies, developers, and users with programmatic access to Twitter data through our APIs (application programming interfaces).\n- At the end of 2020, Twitter introduced a new Twitter API built from the ground up. Twitter API v2 comes with more features and data you can pull and analyze, new endpoints, and a lot of functionalities.\n- With the introduction of that new API, Twitter also introduced a new powerful free product for academics: The Academic Research product track.\n- The track grants free access to full-archive search and other v2 endpoints, with a volume cap of 10,000,000 tweets per month! If you want to know if you qualify for the track or not, check this link.\n- Yet since v2 of the API is fairly new, fewer resources exist if you run into issues through the process of collecting data for your research.\n- Twitter data is unique from data shared by most other social platforms because it reflects information that users choose to share publicly. The API platform provides broad access to public Twitter data that users have chosen to share with the world. It also support APIs that allow users to manage their own non-public Twitter information (e.g., Direct Messages) and provide this information to developers whom they have authorized to do so. \n\n# Import Libraries\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport os\nimport time\nimport requests\nimport json\nimport csv\nfrom tqdm import tqdm\n\nimport tweepy\n\nimport requests\nimport pandas as pd\nimport os\n\nimport matplotlib.pyplot as plt\nfrom turtle import color\nfrom collections import Counter\n```\n:::\n\n\n# Set Twitter API Keys\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nconsumer_key        = 'mvpqVTwR7IBgL4wLAF6VSV8Fd'\nconsumer_secret     = 'MPTgWHei1DLcLSI9BixrufChLU1t2L63a2Z01wqPPWeKnuquGR'\naccess_token        = '1567741611135717376-P5V6SK5VpbNK5E9sPqYjBJBNuYXSsw'\naccess_token_secret = 'ih0sZBZDmCieDXNAXjdPvxmjeLBvV9J6nkM8DfxhWwdPp'\nbearer_token        = 'AAAAAAAAAAAAAAAAAAAAAOJigwEAAAAA85lnBTZlhhTy84L4U2c%2BR8T4e7c%3DPNtcxQs4Lybq3eeN8CTBjyxCuPbPRv2DaZ3H5IgnkzbXj3WbPb'\n```\n:::\n\n\n# Set the Twitter authentication and bearer token\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_token_secret)\n\napi = tweepy.API(auth, wait_on_rate_limit=True)\nheaders = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n```\n:::\n\n\nExtraction function\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndef search_twitter(query, max_results, bearer_token, start_time, end_time, tweet_fields):\n    \n    client = tweepy.Client(bearer_token = bearer_token)\n    tweets = tweepy.Paginator(client.search_recent_tweets, query=query, tweet_fields=tweet_fields,\n                                start_time=start_time, end_time=end_time).flatten(limit = max_results)\n    \n    tweet_search = []\n    for tweet in tweets:\n        tweet_search.append((tweet.text, tweet.author_id, tweet.created_at, tweet.lang))\n        \n    return tweet_search\n```\n:::\n\n\nStoring the tweets to a CSV file\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndef store_tweets_f1(query_list):\n    \n    max_results = 1000\n    start_time = '2023-03-02T00:00:00Z'\n    end_time = '2023-03-07T00:00:00Z'\n    tweet_fields = 'text,author_id,created_at,lang'\n    \n    for query in query_list:\n        tweet_search = search_twitter(query + \" f1 -is:retweet\", max_results, bearer_token, start_time, end_time, tweet_fields)\n        df = pd.DataFrame(tweet_search, columns = ['text', 'author_id', 'created_at', 'lang'])\n        df = df[df['lang'] == 'en']\n        df = df[['text', 'lang']]\n        df.to_csv('../../data/00-raw-data/' + query + '_f1_tweets_2023.csv')\n        \n    return df\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nquery_list = ['ferrari', 'mercedes', 'redbull', 'mclaren', 'aston martin', 'alpha tauri', 'alpine', 'williams', 'haas', 'alfa romeo']\n```\n:::\n\n\n```{{python}}\nstore_tweets_f1(query_list)\n```\n\n",
    "supporting": [
      "twitter_datagathering_files"
    ],
    "filters": [],
    "includes": {}
  }
}